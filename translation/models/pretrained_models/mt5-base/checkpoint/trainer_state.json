{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 10250,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008417508417508417,
      "grad_norm": 9667.3251953125,
      "learning_rate": 2.9974747474747474e-05,
      "loss": 15.0931,
      "step": 10
    },
    {
      "epoch": 0.016835016835016835,
      "grad_norm": 42033.1484375,
      "learning_rate": 2.994949494949495e-05,
      "loss": 14.5906,
      "step": 20
    },
    {
      "epoch": 0.025252525252525252,
      "grad_norm": 5951.3212890625,
      "learning_rate": 2.9924242424242427e-05,
      "loss": 14.4259,
      "step": 30
    },
    {
      "epoch": 0.03367003367003367,
      "grad_norm": 3324.2275390625,
      "learning_rate": 2.98989898989899e-05,
      "loss": 13.0291,
      "step": 40
    },
    {
      "epoch": 0.04208754208754209,
      "grad_norm": 1417.022216796875,
      "learning_rate": 2.9873737373737373e-05,
      "loss": 12.0539,
      "step": 50
    },
    {
      "epoch": 0.050505050505050504,
      "grad_norm": 5249.63720703125,
      "learning_rate": 2.9848484848484847e-05,
      "loss": 11.7519,
      "step": 60
    },
    {
      "epoch": 0.058922558922558925,
      "grad_norm": 22561.09375,
      "learning_rate": 2.9823232323232327e-05,
      "loss": 11.7467,
      "step": 70
    },
    {
      "epoch": 0.06734006734006734,
      "grad_norm": 5308.9921875,
      "learning_rate": 2.97979797979798e-05,
      "loss": 10.6165,
      "step": 80
    },
    {
      "epoch": 0.07575757575757576,
      "grad_norm": 1176.58935546875,
      "learning_rate": 2.9772727272727273e-05,
      "loss": 10.1366,
      "step": 90
    },
    {
      "epoch": 0.08417508417508418,
      "grad_norm": 2828.736083984375,
      "learning_rate": 2.9747474747474746e-05,
      "loss": 9.8011,
      "step": 100
    },
    {
      "epoch": 0.09259259259259259,
      "grad_norm": 826.6690673828125,
      "learning_rate": 2.9722222222222223e-05,
      "loss": 9.944,
      "step": 110
    },
    {
      "epoch": 0.10101010101010101,
      "grad_norm": 4841.95947265625,
      "learning_rate": 2.96969696969697e-05,
      "loss": 9.8329,
      "step": 120
    },
    {
      "epoch": 0.10942760942760943,
      "grad_norm": 3818.68115234375,
      "learning_rate": 2.9671717171717172e-05,
      "loss": 9.1106,
      "step": 130
    },
    {
      "epoch": 0.11784511784511785,
      "grad_norm": 6473.505859375,
      "learning_rate": 2.9646464646464646e-05,
      "loss": 9.1754,
      "step": 140
    },
    {
      "epoch": 0.12626262626262627,
      "grad_norm": 1774.7872314453125,
      "learning_rate": 2.9621212121212122e-05,
      "loss": 8.345,
      "step": 150
    },
    {
      "epoch": 0.13468013468013468,
      "grad_norm": 369.1099548339844,
      "learning_rate": 2.9595959595959595e-05,
      "loss": 7.8617,
      "step": 160
    },
    {
      "epoch": 0.14309764309764308,
      "grad_norm": 810.7764892578125,
      "learning_rate": 2.9570707070707072e-05,
      "loss": 6.9695,
      "step": 170
    },
    {
      "epoch": 0.15151515151515152,
      "grad_norm": 268.8722229003906,
      "learning_rate": 2.9545454545454545e-05,
      "loss": 6.4136,
      "step": 180
    },
    {
      "epoch": 0.15993265993265993,
      "grad_norm": 82.35987854003906,
      "learning_rate": 2.952020202020202e-05,
      "loss": 5.729,
      "step": 190
    },
    {
      "epoch": 0.16835016835016836,
      "grad_norm": 45.54754638671875,
      "learning_rate": 2.9494949494949495e-05,
      "loss": 5.3058,
      "step": 200
    },
    {
      "epoch": 0.17676767676767677,
      "grad_norm": 36.35483169555664,
      "learning_rate": 2.946969696969697e-05,
      "loss": 5.0203,
      "step": 210
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 42.771244049072266,
      "learning_rate": 2.9444444444444445e-05,
      "loss": 4.2621,
      "step": 220
    },
    {
      "epoch": 0.1936026936026936,
      "grad_norm": 57.627540588378906,
      "learning_rate": 2.941919191919192e-05,
      "loss": 3.8336,
      "step": 230
    },
    {
      "epoch": 0.20202020202020202,
      "grad_norm": 27.491331100463867,
      "learning_rate": 2.9393939393939394e-05,
      "loss": 3.5633,
      "step": 240
    },
    {
      "epoch": 0.21043771043771045,
      "grad_norm": 37.25906753540039,
      "learning_rate": 2.9368686868686867e-05,
      "loss": 3.4738,
      "step": 250
    },
    {
      "epoch": 0.21885521885521886,
      "grad_norm": 489.70892333984375,
      "learning_rate": 2.9343434343434344e-05,
      "loss": 3.6124,
      "step": 260
    },
    {
      "epoch": 0.22727272727272727,
      "grad_norm": 38.74910354614258,
      "learning_rate": 2.931818181818182e-05,
      "loss": 3.413,
      "step": 270
    },
    {
      "epoch": 0.2356902356902357,
      "grad_norm": 41.25163650512695,
      "learning_rate": 2.9292929292929294e-05,
      "loss": 3.0553,
      "step": 280
    },
    {
      "epoch": 0.2441077441077441,
      "grad_norm": 23.895143508911133,
      "learning_rate": 2.9267676767676767e-05,
      "loss": 2.9623,
      "step": 290
    },
    {
      "epoch": 0.25252525252525254,
      "grad_norm": 98.93815612792969,
      "learning_rate": 2.924242424242424e-05,
      "loss": 2.8722,
      "step": 300
    },
    {
      "epoch": 0.2609427609427609,
      "grad_norm": 16.691837310791016,
      "learning_rate": 2.921717171717172e-05,
      "loss": 2.628,
      "step": 310
    },
    {
      "epoch": 0.26936026936026936,
      "grad_norm": 23.63001823425293,
      "learning_rate": 2.9191919191919193e-05,
      "loss": 2.7824,
      "step": 320
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 20.259965896606445,
      "learning_rate": 2.9166666666666666e-05,
      "loss": 2.52,
      "step": 330
    },
    {
      "epoch": 0.28619528619528617,
      "grad_norm": 13.65307331085205,
      "learning_rate": 2.914141414141414e-05,
      "loss": 2.6053,
      "step": 340
    },
    {
      "epoch": 0.2946127946127946,
      "grad_norm": 32.83488082885742,
      "learning_rate": 2.911616161616162e-05,
      "loss": 2.5841,
      "step": 350
    },
    {
      "epoch": 0.30303030303030304,
      "grad_norm": 183.8993377685547,
      "learning_rate": 2.9090909090909093e-05,
      "loss": 2.2063,
      "step": 360
    },
    {
      "epoch": 0.3114478114478115,
      "grad_norm": 29.204116821289062,
      "learning_rate": 2.9065656565656566e-05,
      "loss": 2.2948,
      "step": 370
    },
    {
      "epoch": 0.31986531986531985,
      "grad_norm": 71.8239517211914,
      "learning_rate": 2.904040404040404e-05,
      "loss": 2.5365,
      "step": 380
    },
    {
      "epoch": 0.3282828282828283,
      "grad_norm": 393.5036926269531,
      "learning_rate": 2.9015151515151516e-05,
      "loss": 2.5189,
      "step": 390
    },
    {
      "epoch": 0.3367003367003367,
      "grad_norm": 10.60307502746582,
      "learning_rate": 2.8989898989898992e-05,
      "loss": 2.0986,
      "step": 400
    },
    {
      "epoch": 0.3451178451178451,
      "grad_norm": 7.5401740074157715,
      "learning_rate": 2.8964646464646465e-05,
      "loss": 2.177,
      "step": 410
    },
    {
      "epoch": 0.35353535353535354,
      "grad_norm": 46.1457633972168,
      "learning_rate": 2.893939393939394e-05,
      "loss": 2.3589,
      "step": 420
    },
    {
      "epoch": 0.36195286195286197,
      "grad_norm": 26.1547908782959,
      "learning_rate": 2.8914141414141415e-05,
      "loss": 2.1653,
      "step": 430
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 15.786820411682129,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 2.18,
      "step": 440
    },
    {
      "epoch": 0.3787878787878788,
      "grad_norm": 50.20917510986328,
      "learning_rate": 2.8863636363636365e-05,
      "loss": 2.1435,
      "step": 450
    },
    {
      "epoch": 0.3872053872053872,
      "grad_norm": 8.401055335998535,
      "learning_rate": 2.8838383838383838e-05,
      "loss": 2.022,
      "step": 460
    },
    {
      "epoch": 0.3956228956228956,
      "grad_norm": 18.67900848388672,
      "learning_rate": 2.8813131313131315e-05,
      "loss": 1.8109,
      "step": 470
    },
    {
      "epoch": 0.40404040404040403,
      "grad_norm": 8.427495956420898,
      "learning_rate": 2.8787878787878788e-05,
      "loss": 1.8155,
      "step": 480
    },
    {
      "epoch": 0.41245791245791247,
      "grad_norm": 52.312171936035156,
      "learning_rate": 2.8762626262626264e-05,
      "loss": 1.7732,
      "step": 490
    },
    {
      "epoch": 0.4208754208754209,
      "grad_norm": 16.997785568237305,
      "learning_rate": 2.8737373737373737e-05,
      "loss": 1.9643,
      "step": 500
    },
    {
      "epoch": 0.4292929292929293,
      "grad_norm": 11.260360717773438,
      "learning_rate": 2.8712121212121214e-05,
      "loss": 1.87,
      "step": 510
    },
    {
      "epoch": 0.4377104377104377,
      "grad_norm": 16.452299118041992,
      "learning_rate": 2.8686868686868687e-05,
      "loss": 1.8326,
      "step": 520
    },
    {
      "epoch": 0.44612794612794615,
      "grad_norm": 10.946778297424316,
      "learning_rate": 2.866161616161616e-05,
      "loss": 1.6917,
      "step": 530
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 9.767773628234863,
      "learning_rate": 2.8636363636363637e-05,
      "loss": 1.5903,
      "step": 540
    },
    {
      "epoch": 0.46296296296296297,
      "grad_norm": 25.155508041381836,
      "learning_rate": 2.8611111111111113e-05,
      "loss": 1.5363,
      "step": 550
    },
    {
      "epoch": 0.4713804713804714,
      "grad_norm": 5.921866416931152,
      "learning_rate": 2.8585858585858587e-05,
      "loss": 1.5188,
      "step": 560
    },
    {
      "epoch": 0.4797979797979798,
      "grad_norm": 11.131423950195312,
      "learning_rate": 2.856060606060606e-05,
      "loss": 1.4008,
      "step": 570
    },
    {
      "epoch": 0.4882154882154882,
      "grad_norm": 48.24918746948242,
      "learning_rate": 2.8535353535353533e-05,
      "loss": 1.1156,
      "step": 580
    },
    {
      "epoch": 0.49663299663299665,
      "grad_norm": 16.87063217163086,
      "learning_rate": 2.8510101010101013e-05,
      "loss": 1.3379,
      "step": 590
    },
    {
      "epoch": 0.5050505050505051,
      "grad_norm": 10.611139297485352,
      "learning_rate": 2.8484848484848486e-05,
      "loss": 1.3968,
      "step": 600
    },
    {
      "epoch": 0.5134680134680135,
      "grad_norm": 17.489225387573242,
      "learning_rate": 2.845959595959596e-05,
      "loss": 1.3712,
      "step": 610
    },
    {
      "epoch": 0.5218855218855218,
      "grad_norm": 9.991411209106445,
      "learning_rate": 2.8434343434343433e-05,
      "loss": 1.4067,
      "step": 620
    },
    {
      "epoch": 0.5303030303030303,
      "grad_norm": 6.952061653137207,
      "learning_rate": 2.8409090909090912e-05,
      "loss": 1.386,
      "step": 630
    },
    {
      "epoch": 0.5387205387205387,
      "grad_norm": 6.232653617858887,
      "learning_rate": 2.8383838383838386e-05,
      "loss": 1.579,
      "step": 640
    },
    {
      "epoch": 0.5471380471380471,
      "grad_norm": 50.21087646484375,
      "learning_rate": 2.835858585858586e-05,
      "loss": 1.3493,
      "step": 650
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 12.277027130126953,
      "learning_rate": 2.8333333333333332e-05,
      "loss": 1.5383,
      "step": 660
    },
    {
      "epoch": 0.563973063973064,
      "grad_norm": 7.562788009643555,
      "learning_rate": 2.830808080808081e-05,
      "loss": 1.3909,
      "step": 670
    },
    {
      "epoch": 0.5723905723905723,
      "grad_norm": 51.69778060913086,
      "learning_rate": 2.8282828282828285e-05,
      "loss": 1.315,
      "step": 680
    },
    {
      "epoch": 0.5808080808080808,
      "grad_norm": 5.298283576965332,
      "learning_rate": 2.8257575757575758e-05,
      "loss": 1.302,
      "step": 690
    },
    {
      "epoch": 0.5892255892255892,
      "grad_norm": 84.2850341796875,
      "learning_rate": 2.823232323232323e-05,
      "loss": 1.2635,
      "step": 700
    },
    {
      "epoch": 0.5976430976430976,
      "grad_norm": 5.322480201721191,
      "learning_rate": 2.8207070707070708e-05,
      "loss": 1.3722,
      "step": 710
    },
    {
      "epoch": 0.6060606060606061,
      "grad_norm": 16.74196434020996,
      "learning_rate": 2.8181818181818185e-05,
      "loss": 1.343,
      "step": 720
    },
    {
      "epoch": 0.6144781144781145,
      "grad_norm": 4.881004810333252,
      "learning_rate": 2.8156565656565658e-05,
      "loss": 1.3909,
      "step": 730
    },
    {
      "epoch": 0.622895622895623,
      "grad_norm": 18.529157638549805,
      "learning_rate": 2.813131313131313e-05,
      "loss": 1.2903,
      "step": 740
    },
    {
      "epoch": 0.6313131313131313,
      "grad_norm": 31.37026023864746,
      "learning_rate": 2.8106060606060607e-05,
      "loss": 1.3156,
      "step": 750
    },
    {
      "epoch": 0.6397306397306397,
      "grad_norm": 113.29618072509766,
      "learning_rate": 2.808080808080808e-05,
      "loss": 1.1914,
      "step": 760
    },
    {
      "epoch": 0.6481481481481481,
      "grad_norm": 10.418601036071777,
      "learning_rate": 2.8055555555555557e-05,
      "loss": 1.1434,
      "step": 770
    },
    {
      "epoch": 0.6565656565656566,
      "grad_norm": 13.036421775817871,
      "learning_rate": 2.803030303030303e-05,
      "loss": 1.2174,
      "step": 780
    },
    {
      "epoch": 0.664983164983165,
      "grad_norm": 7.448062419891357,
      "learning_rate": 2.8005050505050507e-05,
      "loss": 1.1854,
      "step": 790
    },
    {
      "epoch": 0.6734006734006734,
      "grad_norm": 6.176835536956787,
      "learning_rate": 2.797979797979798e-05,
      "loss": 1.4079,
      "step": 800
    },
    {
      "epoch": 0.6818181818181818,
      "grad_norm": 6.245594501495361,
      "learning_rate": 2.7954545454545453e-05,
      "loss": 1.0937,
      "step": 810
    },
    {
      "epoch": 0.6902356902356902,
      "grad_norm": 5.428690433502197,
      "learning_rate": 2.792929292929293e-05,
      "loss": 0.9842,
      "step": 820
    },
    {
      "epoch": 0.6986531986531986,
      "grad_norm": 9.47002124786377,
      "learning_rate": 2.7904040404040406e-05,
      "loss": 1.2999,
      "step": 830
    },
    {
      "epoch": 0.7070707070707071,
      "grad_norm": 11.580007553100586,
      "learning_rate": 2.787878787878788e-05,
      "loss": 1.2148,
      "step": 840
    },
    {
      "epoch": 0.7154882154882155,
      "grad_norm": 3.63175630569458,
      "learning_rate": 2.7853535353535353e-05,
      "loss": 1.2391,
      "step": 850
    },
    {
      "epoch": 0.7239057239057239,
      "grad_norm": 9.739265441894531,
      "learning_rate": 2.782828282828283e-05,
      "loss": 1.1892,
      "step": 860
    },
    {
      "epoch": 0.7323232323232324,
      "grad_norm": 5.149448394775391,
      "learning_rate": 2.7803030303030306e-05,
      "loss": 1.0652,
      "step": 870
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 241.267578125,
      "learning_rate": 2.777777777777778e-05,
      "loss": 1.0718,
      "step": 880
    },
    {
      "epoch": 0.7491582491582491,
      "grad_norm": 10.049564361572266,
      "learning_rate": 2.7752525252525252e-05,
      "loss": 1.009,
      "step": 890
    },
    {
      "epoch": 0.7575757575757576,
      "grad_norm": 9.436285972595215,
      "learning_rate": 2.7727272727272725e-05,
      "loss": 1.1839,
      "step": 900
    },
    {
      "epoch": 0.765993265993266,
      "grad_norm": 17.097320556640625,
      "learning_rate": 2.7702020202020205e-05,
      "loss": 1.025,
      "step": 910
    },
    {
      "epoch": 0.7744107744107744,
      "grad_norm": 5.843761444091797,
      "learning_rate": 2.767676767676768e-05,
      "loss": 1.011,
      "step": 920
    },
    {
      "epoch": 0.7828282828282829,
      "grad_norm": 5.17745304107666,
      "learning_rate": 2.7651515151515152e-05,
      "loss": 1.086,
      "step": 930
    },
    {
      "epoch": 0.7912457912457912,
      "grad_norm": 19.125965118408203,
      "learning_rate": 2.7626262626262625e-05,
      "loss": 1.0782,
      "step": 940
    },
    {
      "epoch": 0.7996632996632996,
      "grad_norm": 6.613885879516602,
      "learning_rate": 2.76010101010101e-05,
      "loss": 0.9532,
      "step": 950
    },
    {
      "epoch": 0.8080808080808081,
      "grad_norm": 6.450374126434326,
      "learning_rate": 2.7575757575757578e-05,
      "loss": 1.115,
      "step": 960
    },
    {
      "epoch": 0.8164983164983165,
      "grad_norm": 27.457862854003906,
      "learning_rate": 2.755050505050505e-05,
      "loss": 1.0077,
      "step": 970
    },
    {
      "epoch": 0.8249158249158249,
      "grad_norm": 14.528059959411621,
      "learning_rate": 2.7525252525252524e-05,
      "loss": 1.1257,
      "step": 980
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 7.73338508605957,
      "learning_rate": 2.75e-05,
      "loss": 1.1646,
      "step": 990
    },
    {
      "epoch": 0.8417508417508418,
      "grad_norm": 4.746180057525635,
      "learning_rate": 2.7474747474747478e-05,
      "loss": 1.0627,
      "step": 1000
    },
    {
      "epoch": 0.9853658536585366,
      "grad_norm": 20.020008087158203,
      "learning_rate": 2.9970731707317074e-05,
      "loss": 4.5544,
      "step": 1010
    },
    {
      "epoch": 0.9951219512195122,
      "grad_norm": 13.542830467224121,
      "learning_rate": 2.9941463414634147e-05,
      "loss": 4.099,
      "step": 1020
    },
    {
      "epoch": 1.0,
      "eval_loss": 2.520235061645508,
      "eval_runtime": 2.433,
      "eval_samples_per_second": 177.146,
      "eval_steps_per_second": 22.195,
      "step": 1025
    },
    {
      "epoch": 1.0048780487804878,
      "grad_norm": 10.928622245788574,
      "learning_rate": 2.991219512195122e-05,
      "loss": 3.9081,
      "step": 1030
    },
    {
      "epoch": 1.0146341463414634,
      "grad_norm": 20.360897064208984,
      "learning_rate": 2.9882926829268294e-05,
      "loss": 3.525,
      "step": 1040
    },
    {
      "epoch": 1.024390243902439,
      "grad_norm": 11.069668769836426,
      "learning_rate": 2.9853658536585367e-05,
      "loss": 3.3236,
      "step": 1050
    },
    {
      "epoch": 1.0341463414634147,
      "grad_norm": 23.972299575805664,
      "learning_rate": 2.982439024390244e-05,
      "loss": 3.093,
      "step": 1060
    },
    {
      "epoch": 1.0439024390243903,
      "grad_norm": 16.101247787475586,
      "learning_rate": 2.9795121951219514e-05,
      "loss": 3.0226,
      "step": 1070
    },
    {
      "epoch": 1.053658536585366,
      "grad_norm": 8.864217758178711,
      "learning_rate": 2.9765853658536587e-05,
      "loss": 3.1785,
      "step": 1080
    },
    {
      "epoch": 1.0634146341463415,
      "grad_norm": 426.19482421875,
      "learning_rate": 2.973658536585366e-05,
      "loss": 2.5871,
      "step": 1090
    },
    {
      "epoch": 1.0731707317073171,
      "grad_norm": 14.11047077178955,
      "learning_rate": 2.9707317073170734e-05,
      "loss": 2.9902,
      "step": 1100
    },
    {
      "epoch": 1.0829268292682928,
      "grad_norm": 50.606590270996094,
      "learning_rate": 2.9678048780487807e-05,
      "loss": 2.6285,
      "step": 1110
    },
    {
      "epoch": 1.0926829268292684,
      "grad_norm": 124.0584716796875,
      "learning_rate": 2.9648780487804877e-05,
      "loss": 2.6314,
      "step": 1120
    },
    {
      "epoch": 1.102439024390244,
      "grad_norm": 55.498802185058594,
      "learning_rate": 2.961951219512195e-05,
      "loss": 2.5061,
      "step": 1130
    },
    {
      "epoch": 1.1121951219512196,
      "grad_norm": 9.722956657409668,
      "learning_rate": 2.9590243902439023e-05,
      "loss": 2.4106,
      "step": 1140
    },
    {
      "epoch": 1.1219512195121952,
      "grad_norm": 10.594475746154785,
      "learning_rate": 2.9560975609756097e-05,
      "loss": 2.4445,
      "step": 1150
    },
    {
      "epoch": 1.1317073170731708,
      "grad_norm": 68.19452667236328,
      "learning_rate": 2.953170731707317e-05,
      "loss": 2.5313,
      "step": 1160
    },
    {
      "epoch": 1.1414634146341462,
      "grad_norm": 12.880619049072266,
      "learning_rate": 2.9502439024390243e-05,
      "loss": 2.7973,
      "step": 1170
    },
    {
      "epoch": 1.1512195121951219,
      "grad_norm": 21.483041763305664,
      "learning_rate": 2.9473170731707317e-05,
      "loss": 2.4306,
      "step": 1180
    },
    {
      "epoch": 1.1609756097560975,
      "grad_norm": 630.1315307617188,
      "learning_rate": 2.944390243902439e-05,
      "loss": 2.607,
      "step": 1190
    },
    {
      "epoch": 1.170731707317073,
      "grad_norm": 10.509880065917969,
      "learning_rate": 2.9414634146341463e-05,
      "loss": 2.5221,
      "step": 1200
    },
    {
      "epoch": 1.1804878048780487,
      "grad_norm": 22.787080764770508,
      "learning_rate": 2.9385365853658536e-05,
      "loss": 2.4657,
      "step": 1210
    },
    {
      "epoch": 1.1902439024390243,
      "grad_norm": 6.354641437530518,
      "learning_rate": 2.935609756097561e-05,
      "loss": 2.3071,
      "step": 1220
    },
    {
      "epoch": 1.2,
      "grad_norm": 10.333816528320312,
      "learning_rate": 2.9326829268292683e-05,
      "loss": 2.2938,
      "step": 1230
    },
    {
      "epoch": 1.2097560975609756,
      "grad_norm": 12.344757080078125,
      "learning_rate": 2.9297560975609756e-05,
      "loss": 2.1681,
      "step": 1240
    },
    {
      "epoch": 1.2195121951219512,
      "grad_norm": 7.993765830993652,
      "learning_rate": 2.926829268292683e-05,
      "loss": 1.994,
      "step": 1250
    },
    {
      "epoch": 1.2292682926829268,
      "grad_norm": 23.53179359436035,
      "learning_rate": 2.9239024390243903e-05,
      "loss": 2.4115,
      "step": 1260
    },
    {
      "epoch": 1.2390243902439024,
      "grad_norm": 21.11083984375,
      "learning_rate": 2.9209756097560976e-05,
      "loss": 2.0997,
      "step": 1270
    },
    {
      "epoch": 1.248780487804878,
      "grad_norm": 17.37286949157715,
      "learning_rate": 2.918048780487805e-05,
      "loss": 2.2123,
      "step": 1280
    },
    {
      "epoch": 1.2585365853658537,
      "grad_norm": 12.55423641204834,
      "learning_rate": 2.9151219512195123e-05,
      "loss": 2.3928,
      "step": 1290
    },
    {
      "epoch": 1.2682926829268293,
      "grad_norm": 11.209489822387695,
      "learning_rate": 2.9121951219512196e-05,
      "loss": 2.2377,
      "step": 1300
    },
    {
      "epoch": 1.278048780487805,
      "grad_norm": 29.9178524017334,
      "learning_rate": 2.909268292682927e-05,
      "loss": 2.181,
      "step": 1310
    },
    {
      "epoch": 1.2878048780487805,
      "grad_norm": 11.699418067932129,
      "learning_rate": 2.9063414634146343e-05,
      "loss": 2.0538,
      "step": 1320
    },
    {
      "epoch": 1.2975609756097561,
      "grad_norm": 59.62372589111328,
      "learning_rate": 2.9034146341463416e-05,
      "loss": 2.0475,
      "step": 1330
    },
    {
      "epoch": 1.3073170731707318,
      "grad_norm": 24.24758529663086,
      "learning_rate": 2.900487804878049e-05,
      "loss": 2.1326,
      "step": 1340
    },
    {
      "epoch": 1.3170731707317074,
      "grad_norm": 14.943646430969238,
      "learning_rate": 2.8975609756097562e-05,
      "loss": 2.1776,
      "step": 1350
    },
    {
      "epoch": 1.326829268292683,
      "grad_norm": 9.189420700073242,
      "learning_rate": 2.8946341463414632e-05,
      "loss": 2.0201,
      "step": 1360
    },
    {
      "epoch": 1.3365853658536586,
      "grad_norm": 12.09659194946289,
      "learning_rate": 2.8917073170731706e-05,
      "loss": 2.1397,
      "step": 1370
    },
    {
      "epoch": 1.346341463414634,
      "grad_norm": 47.221519470214844,
      "learning_rate": 2.888780487804878e-05,
      "loss": 2.2948,
      "step": 1380
    },
    {
      "epoch": 1.3560975609756096,
      "grad_norm": 8.198049545288086,
      "learning_rate": 2.8858536585365852e-05,
      "loss": 1.9078,
      "step": 1390
    },
    {
      "epoch": 1.3658536585365852,
      "grad_norm": 93.90608978271484,
      "learning_rate": 2.8829268292682925e-05,
      "loss": 1.983,
      "step": 1400
    },
    {
      "epoch": 1.3756097560975609,
      "grad_norm": 6.812544345855713,
      "learning_rate": 2.88e-05,
      "loss": 2.0243,
      "step": 1410
    },
    {
      "epoch": 1.3853658536585365,
      "grad_norm": 14.501465797424316,
      "learning_rate": 2.8770731707317072e-05,
      "loss": 1.7253,
      "step": 1420
    },
    {
      "epoch": 1.395121951219512,
      "grad_norm": 9.476447105407715,
      "learning_rate": 2.8741463414634145e-05,
      "loss": 2.1177,
      "step": 1430
    },
    {
      "epoch": 1.4048780487804877,
      "grad_norm": 6.929424285888672,
      "learning_rate": 2.871219512195122e-05,
      "loss": 1.9597,
      "step": 1440
    },
    {
      "epoch": 1.4146341463414633,
      "grad_norm": 13.304391860961914,
      "learning_rate": 2.8682926829268292e-05,
      "loss": 2.2542,
      "step": 1450
    },
    {
      "epoch": 1.424390243902439,
      "grad_norm": 8.639328956604004,
      "learning_rate": 2.8653658536585365e-05,
      "loss": 1.9807,
      "step": 1460
    },
    {
      "epoch": 1.4341463414634146,
      "grad_norm": 7.18489933013916,
      "learning_rate": 2.862439024390244e-05,
      "loss": 2.1709,
      "step": 1470
    },
    {
      "epoch": 1.4439024390243902,
      "grad_norm": 7.8439106941223145,
      "learning_rate": 2.8595121951219512e-05,
      "loss": 1.7286,
      "step": 1480
    },
    {
      "epoch": 1.4536585365853658,
      "grad_norm": 30.67522430419922,
      "learning_rate": 2.8565853658536585e-05,
      "loss": 1.663,
      "step": 1490
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 12.662217140197754,
      "learning_rate": 2.8536585365853658e-05,
      "loss": 1.9082,
      "step": 1500
    },
    {
      "epoch": 1.473170731707317,
      "grad_norm": 6.704500198364258,
      "learning_rate": 2.850731707317073e-05,
      "loss": 1.7933,
      "step": 1510
    },
    {
      "epoch": 1.4829268292682927,
      "grad_norm": 15.18234920501709,
      "learning_rate": 2.8478048780487805e-05,
      "loss": 1.6823,
      "step": 1520
    },
    {
      "epoch": 1.4926829268292683,
      "grad_norm": 7.515874862670898,
      "learning_rate": 2.8448780487804878e-05,
      "loss": 1.5596,
      "step": 1530
    },
    {
      "epoch": 1.502439024390244,
      "grad_norm": 6.421756267547607,
      "learning_rate": 2.841951219512195e-05,
      "loss": 1.8024,
      "step": 1540
    },
    {
      "epoch": 1.5121951219512195,
      "grad_norm": 10.920669555664062,
      "learning_rate": 2.8390243902439025e-05,
      "loss": 1.797,
      "step": 1550
    },
    {
      "epoch": 1.5219512195121951,
      "grad_norm": 7.131457805633545,
      "learning_rate": 2.8360975609756098e-05,
      "loss": 1.6467,
      "step": 1560
    },
    {
      "epoch": 1.5317073170731708,
      "grad_norm": 8.593632698059082,
      "learning_rate": 2.833170731707317e-05,
      "loss": 1.7154,
      "step": 1570
    },
    {
      "epoch": 1.5414634146341464,
      "grad_norm": 8.577921867370605,
      "learning_rate": 2.8302439024390245e-05,
      "loss": 1.8923,
      "step": 1580
    },
    {
      "epoch": 1.551219512195122,
      "grad_norm": 8.494412422180176,
      "learning_rate": 2.8273170731707318e-05,
      "loss": 1.4892,
      "step": 1590
    },
    {
      "epoch": 1.5609756097560976,
      "grad_norm": 8.852643013000488,
      "learning_rate": 2.8243902439024388e-05,
      "loss": 1.6369,
      "step": 1600
    },
    {
      "epoch": 1.5707317073170732,
      "grad_norm": 7.906232833862305,
      "learning_rate": 2.821463414634146e-05,
      "loss": 1.4745,
      "step": 1610
    },
    {
      "epoch": 1.5804878048780489,
      "grad_norm": 6.490495204925537,
      "learning_rate": 2.8185365853658534e-05,
      "loss": 1.5356,
      "step": 1620
    },
    {
      "epoch": 1.5902439024390245,
      "grad_norm": 7.213185787200928,
      "learning_rate": 2.8156097560975608e-05,
      "loss": 1.3186,
      "step": 1630
    },
    {
      "epoch": 1.6,
      "grad_norm": 173.5078582763672,
      "learning_rate": 2.812682926829268e-05,
      "loss": 1.565,
      "step": 1640
    },
    {
      "epoch": 1.6097560975609757,
      "grad_norm": 13.389164924621582,
      "learning_rate": 2.8097560975609758e-05,
      "loss": 1.6348,
      "step": 1650
    },
    {
      "epoch": 1.6195121951219513,
      "grad_norm": 8.332003593444824,
      "learning_rate": 2.806829268292683e-05,
      "loss": 1.5444,
      "step": 1660
    },
    {
      "epoch": 1.629268292682927,
      "grad_norm": 5.3332414627075195,
      "learning_rate": 2.8039024390243904e-05,
      "loss": 1.165,
      "step": 1670
    },
    {
      "epoch": 1.6390243902439026,
      "grad_norm": 9.216840744018555,
      "learning_rate": 2.8009756097560977e-05,
      "loss": 1.7264,
      "step": 1680
    },
    {
      "epoch": 1.6487804878048782,
      "grad_norm": 9.365610122680664,
      "learning_rate": 2.798048780487805e-05,
      "loss": 1.6401,
      "step": 1690
    },
    {
      "epoch": 1.6585365853658538,
      "grad_norm": 7.251147747039795,
      "learning_rate": 2.7951219512195124e-05,
      "loss": 1.6602,
      "step": 1700
    },
    {
      "epoch": 1.6682926829268294,
      "grad_norm": 13.828434944152832,
      "learning_rate": 2.7921951219512197e-05,
      "loss": 1.453,
      "step": 1710
    },
    {
      "epoch": 1.678048780487805,
      "grad_norm": 8.703268051147461,
      "learning_rate": 2.789268292682927e-05,
      "loss": 1.4056,
      "step": 1720
    },
    {
      "epoch": 1.6878048780487804,
      "grad_norm": 7.503607273101807,
      "learning_rate": 2.7863414634146344e-05,
      "loss": 1.4871,
      "step": 1730
    },
    {
      "epoch": 1.697560975609756,
      "grad_norm": 7.914821624755859,
      "learning_rate": 2.7834146341463417e-05,
      "loss": 1.4349,
      "step": 1740
    },
    {
      "epoch": 1.7073170731707317,
      "grad_norm": 7.158156394958496,
      "learning_rate": 2.780487804878049e-05,
      "loss": 1.6146,
      "step": 1750
    },
    {
      "epoch": 1.7170731707317073,
      "grad_norm": 9.923163414001465,
      "learning_rate": 2.7775609756097564e-05,
      "loss": 1.517,
      "step": 1760
    },
    {
      "epoch": 1.726829268292683,
      "grad_norm": 7.473989009857178,
      "learning_rate": 2.7746341463414637e-05,
      "loss": 1.4612,
      "step": 1770
    },
    {
      "epoch": 1.7365853658536585,
      "grad_norm": 6.558455467224121,
      "learning_rate": 2.771707317073171e-05,
      "loss": 1.4873,
      "step": 1780
    },
    {
      "epoch": 1.7463414634146341,
      "grad_norm": 5.9157023429870605,
      "learning_rate": 2.7687804878048784e-05,
      "loss": 1.2397,
      "step": 1790
    },
    {
      "epoch": 1.7560975609756098,
      "grad_norm": 6.682568550109863,
      "learning_rate": 2.7658536585365857e-05,
      "loss": 1.5518,
      "step": 1800
    },
    {
      "epoch": 1.7658536585365854,
      "grad_norm": 9.516570091247559,
      "learning_rate": 2.762926829268293e-05,
      "loss": 1.3295,
      "step": 1810
    },
    {
      "epoch": 1.775609756097561,
      "grad_norm": 10.188446998596191,
      "learning_rate": 2.7600000000000003e-05,
      "loss": 1.6439,
      "step": 1820
    },
    {
      "epoch": 1.7853658536585366,
      "grad_norm": 13.080517768859863,
      "learning_rate": 2.7570731707317077e-05,
      "loss": 1.4113,
      "step": 1830
    },
    {
      "epoch": 1.7951219512195122,
      "grad_norm": 8.936734199523926,
      "learning_rate": 2.7541463414634147e-05,
      "loss": 1.4219,
      "step": 1840
    },
    {
      "epoch": 1.8048780487804879,
      "grad_norm": 8.219959259033203,
      "learning_rate": 2.751219512195122e-05,
      "loss": 1.4584,
      "step": 1850
    },
    {
      "epoch": 1.8146341463414632,
      "grad_norm": 38.39134216308594,
      "learning_rate": 2.7482926829268293e-05,
      "loss": 1.3296,
      "step": 1860
    },
    {
      "epoch": 1.8243902439024389,
      "grad_norm": 6.507752895355225,
      "learning_rate": 2.7453658536585366e-05,
      "loss": 1.3676,
      "step": 1870
    },
    {
      "epoch": 1.8341463414634145,
      "grad_norm": 9.191829681396484,
      "learning_rate": 2.742439024390244e-05,
      "loss": 1.3715,
      "step": 1880
    },
    {
      "epoch": 1.84390243902439,
      "grad_norm": 6.417561054229736,
      "learning_rate": 2.7395121951219513e-05,
      "loss": 1.5101,
      "step": 1890
    },
    {
      "epoch": 1.8536585365853657,
      "grad_norm": 6.788885116577148,
      "learning_rate": 2.7365853658536586e-05,
      "loss": 1.4233,
      "step": 1900
    },
    {
      "epoch": 1.8634146341463413,
      "grad_norm": 7.630199909210205,
      "learning_rate": 2.733658536585366e-05,
      "loss": 1.5498,
      "step": 1910
    },
    {
      "epoch": 1.873170731707317,
      "grad_norm": 23.495088577270508,
      "learning_rate": 2.7307317073170733e-05,
      "loss": 1.3575,
      "step": 1920
    },
    {
      "epoch": 1.8829268292682926,
      "grad_norm": 25.151994705200195,
      "learning_rate": 2.7278048780487806e-05,
      "loss": 1.7386,
      "step": 1930
    },
    {
      "epoch": 1.8926829268292682,
      "grad_norm": 5.078600883483887,
      "learning_rate": 2.724878048780488e-05,
      "loss": 1.4371,
      "step": 1940
    },
    {
      "epoch": 1.9024390243902438,
      "grad_norm": 36.03864669799805,
      "learning_rate": 2.7219512195121953e-05,
      "loss": 1.3425,
      "step": 1950
    },
    {
      "epoch": 1.9121951219512194,
      "grad_norm": 22.524757385253906,
      "learning_rate": 2.7190243902439026e-05,
      "loss": 1.2157,
      "step": 1960
    },
    {
      "epoch": 1.921951219512195,
      "grad_norm": 7.352092742919922,
      "learning_rate": 2.71609756097561e-05,
      "loss": 1.3338,
      "step": 1970
    },
    {
      "epoch": 1.9317073170731707,
      "grad_norm": 8.563837051391602,
      "learning_rate": 2.7131707317073173e-05,
      "loss": 1.2697,
      "step": 1980
    },
    {
      "epoch": 1.9414634146341463,
      "grad_norm": 7.530854225158691,
      "learning_rate": 2.7102439024390246e-05,
      "loss": 1.4237,
      "step": 1990
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 20.94258689880371,
      "learning_rate": 2.707317073170732e-05,
      "loss": 1.3874,
      "step": 2000
    },
    {
      "epoch": 1.9609756097560975,
      "grad_norm": 6.8654022216796875,
      "learning_rate": 2.7043902439024392e-05,
      "loss": 1.3467,
      "step": 2010
    },
    {
      "epoch": 1.9707317073170731,
      "grad_norm": 11.135994911193848,
      "learning_rate": 2.7014634146341466e-05,
      "loss": 1.4189,
      "step": 2020
    },
    {
      "epoch": 1.9804878048780488,
      "grad_norm": 8.666733741760254,
      "learning_rate": 2.698536585365854e-05,
      "loss": 1.5262,
      "step": 2030
    },
    {
      "epoch": 1.9902439024390244,
      "grad_norm": 5.953673362731934,
      "learning_rate": 2.6956097560975612e-05,
      "loss": 1.1621,
      "step": 2040
    },
    {
      "epoch": 2.0,
      "grad_norm": 19.818737030029297,
      "learning_rate": 2.6926829268292686e-05,
      "loss": 1.4724,
      "step": 2050
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.7312766313552856,
      "eval_runtime": 2.5091,
      "eval_samples_per_second": 171.773,
      "eval_steps_per_second": 21.521,
      "step": 2050
    },
    {
      "epoch": 2.0097560975609756,
      "grad_norm": 13.470685005187988,
      "learning_rate": 2.689756097560976e-05,
      "loss": 1.2855,
      "step": 2060
    },
    {
      "epoch": 2.0195121951219512,
      "grad_norm": 9.242337226867676,
      "learning_rate": 2.6868292682926832e-05,
      "loss": 1.3871,
      "step": 2070
    },
    {
      "epoch": 2.029268292682927,
      "grad_norm": 7.388720512390137,
      "learning_rate": 2.6839024390243902e-05,
      "loss": 1.2312,
      "step": 2080
    },
    {
      "epoch": 2.0390243902439025,
      "grad_norm": 7.452484130859375,
      "learning_rate": 2.6809756097560975e-05,
      "loss": 1.191,
      "step": 2090
    },
    {
      "epoch": 2.048780487804878,
      "grad_norm": 14.520885467529297,
      "learning_rate": 2.678048780487805e-05,
      "loss": 1.4218,
      "step": 2100
    },
    {
      "epoch": 2.0585365853658537,
      "grad_norm": 9.448243141174316,
      "learning_rate": 2.6751219512195122e-05,
      "loss": 1.309,
      "step": 2110
    },
    {
      "epoch": 2.0682926829268293,
      "grad_norm": 6.259853839874268,
      "learning_rate": 2.6721951219512195e-05,
      "loss": 1.3801,
      "step": 2120
    },
    {
      "epoch": 2.078048780487805,
      "grad_norm": 9.608914375305176,
      "learning_rate": 2.669268292682927e-05,
      "loss": 1.1709,
      "step": 2130
    },
    {
      "epoch": 2.0878048780487806,
      "grad_norm": 4.630585193634033,
      "learning_rate": 2.6663414634146342e-05,
      "loss": 1.2902,
      "step": 2140
    },
    {
      "epoch": 2.097560975609756,
      "grad_norm": 9.338573455810547,
      "learning_rate": 2.6634146341463415e-05,
      "loss": 1.282,
      "step": 2150
    },
    {
      "epoch": 2.107317073170732,
      "grad_norm": 6.728259563446045,
      "learning_rate": 2.660487804878049e-05,
      "loss": 1.2522,
      "step": 2160
    },
    {
      "epoch": 2.1170731707317074,
      "grad_norm": 31.50692367553711,
      "learning_rate": 2.657560975609756e-05,
      "loss": 1.3627,
      "step": 2170
    },
    {
      "epoch": 2.126829268292683,
      "grad_norm": 6.497741222381592,
      "learning_rate": 2.6546341463414635e-05,
      "loss": 1.2293,
      "step": 2180
    },
    {
      "epoch": 2.1365853658536587,
      "grad_norm": 12.479959487915039,
      "learning_rate": 2.6517073170731708e-05,
      "loss": 1.2279,
      "step": 2190
    },
    {
      "epoch": 2.1463414634146343,
      "grad_norm": 15.84811019897461,
      "learning_rate": 2.648780487804878e-05,
      "loss": 1.3163,
      "step": 2200
    },
    {
      "epoch": 2.15609756097561,
      "grad_norm": 6.802453517913818,
      "learning_rate": 2.6458536585365855e-05,
      "loss": 1.188,
      "step": 2210
    },
    {
      "epoch": 2.1658536585365855,
      "grad_norm": 9.911239624023438,
      "learning_rate": 2.6429268292682928e-05,
      "loss": 1.2318,
      "step": 2220
    },
    {
      "epoch": 2.175609756097561,
      "grad_norm": 9.380725860595703,
      "learning_rate": 2.64e-05,
      "loss": 1.2821,
      "step": 2230
    },
    {
      "epoch": 2.1853658536585368,
      "grad_norm": 10.149168014526367,
      "learning_rate": 2.6370731707317075e-05,
      "loss": 1.3818,
      "step": 2240
    },
    {
      "epoch": 2.1951219512195124,
      "grad_norm": 67.44275665283203,
      "learning_rate": 2.6341463414634148e-05,
      "loss": 1.2371,
      "step": 2250
    },
    {
      "epoch": 2.204878048780488,
      "grad_norm": 7.399629592895508,
      "learning_rate": 2.631219512195122e-05,
      "loss": 1.459,
      "step": 2260
    },
    {
      "epoch": 2.2146341463414636,
      "grad_norm": 5.8982954025268555,
      "learning_rate": 2.6282926829268294e-05,
      "loss": 1.1718,
      "step": 2270
    },
    {
      "epoch": 2.2243902439024392,
      "grad_norm": 6.85012149810791,
      "learning_rate": 2.6253658536585368e-05,
      "loss": 1.2981,
      "step": 2280
    },
    {
      "epoch": 2.234146341463415,
      "grad_norm": 6.636291027069092,
      "learning_rate": 2.622439024390244e-05,
      "loss": 1.1701,
      "step": 2290
    },
    {
      "epoch": 2.2439024390243905,
      "grad_norm": 6.843070030212402,
      "learning_rate": 2.6195121951219514e-05,
      "loss": 1.0741,
      "step": 2300
    },
    {
      "epoch": 2.253658536585366,
      "grad_norm": 5.198065280914307,
      "learning_rate": 2.6165853658536588e-05,
      "loss": 1.3166,
      "step": 2310
    },
    {
      "epoch": 2.2634146341463417,
      "grad_norm": 353.5003662109375,
      "learning_rate": 2.6136585365853658e-05,
      "loss": 1.3497,
      "step": 2320
    },
    {
      "epoch": 2.2731707317073173,
      "grad_norm": 5.884786605834961,
      "learning_rate": 2.610731707317073e-05,
      "loss": 1.0907,
      "step": 2330
    },
    {
      "epoch": 2.2829268292682925,
      "grad_norm": 10.181726455688477,
      "learning_rate": 2.6078048780487804e-05,
      "loss": 1.3116,
      "step": 2340
    },
    {
      "epoch": 2.292682926829268,
      "grad_norm": 6.477316379547119,
      "learning_rate": 2.6048780487804877e-05,
      "loss": 1.3168,
      "step": 2350
    },
    {
      "epoch": 2.3024390243902437,
      "grad_norm": 9.485061645507812,
      "learning_rate": 2.601951219512195e-05,
      "loss": 1.3443,
      "step": 2360
    },
    {
      "epoch": 2.3121951219512193,
      "grad_norm": 5.824450492858887,
      "learning_rate": 2.5990243902439024e-05,
      "loss": 1.2291,
      "step": 2370
    },
    {
      "epoch": 2.321951219512195,
      "grad_norm": 7.117096900939941,
      "learning_rate": 2.5960975609756097e-05,
      "loss": 1.0489,
      "step": 2380
    },
    {
      "epoch": 2.3317073170731706,
      "grad_norm": 7.905430316925049,
      "learning_rate": 2.593170731707317e-05,
      "loss": 1.2939,
      "step": 2390
    },
    {
      "epoch": 2.341463414634146,
      "grad_norm": 7.629115104675293,
      "learning_rate": 2.5902439024390244e-05,
      "loss": 1.1923,
      "step": 2400
    },
    {
      "epoch": 2.351219512195122,
      "grad_norm": 7.715949058532715,
      "learning_rate": 2.5873170731707317e-05,
      "loss": 1.2602,
      "step": 2410
    },
    {
      "epoch": 2.3609756097560974,
      "grad_norm": 5.501751899719238,
      "learning_rate": 2.584390243902439e-05,
      "loss": 1.0693,
      "step": 2420
    },
    {
      "epoch": 2.370731707317073,
      "grad_norm": 8.857738494873047,
      "learning_rate": 2.5814634146341464e-05,
      "loss": 1.4284,
      "step": 2430
    },
    {
      "epoch": 2.3804878048780487,
      "grad_norm": 6.406196594238281,
      "learning_rate": 2.5785365853658537e-05,
      "loss": 1.0472,
      "step": 2440
    },
    {
      "epoch": 2.3902439024390243,
      "grad_norm": 8.274758338928223,
      "learning_rate": 2.575609756097561e-05,
      "loss": 1.1124,
      "step": 2450
    },
    {
      "epoch": 2.4,
      "grad_norm": 5.467721939086914,
      "learning_rate": 2.5726829268292684e-05,
      "loss": 1.0961,
      "step": 2460
    },
    {
      "epoch": 2.4097560975609755,
      "grad_norm": 5.693801403045654,
      "learning_rate": 2.5697560975609757e-05,
      "loss": 1.2683,
      "step": 2470
    },
    {
      "epoch": 2.419512195121951,
      "grad_norm": 5.489632606506348,
      "learning_rate": 2.566829268292683e-05,
      "loss": 1.219,
      "step": 2480
    },
    {
      "epoch": 2.4292682926829268,
      "grad_norm": 6.334614276885986,
      "learning_rate": 2.5639024390243903e-05,
      "loss": 1.0957,
      "step": 2490
    },
    {
      "epoch": 2.4390243902439024,
      "grad_norm": 6.333797454833984,
      "learning_rate": 2.5609756097560977e-05,
      "loss": 1.16,
      "step": 2500
    },
    {
      "epoch": 2.448780487804878,
      "grad_norm": 5.257286071777344,
      "learning_rate": 2.558048780487805e-05,
      "loss": 1.1463,
      "step": 2510
    },
    {
      "epoch": 2.4585365853658536,
      "grad_norm": 5.965444564819336,
      "learning_rate": 2.5551219512195123e-05,
      "loss": 1.0203,
      "step": 2520
    },
    {
      "epoch": 2.4682926829268292,
      "grad_norm": 8.500627517700195,
      "learning_rate": 2.5521951219512197e-05,
      "loss": 1.3259,
      "step": 2530
    },
    {
      "epoch": 2.478048780487805,
      "grad_norm": 12.841192245483398,
      "learning_rate": 2.549268292682927e-05,
      "loss": 1.204,
      "step": 2540
    },
    {
      "epoch": 2.4878048780487805,
      "grad_norm": 7.230868339538574,
      "learning_rate": 2.5463414634146343e-05,
      "loss": 1.0942,
      "step": 2550
    },
    {
      "epoch": 2.497560975609756,
      "grad_norm": 5.120028972625732,
      "learning_rate": 2.5434146341463413e-05,
      "loss": 1.1731,
      "step": 2560
    },
    {
      "epoch": 2.5073170731707317,
      "grad_norm": 7.000409126281738,
      "learning_rate": 2.5404878048780486e-05,
      "loss": 1.0323,
      "step": 2570
    },
    {
      "epoch": 2.5170731707317073,
      "grad_norm": 6.1048970222473145,
      "learning_rate": 2.537560975609756e-05,
      "loss": 1.0089,
      "step": 2580
    },
    {
      "epoch": 2.526829268292683,
      "grad_norm": 6.313553333282471,
      "learning_rate": 2.5346341463414633e-05,
      "loss": 1.1386,
      "step": 2590
    },
    {
      "epoch": 2.5365853658536586,
      "grad_norm": 7.979403495788574,
      "learning_rate": 2.5317073170731706e-05,
      "loss": 1.1313,
      "step": 2600
    },
    {
      "epoch": 2.546341463414634,
      "grad_norm": 6.842154026031494,
      "learning_rate": 2.528780487804878e-05,
      "loss": 1.3017,
      "step": 2610
    },
    {
      "epoch": 2.55609756097561,
      "grad_norm": 6.176662445068359,
      "learning_rate": 2.5258536585365853e-05,
      "loss": 1.312,
      "step": 2620
    },
    {
      "epoch": 2.5658536585365854,
      "grad_norm": 6.169057369232178,
      "learning_rate": 2.5229268292682926e-05,
      "loss": 1.0297,
      "step": 2630
    },
    {
      "epoch": 2.575609756097561,
      "grad_norm": 7.852907180786133,
      "learning_rate": 2.52e-05,
      "loss": 1.2778,
      "step": 2640
    },
    {
      "epoch": 2.5853658536585367,
      "grad_norm": 6.7021894454956055,
      "learning_rate": 2.5170731707317073e-05,
      "loss": 1.0871,
      "step": 2650
    },
    {
      "epoch": 2.5951219512195123,
      "grad_norm": 7.211501598358154,
      "learning_rate": 2.5141463414634146e-05,
      "loss": 1.0506,
      "step": 2660
    },
    {
      "epoch": 2.604878048780488,
      "grad_norm": 7.655062675476074,
      "learning_rate": 2.511219512195122e-05,
      "loss": 1.137,
      "step": 2670
    },
    {
      "epoch": 2.6146341463414635,
      "grad_norm": 8.20338249206543,
      "learning_rate": 2.5082926829268292e-05,
      "loss": 1.1415,
      "step": 2680
    },
    {
      "epoch": 2.624390243902439,
      "grad_norm": 8.617607116699219,
      "learning_rate": 2.5053658536585366e-05,
      "loss": 1.0045,
      "step": 2690
    },
    {
      "epoch": 2.6341463414634148,
      "grad_norm": 4.7180705070495605,
      "learning_rate": 2.502439024390244e-05,
      "loss": 1.1012,
      "step": 2700
    },
    {
      "epoch": 2.6439024390243904,
      "grad_norm": 44.14564514160156,
      "learning_rate": 2.4995121951219512e-05,
      "loss": 1.0162,
      "step": 2710
    },
    {
      "epoch": 2.653658536585366,
      "grad_norm": 9.273993492126465,
      "learning_rate": 2.4965853658536586e-05,
      "loss": 1.183,
      "step": 2720
    },
    {
      "epoch": 2.6634146341463416,
      "grad_norm": 4.832876682281494,
      "learning_rate": 2.493658536585366e-05,
      "loss": 1.0664,
      "step": 2730
    },
    {
      "epoch": 2.6731707317073172,
      "grad_norm": 7.548704147338867,
      "learning_rate": 2.4907317073170732e-05,
      "loss": 0.9346,
      "step": 2740
    },
    {
      "epoch": 2.682926829268293,
      "grad_norm": 4.750675201416016,
      "learning_rate": 2.4878048780487805e-05,
      "loss": 1.0303,
      "step": 2750
    },
    {
      "epoch": 2.692682926829268,
      "grad_norm": 5.807455539703369,
      "learning_rate": 2.484878048780488e-05,
      "loss": 1.1835,
      "step": 2760
    },
    {
      "epoch": 2.7024390243902436,
      "grad_norm": 6.987988471984863,
      "learning_rate": 2.4819512195121952e-05,
      "loss": 1.2054,
      "step": 2770
    },
    {
      "epoch": 2.7121951219512193,
      "grad_norm": 7.281709671020508,
      "learning_rate": 2.4790243902439025e-05,
      "loss": 1.1623,
      "step": 2780
    },
    {
      "epoch": 2.721951219512195,
      "grad_norm": 6.736603260040283,
      "learning_rate": 2.4760975609756095e-05,
      "loss": 1.1699,
      "step": 2790
    },
    {
      "epoch": 2.7317073170731705,
      "grad_norm": 6.051003932952881,
      "learning_rate": 2.473170731707317e-05,
      "loss": 0.9388,
      "step": 2800
    },
    {
      "epoch": 2.741463414634146,
      "grad_norm": 5.008315563201904,
      "learning_rate": 2.4702439024390242e-05,
      "loss": 1.1784,
      "step": 2810
    },
    {
      "epoch": 2.7512195121951217,
      "grad_norm": 6.9502763748168945,
      "learning_rate": 2.4673170731707315e-05,
      "loss": 1.0223,
      "step": 2820
    },
    {
      "epoch": 2.7609756097560973,
      "grad_norm": 4.947101593017578,
      "learning_rate": 2.464390243902439e-05,
      "loss": 1.0312,
      "step": 2830
    },
    {
      "epoch": 2.770731707317073,
      "grad_norm": 7.750904560089111,
      "learning_rate": 2.461463414634146e-05,
      "loss": 1.0173,
      "step": 2840
    },
    {
      "epoch": 2.7804878048780486,
      "grad_norm": 4.137866973876953,
      "learning_rate": 2.4585365853658535e-05,
      "loss": 1.1853,
      "step": 2850
    },
    {
      "epoch": 2.790243902439024,
      "grad_norm": 5.082206726074219,
      "learning_rate": 2.4556097560975608e-05,
      "loss": 1.0278,
      "step": 2860
    },
    {
      "epoch": 2.8,
      "grad_norm": 4.747442722320557,
      "learning_rate": 2.452682926829268e-05,
      "loss": 0.8993,
      "step": 2870
    },
    {
      "epoch": 2.8097560975609754,
      "grad_norm": 7.5195817947387695,
      "learning_rate": 2.4497560975609755e-05,
      "loss": 1.0495,
      "step": 2880
    },
    {
      "epoch": 2.819512195121951,
      "grad_norm": 7.538872241973877,
      "learning_rate": 2.4468292682926828e-05,
      "loss": 0.9882,
      "step": 2890
    },
    {
      "epoch": 2.8292682926829267,
      "grad_norm": 4.194428443908691,
      "learning_rate": 2.44390243902439e-05,
      "loss": 1.0174,
      "step": 2900
    },
    {
      "epoch": 2.8390243902439023,
      "grad_norm": 15.37881851196289,
      "learning_rate": 2.4409756097560975e-05,
      "loss": 1.1695,
      "step": 2910
    },
    {
      "epoch": 2.848780487804878,
      "grad_norm": 4.429619789123535,
      "learning_rate": 2.4380487804878048e-05,
      "loss": 1.1419,
      "step": 2920
    },
    {
      "epoch": 2.8585365853658535,
      "grad_norm": 4.1669721603393555,
      "learning_rate": 2.4351219512195125e-05,
      "loss": 1.0618,
      "step": 2930
    },
    {
      "epoch": 2.868292682926829,
      "grad_norm": 8.28670883178711,
      "learning_rate": 2.4321951219512198e-05,
      "loss": 0.9856,
      "step": 2940
    },
    {
      "epoch": 2.8780487804878048,
      "grad_norm": 5.4731879234313965,
      "learning_rate": 2.429268292682927e-05,
      "loss": 1.1907,
      "step": 2950
    },
    {
      "epoch": 2.8878048780487804,
      "grad_norm": 5.375580310821533,
      "learning_rate": 2.4263414634146344e-05,
      "loss": 1.1737,
      "step": 2960
    },
    {
      "epoch": 2.897560975609756,
      "grad_norm": 5.028619289398193,
      "learning_rate": 2.4234146341463418e-05,
      "loss": 0.9393,
      "step": 2970
    },
    {
      "epoch": 2.9073170731707316,
      "grad_norm": 8.386882781982422,
      "learning_rate": 2.420487804878049e-05,
      "loss": 1.1333,
      "step": 2980
    },
    {
      "epoch": 2.9170731707317072,
      "grad_norm": 3.244149923324585,
      "learning_rate": 2.4175609756097564e-05,
      "loss": 0.9852,
      "step": 2990
    },
    {
      "epoch": 2.926829268292683,
      "grad_norm": 4.649903297424316,
      "learning_rate": 2.4146341463414638e-05,
      "loss": 1.1113,
      "step": 3000
    },
    {
      "epoch": 2.9365853658536585,
      "grad_norm": 6.607156753540039,
      "learning_rate": 2.411707317073171e-05,
      "loss": 1.2004,
      "step": 3010
    },
    {
      "epoch": 2.946341463414634,
      "grad_norm": 4.731781005859375,
      "learning_rate": 2.4087804878048784e-05,
      "loss": 1.3161,
      "step": 3020
    },
    {
      "epoch": 2.9560975609756097,
      "grad_norm": 6.239696025848389,
      "learning_rate": 2.4058536585365854e-05,
      "loss": 0.9093,
      "step": 3030
    },
    {
      "epoch": 2.9658536585365853,
      "grad_norm": 4.1548848152160645,
      "learning_rate": 2.4029268292682927e-05,
      "loss": 0.8719,
      "step": 3040
    },
    {
      "epoch": 2.975609756097561,
      "grad_norm": 5.919632911682129,
      "learning_rate": 2.4e-05,
      "loss": 1.1448,
      "step": 3050
    },
    {
      "epoch": 2.9853658536585366,
      "grad_norm": 118.2912826538086,
      "learning_rate": 2.3970731707317074e-05,
      "loss": 1.0603,
      "step": 3060
    },
    {
      "epoch": 2.995121951219512,
      "grad_norm": 38.41066360473633,
      "learning_rate": 2.3941463414634147e-05,
      "loss": 1.1515,
      "step": 3070
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.5934591889381409,
      "eval_runtime": 2.4487,
      "eval_samples_per_second": 176.014,
      "eval_steps_per_second": 22.053,
      "step": 3075
    },
    {
      "epoch": 3.004878048780488,
      "grad_norm": 12.240274429321289,
      "learning_rate": 2.391219512195122e-05,
      "loss": 1.0516,
      "step": 3080
    },
    {
      "epoch": 3.0146341463414634,
      "grad_norm": 6.69839334487915,
      "learning_rate": 2.3882926829268294e-05,
      "loss": 0.8941,
      "step": 3090
    },
    {
      "epoch": 3.024390243902439,
      "grad_norm": 3.612974166870117,
      "learning_rate": 2.3853658536585367e-05,
      "loss": 1.0247,
      "step": 3100
    },
    {
      "epoch": 3.0341463414634147,
      "grad_norm": 4.692595958709717,
      "learning_rate": 2.382439024390244e-05,
      "loss": 0.9733,
      "step": 3110
    },
    {
      "epoch": 3.0439024390243903,
      "grad_norm": 6.016700267791748,
      "learning_rate": 2.3795121951219514e-05,
      "loss": 0.8451,
      "step": 3120
    },
    {
      "epoch": 3.053658536585366,
      "grad_norm": 4.420607089996338,
      "learning_rate": 2.3765853658536587e-05,
      "loss": 0.9689,
      "step": 3130
    },
    {
      "epoch": 3.0634146341463415,
      "grad_norm": 6.228527069091797,
      "learning_rate": 2.373658536585366e-05,
      "loss": 1.0186,
      "step": 3140
    },
    {
      "epoch": 3.073170731707317,
      "grad_norm": 4.754893779754639,
      "learning_rate": 2.3707317073170733e-05,
      "loss": 0.9264,
      "step": 3150
    },
    {
      "epoch": 3.0829268292682928,
      "grad_norm": 3.7534968852996826,
      "learning_rate": 2.3678048780487807e-05,
      "loss": 1.0549,
      "step": 3160
    },
    {
      "epoch": 3.0926829268292684,
      "grad_norm": 3.6030845642089844,
      "learning_rate": 2.364878048780488e-05,
      "loss": 0.8505,
      "step": 3170
    },
    {
      "epoch": 3.102439024390244,
      "grad_norm": 5.3754377365112305,
      "learning_rate": 2.3619512195121953e-05,
      "loss": 1.2905,
      "step": 3180
    },
    {
      "epoch": 3.1121951219512196,
      "grad_norm": 8.11376667022705,
      "learning_rate": 2.3590243902439027e-05,
      "loss": 0.9521,
      "step": 3190
    },
    {
      "epoch": 3.1219512195121952,
      "grad_norm": 5.454588413238525,
      "learning_rate": 2.35609756097561e-05,
      "loss": 1.0001,
      "step": 3200
    },
    {
      "epoch": 3.131707317073171,
      "grad_norm": 6.834694862365723,
      "learning_rate": 2.3531707317073173e-05,
      "loss": 1.1799,
      "step": 3210
    },
    {
      "epoch": 3.1414634146341465,
      "grad_norm": 5.9632954597473145,
      "learning_rate": 2.3502439024390246e-05,
      "loss": 1.0615,
      "step": 3220
    },
    {
      "epoch": 3.151219512195122,
      "grad_norm": 5.476807594299316,
      "learning_rate": 2.347317073170732e-05,
      "loss": 0.9849,
      "step": 3230
    },
    {
      "epoch": 3.1609756097560977,
      "grad_norm": 5.621461391448975,
      "learning_rate": 2.3443902439024393e-05,
      "loss": 0.9572,
      "step": 3240
    },
    {
      "epoch": 3.1707317073170733,
      "grad_norm": 3.116405963897705,
      "learning_rate": 2.3414634146341466e-05,
      "loss": 0.8631,
      "step": 3250
    },
    {
      "epoch": 3.180487804878049,
      "grad_norm": 5.97628116607666,
      "learning_rate": 2.338536585365854e-05,
      "loss": 1.1257,
      "step": 3260
    },
    {
      "epoch": 3.1902439024390246,
      "grad_norm": 9.784906387329102,
      "learning_rate": 2.335609756097561e-05,
      "loss": 1.2515,
      "step": 3270
    },
    {
      "epoch": 3.2,
      "grad_norm": 6.187481880187988,
      "learning_rate": 2.3326829268292683e-05,
      "loss": 0.8927,
      "step": 3280
    },
    {
      "epoch": 3.209756097560976,
      "grad_norm": 5.1543684005737305,
      "learning_rate": 2.3297560975609756e-05,
      "loss": 0.9129,
      "step": 3290
    },
    {
      "epoch": 3.2195121951219514,
      "grad_norm": 93.96582794189453,
      "learning_rate": 2.326829268292683e-05,
      "loss": 1.0552,
      "step": 3300
    },
    {
      "epoch": 3.229268292682927,
      "grad_norm": 8.903048515319824,
      "learning_rate": 2.3239024390243903e-05,
      "loss": 0.856,
      "step": 3310
    },
    {
      "epoch": 3.2390243902439027,
      "grad_norm": 4.22390604019165,
      "learning_rate": 2.3209756097560976e-05,
      "loss": 0.91,
      "step": 3320
    },
    {
      "epoch": 3.2487804878048783,
      "grad_norm": 7.861635684967041,
      "learning_rate": 2.318048780487805e-05,
      "loss": 0.9081,
      "step": 3330
    },
    {
      "epoch": 3.258536585365854,
      "grad_norm": 6.155749320983887,
      "learning_rate": 2.3151219512195123e-05,
      "loss": 0.7805,
      "step": 3340
    },
    {
      "epoch": 3.2682926829268295,
      "grad_norm": 6.445687770843506,
      "learning_rate": 2.3121951219512196e-05,
      "loss": 1.0809,
      "step": 3350
    },
    {
      "epoch": 3.278048780487805,
      "grad_norm": 10.18912124633789,
      "learning_rate": 2.309268292682927e-05,
      "loss": 0.9558,
      "step": 3360
    },
    {
      "epoch": 3.2878048780487803,
      "grad_norm": 4.518614292144775,
      "learning_rate": 2.3063414634146342e-05,
      "loss": 0.8516,
      "step": 3370
    },
    {
      "epoch": 3.297560975609756,
      "grad_norm": 10.832852363586426,
      "learning_rate": 2.3034146341463416e-05,
      "loss": 0.983,
      "step": 3380
    },
    {
      "epoch": 3.3073170731707315,
      "grad_norm": 5.819479465484619,
      "learning_rate": 2.300487804878049e-05,
      "loss": 0.9516,
      "step": 3390
    },
    {
      "epoch": 3.317073170731707,
      "grad_norm": 4.9444427490234375,
      "learning_rate": 2.2975609756097562e-05,
      "loss": 0.8441,
      "step": 3400
    },
    {
      "epoch": 3.3268292682926828,
      "grad_norm": 4.42572546005249,
      "learning_rate": 2.2946341463414636e-05,
      "loss": 0.8608,
      "step": 3410
    },
    {
      "epoch": 3.3365853658536584,
      "grad_norm": 4.824013710021973,
      "learning_rate": 2.291707317073171e-05,
      "loss": 0.9884,
      "step": 3420
    },
    {
      "epoch": 3.346341463414634,
      "grad_norm": 5.809599876403809,
      "learning_rate": 2.2887804878048782e-05,
      "loss": 0.8182,
      "step": 3430
    },
    {
      "epoch": 3.3560975609756096,
      "grad_norm": 3.667370557785034,
      "learning_rate": 2.2858536585365855e-05,
      "loss": 1.0308,
      "step": 3440
    },
    {
      "epoch": 3.3658536585365852,
      "grad_norm": 7.393751621246338,
      "learning_rate": 2.282926829268293e-05,
      "loss": 1.0452,
      "step": 3450
    },
    {
      "epoch": 3.375609756097561,
      "grad_norm": 7.171334266662598,
      "learning_rate": 2.2800000000000002e-05,
      "loss": 1.0516,
      "step": 3460
    },
    {
      "epoch": 3.3853658536585365,
      "grad_norm": 5.022317886352539,
      "learning_rate": 2.2770731707317075e-05,
      "loss": 1.1677,
      "step": 3470
    },
    {
      "epoch": 3.395121951219512,
      "grad_norm": 6.004487991333008,
      "learning_rate": 2.274146341463415e-05,
      "loss": 0.7247,
      "step": 3480
    },
    {
      "epoch": 3.4048780487804877,
      "grad_norm": 3.8700053691864014,
      "learning_rate": 2.2712195121951222e-05,
      "loss": 0.9494,
      "step": 3490
    },
    {
      "epoch": 3.4146341463414633,
      "grad_norm": 3.034752130508423,
      "learning_rate": 2.2682926829268295e-05,
      "loss": 0.8507,
      "step": 3500
    },
    {
      "epoch": 3.424390243902439,
      "grad_norm": 4.654449462890625,
      "learning_rate": 2.2653658536585365e-05,
      "loss": 0.7922,
      "step": 3510
    },
    {
      "epoch": 3.4341463414634146,
      "grad_norm": 4.330408096313477,
      "learning_rate": 2.2624390243902438e-05,
      "loss": 0.8691,
      "step": 3520
    },
    {
      "epoch": 3.44390243902439,
      "grad_norm": 4.3159027099609375,
      "learning_rate": 2.259512195121951e-05,
      "loss": 1.0917,
      "step": 3530
    },
    {
      "epoch": 3.453658536585366,
      "grad_norm": 5.756659507751465,
      "learning_rate": 2.2565853658536585e-05,
      "loss": 0.9273,
      "step": 3540
    },
    {
      "epoch": 3.4634146341463414,
      "grad_norm": 15.113730430603027,
      "learning_rate": 2.2536585365853658e-05,
      "loss": 1.0305,
      "step": 3550
    },
    {
      "epoch": 3.473170731707317,
      "grad_norm": 6.342767238616943,
      "learning_rate": 2.250731707317073e-05,
      "loss": 0.8088,
      "step": 3560
    },
    {
      "epoch": 3.4829268292682927,
      "grad_norm": 3.6638872623443604,
      "learning_rate": 2.2478048780487805e-05,
      "loss": 0.8982,
      "step": 3570
    },
    {
      "epoch": 3.4926829268292683,
      "grad_norm": 9.675384521484375,
      "learning_rate": 2.2448780487804878e-05,
      "loss": 0.9414,
      "step": 3580
    },
    {
      "epoch": 3.502439024390244,
      "grad_norm": 6.06015682220459,
      "learning_rate": 2.241951219512195e-05,
      "loss": 0.8731,
      "step": 3590
    },
    {
      "epoch": 3.5121951219512195,
      "grad_norm": 3.999300241470337,
      "learning_rate": 2.2390243902439025e-05,
      "loss": 0.8065,
      "step": 3600
    },
    {
      "epoch": 3.521951219512195,
      "grad_norm": 5.597350120544434,
      "learning_rate": 2.2360975609756098e-05,
      "loss": 0.9048,
      "step": 3610
    },
    {
      "epoch": 3.5317073170731708,
      "grad_norm": 3.750627279281616,
      "learning_rate": 2.233170731707317e-05,
      "loss": 0.9332,
      "step": 3620
    },
    {
      "epoch": 3.5414634146341464,
      "grad_norm": 5.924662113189697,
      "learning_rate": 2.2302439024390244e-05,
      "loss": 0.8649,
      "step": 3630
    },
    {
      "epoch": 3.551219512195122,
      "grad_norm": 3.7948834896087646,
      "learning_rate": 2.2273170731707318e-05,
      "loss": 0.9314,
      "step": 3640
    },
    {
      "epoch": 3.5609756097560976,
      "grad_norm": 3.508833169937134,
      "learning_rate": 2.224390243902439e-05,
      "loss": 0.8792,
      "step": 3650
    },
    {
      "epoch": 3.5707317073170732,
      "grad_norm": 4.379298210144043,
      "learning_rate": 2.2214634146341464e-05,
      "loss": 0.9148,
      "step": 3660
    },
    {
      "epoch": 3.580487804878049,
      "grad_norm": 7.966890335083008,
      "learning_rate": 2.2185365853658538e-05,
      "loss": 1.1108,
      "step": 3670
    },
    {
      "epoch": 3.5902439024390245,
      "grad_norm": 5.897153854370117,
      "learning_rate": 2.215609756097561e-05,
      "loss": 0.8915,
      "step": 3680
    },
    {
      "epoch": 3.6,
      "grad_norm": 3.789283037185669,
      "learning_rate": 2.2126829268292684e-05,
      "loss": 0.866,
      "step": 3690
    },
    {
      "epoch": 3.6097560975609757,
      "grad_norm": 16.9852294921875,
      "learning_rate": 2.2097560975609757e-05,
      "loss": 1.0352,
      "step": 3700
    },
    {
      "epoch": 3.6195121951219513,
      "grad_norm": 13.425381660461426,
      "learning_rate": 2.206829268292683e-05,
      "loss": 0.9248,
      "step": 3710
    },
    {
      "epoch": 3.629268292682927,
      "grad_norm": 4.208598613739014,
      "learning_rate": 2.2039024390243904e-05,
      "loss": 0.8857,
      "step": 3720
    },
    {
      "epoch": 3.6390243902439026,
      "grad_norm": 10.301136016845703,
      "learning_rate": 2.2009756097560977e-05,
      "loss": 0.8898,
      "step": 3730
    },
    {
      "epoch": 3.648780487804878,
      "grad_norm": 6.846684455871582,
      "learning_rate": 2.198048780487805e-05,
      "loss": 0.8435,
      "step": 3740
    },
    {
      "epoch": 3.658536585365854,
      "grad_norm": 3.7762250900268555,
      "learning_rate": 2.195121951219512e-05,
      "loss": 1.0998,
      "step": 3750
    },
    {
      "epoch": 3.6682926829268294,
      "grad_norm": 5.727950572967529,
      "learning_rate": 2.1921951219512194e-05,
      "loss": 0.9634,
      "step": 3760
    },
    {
      "epoch": 3.678048780487805,
      "grad_norm": 6.2630133628845215,
      "learning_rate": 2.1892682926829267e-05,
      "loss": 1.1056,
      "step": 3770
    },
    {
      "epoch": 3.68780487804878,
      "grad_norm": 6.9889140129089355,
      "learning_rate": 2.186341463414634e-05,
      "loss": 0.8442,
      "step": 3780
    },
    {
      "epoch": 3.697560975609756,
      "grad_norm": 4.283371925354004,
      "learning_rate": 2.1834146341463414e-05,
      "loss": 1.0191,
      "step": 3790
    },
    {
      "epoch": 3.7073170731707314,
      "grad_norm": 8.609506607055664,
      "learning_rate": 2.1804878048780487e-05,
      "loss": 0.9622,
      "step": 3800
    },
    {
      "epoch": 3.717073170731707,
      "grad_norm": 12.314787864685059,
      "learning_rate": 2.177560975609756e-05,
      "loss": 1.0636,
      "step": 3810
    },
    {
      "epoch": 3.7268292682926827,
      "grad_norm": 4.34650182723999,
      "learning_rate": 2.1746341463414633e-05,
      "loss": 0.9423,
      "step": 3820
    },
    {
      "epoch": 3.7365853658536583,
      "grad_norm": 4.675287246704102,
      "learning_rate": 2.1717073170731707e-05,
      "loss": 0.8141,
      "step": 3830
    },
    {
      "epoch": 3.746341463414634,
      "grad_norm": 19.353015899658203,
      "learning_rate": 2.168780487804878e-05,
      "loss": 0.8719,
      "step": 3840
    },
    {
      "epoch": 3.7560975609756095,
      "grad_norm": 5.321389675140381,
      "learning_rate": 2.1658536585365853e-05,
      "loss": 0.9124,
      "step": 3850
    },
    {
      "epoch": 3.765853658536585,
      "grad_norm": 4.6952056884765625,
      "learning_rate": 2.1629268292682927e-05,
      "loss": 1.0243,
      "step": 3860
    },
    {
      "epoch": 3.7756097560975608,
      "grad_norm": 6.238461494445801,
      "learning_rate": 2.16e-05,
      "loss": 0.8944,
      "step": 3870
    },
    {
      "epoch": 3.7853658536585364,
      "grad_norm": 3.824191093444824,
      "learning_rate": 2.1570731707317073e-05,
      "loss": 1.105,
      "step": 3880
    },
    {
      "epoch": 3.795121951219512,
      "grad_norm": 5.917962551116943,
      "learning_rate": 2.1541463414634146e-05,
      "loss": 1.0716,
      "step": 3890
    },
    {
      "epoch": 3.8048780487804876,
      "grad_norm": 5.742239475250244,
      "learning_rate": 2.151219512195122e-05,
      "loss": 1.0069,
      "step": 3900
    },
    {
      "epoch": 3.8146341463414632,
      "grad_norm": 14.698866844177246,
      "learning_rate": 2.1482926829268293e-05,
      "loss": 0.8687,
      "step": 3910
    },
    {
      "epoch": 3.824390243902439,
      "grad_norm": 4.640645503997803,
      "learning_rate": 2.1453658536585366e-05,
      "loss": 0.8935,
      "step": 3920
    },
    {
      "epoch": 3.8341463414634145,
      "grad_norm": 8.567912101745605,
      "learning_rate": 2.142439024390244e-05,
      "loss": 0.8779,
      "step": 3930
    },
    {
      "epoch": 3.84390243902439,
      "grad_norm": 4.410925388336182,
      "learning_rate": 2.1395121951219513e-05,
      "loss": 0.9773,
      "step": 3940
    },
    {
      "epoch": 3.8536585365853657,
      "grad_norm": 4.80672025680542,
      "learning_rate": 2.1365853658536586e-05,
      "loss": 0.8315,
      "step": 3950
    },
    {
      "epoch": 3.8634146341463413,
      "grad_norm": 7.989535808563232,
      "learning_rate": 2.133658536585366e-05,
      "loss": 0.9462,
      "step": 3960
    },
    {
      "epoch": 3.873170731707317,
      "grad_norm": 5.40386438369751,
      "learning_rate": 2.1307317073170733e-05,
      "loss": 0.8659,
      "step": 3970
    },
    {
      "epoch": 3.8829268292682926,
      "grad_norm": 11.439566612243652,
      "learning_rate": 2.1278048780487806e-05,
      "loss": 0.7889,
      "step": 3980
    },
    {
      "epoch": 3.892682926829268,
      "grad_norm": 5.376570701599121,
      "learning_rate": 2.1248780487804876e-05,
      "loss": 0.8311,
      "step": 3990
    },
    {
      "epoch": 3.902439024390244,
      "grad_norm": 4.285199165344238,
      "learning_rate": 2.121951219512195e-05,
      "loss": 0.7236,
      "step": 4000
    },
    {
      "epoch": 3.9121951219512194,
      "grad_norm": 4.237185955047607,
      "learning_rate": 2.1190243902439022e-05,
      "loss": 0.7684,
      "step": 4010
    },
    {
      "epoch": 3.921951219512195,
      "grad_norm": 5.8507914543151855,
      "learning_rate": 2.1160975609756096e-05,
      "loss": 0.7803,
      "step": 4020
    },
    {
      "epoch": 3.9317073170731707,
      "grad_norm": 5.4081573486328125,
      "learning_rate": 2.113170731707317e-05,
      "loss": 0.7943,
      "step": 4030
    },
    {
      "epoch": 3.9414634146341463,
      "grad_norm": 7.219639301300049,
      "learning_rate": 2.1102439024390242e-05,
      "loss": 0.9054,
      "step": 4040
    },
    {
      "epoch": 3.951219512195122,
      "grad_norm": 4.902370452880859,
      "learning_rate": 2.1073170731707316e-05,
      "loss": 0.9426,
      "step": 4050
    },
    {
      "epoch": 3.9609756097560975,
      "grad_norm": 7.207034587860107,
      "learning_rate": 2.104390243902439e-05,
      "loss": 0.9985,
      "step": 4060
    },
    {
      "epoch": 3.970731707317073,
      "grad_norm": 4.551841735839844,
      "learning_rate": 2.1014634146341462e-05,
      "loss": 0.869,
      "step": 4070
    },
    {
      "epoch": 3.9804878048780488,
      "grad_norm": 6.486295700073242,
      "learning_rate": 2.0985365853658535e-05,
      "loss": 0.9381,
      "step": 4080
    },
    {
      "epoch": 3.9902439024390244,
      "grad_norm": 4.446776866912842,
      "learning_rate": 2.095609756097561e-05,
      "loss": 0.8115,
      "step": 4090
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.8378312587738037,
      "learning_rate": 2.0926829268292682e-05,
      "loss": 0.8108,
      "step": 4100
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.5441461801528931,
      "eval_runtime": 2.2219,
      "eval_samples_per_second": 193.977,
      "eval_steps_per_second": 24.303,
      "step": 4100
    },
    {
      "epoch": 4.009756097560976,
      "grad_norm": 5.5055036544799805,
      "learning_rate": 2.0897560975609755e-05,
      "loss": 0.7588,
      "step": 4110
    },
    {
      "epoch": 4.019512195121951,
      "grad_norm": 6.063300132751465,
      "learning_rate": 2.086829268292683e-05,
      "loss": 0.8113,
      "step": 4120
    },
    {
      "epoch": 4.029268292682927,
      "grad_norm": 4.282827377319336,
      "learning_rate": 2.0839024390243902e-05,
      "loss": 0.9965,
      "step": 4130
    },
    {
      "epoch": 4.0390243902439025,
      "grad_norm": 4.0948591232299805,
      "learning_rate": 2.0809756097560975e-05,
      "loss": 0.8482,
      "step": 4140
    },
    {
      "epoch": 4.048780487804878,
      "grad_norm": 5.020638942718506,
      "learning_rate": 2.078048780487805e-05,
      "loss": 0.82,
      "step": 4150
    },
    {
      "epoch": 4.058536585365854,
      "grad_norm": 7.345036029815674,
      "learning_rate": 2.0751219512195122e-05,
      "loss": 0.7998,
      "step": 4160
    },
    {
      "epoch": 4.068292682926829,
      "grad_norm": 4.915556907653809,
      "learning_rate": 2.0721951219512195e-05,
      "loss": 0.7114,
      "step": 4170
    },
    {
      "epoch": 4.078048780487805,
      "grad_norm": 5.1429595947265625,
      "learning_rate": 2.069268292682927e-05,
      "loss": 0.8598,
      "step": 4180
    },
    {
      "epoch": 4.087804878048781,
      "grad_norm": 8.880992889404297,
      "learning_rate": 2.066341463414634e-05,
      "loss": 0.7664,
      "step": 4190
    },
    {
      "epoch": 4.097560975609756,
      "grad_norm": 6.015589714050293,
      "learning_rate": 2.0634146341463415e-05,
      "loss": 0.8327,
      "step": 4200
    },
    {
      "epoch": 4.107317073170732,
      "grad_norm": 19.572351455688477,
      "learning_rate": 2.060487804878049e-05,
      "loss": 0.8551,
      "step": 4210
    },
    {
      "epoch": 4.117073170731707,
      "grad_norm": 7.176087379455566,
      "learning_rate": 2.057560975609756e-05,
      "loss": 0.9271,
      "step": 4220
    },
    {
      "epoch": 4.126829268292683,
      "grad_norm": 5.741461277008057,
      "learning_rate": 2.0546341463414635e-05,
      "loss": 0.8255,
      "step": 4230
    },
    {
      "epoch": 4.136585365853659,
      "grad_norm": 11.838271141052246,
      "learning_rate": 2.0517073170731708e-05,
      "loss": 0.7936,
      "step": 4240
    },
    {
      "epoch": 4.146341463414634,
      "grad_norm": 12.516536712646484,
      "learning_rate": 2.048780487804878e-05,
      "loss": 0.7565,
      "step": 4250
    },
    {
      "epoch": 4.15609756097561,
      "grad_norm": 6.0334014892578125,
      "learning_rate": 2.0458536585365855e-05,
      "loss": 0.8264,
      "step": 4260
    },
    {
      "epoch": 4.1658536585365855,
      "grad_norm": 5.398480415344238,
      "learning_rate": 2.0429268292682928e-05,
      "loss": 0.9416,
      "step": 4270
    },
    {
      "epoch": 4.175609756097561,
      "grad_norm": 4.67575740814209,
      "learning_rate": 2.04e-05,
      "loss": 0.7867,
      "step": 4280
    },
    {
      "epoch": 4.185365853658537,
      "grad_norm": 3.485051155090332,
      "learning_rate": 2.0370731707317074e-05,
      "loss": 0.898,
      "step": 4290
    },
    {
      "epoch": 4.195121951219512,
      "grad_norm": 7.7966814041137695,
      "learning_rate": 2.0341463414634148e-05,
      "loss": 0.7645,
      "step": 4300
    },
    {
      "epoch": 4.204878048780488,
      "grad_norm": 3.6713056564331055,
      "learning_rate": 2.031219512195122e-05,
      "loss": 0.7484,
      "step": 4310
    },
    {
      "epoch": 4.214634146341464,
      "grad_norm": 6.319855690002441,
      "learning_rate": 2.0282926829268294e-05,
      "loss": 1.0051,
      "step": 4320
    },
    {
      "epoch": 4.224390243902439,
      "grad_norm": 4.859902381896973,
      "learning_rate": 2.0253658536585368e-05,
      "loss": 0.7631,
      "step": 4330
    },
    {
      "epoch": 4.234146341463415,
      "grad_norm": 7.53424072265625,
      "learning_rate": 2.022439024390244e-05,
      "loss": 0.9065,
      "step": 4340
    },
    {
      "epoch": 4.2439024390243905,
      "grad_norm": 4.26215124130249,
      "learning_rate": 2.0195121951219514e-05,
      "loss": 0.7979,
      "step": 4350
    },
    {
      "epoch": 4.253658536585366,
      "grad_norm": 5.093909740447998,
      "learning_rate": 2.0165853658536587e-05,
      "loss": 0.9006,
      "step": 4360
    },
    {
      "epoch": 4.263414634146342,
      "grad_norm": 5.72164249420166,
      "learning_rate": 2.013658536585366e-05,
      "loss": 0.6881,
      "step": 4370
    },
    {
      "epoch": 4.273170731707317,
      "grad_norm": 7.140091419219971,
      "learning_rate": 2.0107317073170734e-05,
      "loss": 0.8909,
      "step": 4380
    },
    {
      "epoch": 4.282926829268293,
      "grad_norm": 9.331727981567383,
      "learning_rate": 2.0078048780487807e-05,
      "loss": 0.88,
      "step": 4390
    },
    {
      "epoch": 4.2926829268292686,
      "grad_norm": 4.616278171539307,
      "learning_rate": 2.004878048780488e-05,
      "loss": 0.9244,
      "step": 4400
    },
    {
      "epoch": 4.302439024390244,
      "grad_norm": 6.324718952178955,
      "learning_rate": 2.0019512195121954e-05,
      "loss": 0.8378,
      "step": 4410
    },
    {
      "epoch": 4.31219512195122,
      "grad_norm": 5.399633407592773,
      "learning_rate": 1.9990243902439027e-05,
      "loss": 0.7527,
      "step": 4420
    },
    {
      "epoch": 4.321951219512195,
      "grad_norm": 9.804815292358398,
      "learning_rate": 1.99609756097561e-05,
      "loss": 1.1604,
      "step": 4430
    },
    {
      "epoch": 4.331707317073171,
      "grad_norm": 7.886103630065918,
      "learning_rate": 1.9931707317073174e-05,
      "loss": 0.8056,
      "step": 4440
    },
    {
      "epoch": 4.341463414634147,
      "grad_norm": 5.6459126472473145,
      "learning_rate": 1.9902439024390247e-05,
      "loss": 0.8421,
      "step": 4450
    },
    {
      "epoch": 4.351219512195122,
      "grad_norm": 5.201016902923584,
      "learning_rate": 1.9873170731707317e-05,
      "loss": 0.9713,
      "step": 4460
    },
    {
      "epoch": 4.360975609756098,
      "grad_norm": 4.08360481262207,
      "learning_rate": 1.984390243902439e-05,
      "loss": 0.8126,
      "step": 4470
    },
    {
      "epoch": 4.3707317073170735,
      "grad_norm": 7.087599754333496,
      "learning_rate": 1.9814634146341464e-05,
      "loss": 0.7775,
      "step": 4480
    },
    {
      "epoch": 4.380487804878049,
      "grad_norm": 4.600651741027832,
      "learning_rate": 1.9785365853658537e-05,
      "loss": 0.828,
      "step": 4490
    },
    {
      "epoch": 4.390243902439025,
      "grad_norm": 4.6700239181518555,
      "learning_rate": 1.975609756097561e-05,
      "loss": 0.7852,
      "step": 4500
    },
    {
      "epoch": 4.4,
      "grad_norm": 3.4487977027893066,
      "learning_rate": 1.9726829268292683e-05,
      "loss": 0.7278,
      "step": 4510
    },
    {
      "epoch": 4.409756097560976,
      "grad_norm": 5.87657356262207,
      "learning_rate": 1.9697560975609757e-05,
      "loss": 0.9043,
      "step": 4520
    },
    {
      "epoch": 4.419512195121952,
      "grad_norm": 9.110379219055176,
      "learning_rate": 1.966829268292683e-05,
      "loss": 0.7736,
      "step": 4530
    },
    {
      "epoch": 4.429268292682927,
      "grad_norm": 6.912558555603027,
      "learning_rate": 1.9639024390243903e-05,
      "loss": 0.7285,
      "step": 4540
    },
    {
      "epoch": 4.439024390243903,
      "grad_norm": 4.22367525100708,
      "learning_rate": 1.9609756097560977e-05,
      "loss": 0.8508,
      "step": 4550
    },
    {
      "epoch": 4.4487804878048784,
      "grad_norm": 4.741119861602783,
      "learning_rate": 1.958048780487805e-05,
      "loss": 0.8047,
      "step": 4560
    },
    {
      "epoch": 4.458536585365854,
      "grad_norm": 4.483570098876953,
      "learning_rate": 1.9551219512195123e-05,
      "loss": 0.7203,
      "step": 4570
    },
    {
      "epoch": 4.46829268292683,
      "grad_norm": 7.312777996063232,
      "learning_rate": 1.9521951219512196e-05,
      "loss": 0.8981,
      "step": 4580
    },
    {
      "epoch": 4.478048780487805,
      "grad_norm": 4.66420316696167,
      "learning_rate": 1.949268292682927e-05,
      "loss": 0.8084,
      "step": 4590
    },
    {
      "epoch": 4.487804878048781,
      "grad_norm": 5.340563774108887,
      "learning_rate": 1.9463414634146343e-05,
      "loss": 0.6201,
      "step": 4600
    },
    {
      "epoch": 4.4975609756097565,
      "grad_norm": 6.637351989746094,
      "learning_rate": 1.9434146341463416e-05,
      "loss": 0.8712,
      "step": 4610
    },
    {
      "epoch": 4.507317073170732,
      "grad_norm": 3.5159668922424316,
      "learning_rate": 1.940487804878049e-05,
      "loss": 0.9726,
      "step": 4620
    },
    {
      "epoch": 4.517073170731708,
      "grad_norm": 6.651480197906494,
      "learning_rate": 1.9375609756097563e-05,
      "loss": 0.8597,
      "step": 4630
    },
    {
      "epoch": 4.526829268292683,
      "grad_norm": 6.284060001373291,
      "learning_rate": 1.9346341463414636e-05,
      "loss": 0.8968,
      "step": 4640
    },
    {
      "epoch": 4.536585365853659,
      "grad_norm": 5.202035427093506,
      "learning_rate": 1.931707317073171e-05,
      "loss": 0.6563,
      "step": 4650
    },
    {
      "epoch": 4.546341463414635,
      "grad_norm": 5.065072059631348,
      "learning_rate": 1.9287804878048783e-05,
      "loss": 0.7004,
      "step": 4660
    },
    {
      "epoch": 4.55609756097561,
      "grad_norm": 5.127742290496826,
      "learning_rate": 1.9258536585365856e-05,
      "loss": 0.9034,
      "step": 4670
    },
    {
      "epoch": 4.565853658536585,
      "grad_norm": 3.7451980113983154,
      "learning_rate": 1.922926829268293e-05,
      "loss": 0.855,
      "step": 4680
    },
    {
      "epoch": 4.575609756097561,
      "grad_norm": 4.976136684417725,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 0.8747,
      "step": 4690
    },
    {
      "epoch": 4.585365853658536,
      "grad_norm": 3.753586769104004,
      "learning_rate": 1.9170731707317072e-05,
      "loss": 0.7586,
      "step": 4700
    },
    {
      "epoch": 4.595121951219512,
      "grad_norm": 3.8752493858337402,
      "learning_rate": 1.9141463414634146e-05,
      "loss": 0.5951,
      "step": 4710
    },
    {
      "epoch": 4.6048780487804875,
      "grad_norm": 8.950323104858398,
      "learning_rate": 1.911219512195122e-05,
      "loss": 0.8724,
      "step": 4720
    },
    {
      "epoch": 4.614634146341463,
      "grad_norm": 3.364201545715332,
      "learning_rate": 1.9082926829268292e-05,
      "loss": 0.7081,
      "step": 4730
    },
    {
      "epoch": 4.624390243902439,
      "grad_norm": 2.6179819107055664,
      "learning_rate": 1.9053658536585366e-05,
      "loss": 0.8066,
      "step": 4740
    },
    {
      "epoch": 4.634146341463414,
      "grad_norm": 4.739836692810059,
      "learning_rate": 1.902439024390244e-05,
      "loss": 0.8065,
      "step": 4750
    },
    {
      "epoch": 4.64390243902439,
      "grad_norm": 5.646470546722412,
      "learning_rate": 1.8995121951219512e-05,
      "loss": 0.7721,
      "step": 4760
    },
    {
      "epoch": 4.6536585365853655,
      "grad_norm": 6.350094318389893,
      "learning_rate": 1.8965853658536585e-05,
      "loss": 0.749,
      "step": 4770
    },
    {
      "epoch": 4.663414634146341,
      "grad_norm": 5.01543664932251,
      "learning_rate": 1.893658536585366e-05,
      "loss": 0.5679,
      "step": 4780
    },
    {
      "epoch": 4.673170731707317,
      "grad_norm": 7.084141254425049,
      "learning_rate": 1.8907317073170732e-05,
      "loss": 0.8166,
      "step": 4790
    },
    {
      "epoch": 4.682926829268292,
      "grad_norm": 3.0738942623138428,
      "learning_rate": 1.8878048780487805e-05,
      "loss": 0.6981,
      "step": 4800
    },
    {
      "epoch": 4.692682926829268,
      "grad_norm": 5.858218669891357,
      "learning_rate": 1.884878048780488e-05,
      "loss": 0.753,
      "step": 4810
    },
    {
      "epoch": 4.702439024390244,
      "grad_norm": 7.73258638381958,
      "learning_rate": 1.8819512195121952e-05,
      "loss": 0.6771,
      "step": 4820
    },
    {
      "epoch": 4.712195121951219,
      "grad_norm": 6.539041042327881,
      "learning_rate": 1.8790243902439025e-05,
      "loss": 0.8018,
      "step": 4830
    },
    {
      "epoch": 4.721951219512195,
      "grad_norm": 3.7207844257354736,
      "learning_rate": 1.87609756097561e-05,
      "loss": 0.68,
      "step": 4840
    },
    {
      "epoch": 4.7317073170731705,
      "grad_norm": 8.052550315856934,
      "learning_rate": 1.8731707317073172e-05,
      "loss": 0.8063,
      "step": 4850
    },
    {
      "epoch": 4.741463414634146,
      "grad_norm": 4.488240718841553,
      "learning_rate": 1.8702439024390245e-05,
      "loss": 0.7291,
      "step": 4860
    },
    {
      "epoch": 4.751219512195122,
      "grad_norm": 3.838390827178955,
      "learning_rate": 1.8673170731707318e-05,
      "loss": 0.7588,
      "step": 4870
    },
    {
      "epoch": 4.760975609756097,
      "grad_norm": 2.736431121826172,
      "learning_rate": 1.864390243902439e-05,
      "loss": 0.7261,
      "step": 4880
    },
    {
      "epoch": 4.770731707317073,
      "grad_norm": 4.662909030914307,
      "learning_rate": 1.8614634146341465e-05,
      "loss": 0.851,
      "step": 4890
    },
    {
      "epoch": 4.780487804878049,
      "grad_norm": 4.105195045471191,
      "learning_rate": 1.8585365853658538e-05,
      "loss": 0.7065,
      "step": 4900
    },
    {
      "epoch": 4.790243902439024,
      "grad_norm": 5.040012836456299,
      "learning_rate": 1.855609756097561e-05,
      "loss": 0.8964,
      "step": 4910
    },
    {
      "epoch": 4.8,
      "grad_norm": 3.5958077907562256,
      "learning_rate": 1.8526829268292685e-05,
      "loss": 0.8904,
      "step": 4920
    },
    {
      "epoch": 4.809756097560975,
      "grad_norm": 3.7191953659057617,
      "learning_rate": 1.8497560975609758e-05,
      "loss": 0.7474,
      "step": 4930
    },
    {
      "epoch": 4.819512195121951,
      "grad_norm": 8.978022575378418,
      "learning_rate": 1.8468292682926828e-05,
      "loss": 0.9131,
      "step": 4940
    },
    {
      "epoch": 4.829268292682927,
      "grad_norm": 19.16835594177246,
      "learning_rate": 1.84390243902439e-05,
      "loss": 0.7967,
      "step": 4950
    },
    {
      "epoch": 4.839024390243902,
      "grad_norm": 5.055037498474121,
      "learning_rate": 1.8409756097560974e-05,
      "loss": 0.8115,
      "step": 4960
    },
    {
      "epoch": 4.848780487804878,
      "grad_norm": 5.555362701416016,
      "learning_rate": 1.8380487804878048e-05,
      "loss": 0.8842,
      "step": 4970
    },
    {
      "epoch": 4.8585365853658535,
      "grad_norm": 4.389231204986572,
      "learning_rate": 1.835121951219512e-05,
      "loss": 0.8525,
      "step": 4980
    },
    {
      "epoch": 4.868292682926829,
      "grad_norm": 9.025225639343262,
      "learning_rate": 1.8321951219512194e-05,
      "loss": 0.9199,
      "step": 4990
    },
    {
      "epoch": 4.878048780487805,
      "grad_norm": 5.230112552642822,
      "learning_rate": 1.8292682926829268e-05,
      "loss": 0.7871,
      "step": 5000
    },
    {
      "epoch": 4.88780487804878,
      "grad_norm": 5.969156742095947,
      "learning_rate": 1.826341463414634e-05,
      "loss": 0.8029,
      "step": 5010
    },
    {
      "epoch": 4.897560975609756,
      "grad_norm": 3.1637399196624756,
      "learning_rate": 1.8234146341463414e-05,
      "loss": 0.8371,
      "step": 5020
    },
    {
      "epoch": 4.907317073170732,
      "grad_norm": 4.498237133026123,
      "learning_rate": 1.8204878048780487e-05,
      "loss": 0.8077,
      "step": 5030
    },
    {
      "epoch": 4.917073170731707,
      "grad_norm": 3.973027229309082,
      "learning_rate": 1.817560975609756e-05,
      "loss": 0.6493,
      "step": 5040
    },
    {
      "epoch": 4.926829268292683,
      "grad_norm": 4.754179954528809,
      "learning_rate": 1.8146341463414634e-05,
      "loss": 0.7383,
      "step": 5050
    },
    {
      "epoch": 4.9365853658536585,
      "grad_norm": 6.024353981018066,
      "learning_rate": 1.8117073170731707e-05,
      "loss": 0.7383,
      "step": 5060
    },
    {
      "epoch": 4.946341463414634,
      "grad_norm": 5.219964981079102,
      "learning_rate": 1.808780487804878e-05,
      "loss": 0.609,
      "step": 5070
    },
    {
      "epoch": 4.95609756097561,
      "grad_norm": 15.292434692382812,
      "learning_rate": 1.8058536585365854e-05,
      "loss": 0.8142,
      "step": 5080
    },
    {
      "epoch": 4.965853658536585,
      "grad_norm": 3.7500431537628174,
      "learning_rate": 1.8029268292682927e-05,
      "loss": 0.916,
      "step": 5090
    },
    {
      "epoch": 4.975609756097561,
      "grad_norm": 4.356119155883789,
      "learning_rate": 1.8e-05,
      "loss": 1.0074,
      "step": 5100
    },
    {
      "epoch": 4.985365853658537,
      "grad_norm": 5.471748352050781,
      "learning_rate": 1.7970731707317074e-05,
      "loss": 0.6786,
      "step": 5110
    },
    {
      "epoch": 4.995121951219512,
      "grad_norm": 4.976168632507324,
      "learning_rate": 1.7941463414634147e-05,
      "loss": 1.0833,
      "step": 5120
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.49123647809028625,
      "eval_runtime": 2.5878,
      "eval_samples_per_second": 166.551,
      "eval_steps_per_second": 20.867,
      "step": 5125
    },
    {
      "epoch": 5.004878048780488,
      "grad_norm": 3.0587635040283203,
      "learning_rate": 1.791219512195122e-05,
      "loss": 0.836,
      "step": 5130
    },
    {
      "epoch": 5.014634146341463,
      "grad_norm": 4.839929580688477,
      "learning_rate": 1.7882926829268294e-05,
      "loss": 0.7741,
      "step": 5140
    },
    {
      "epoch": 5.024390243902439,
      "grad_norm": 4.718790054321289,
      "learning_rate": 1.7853658536585367e-05,
      "loss": 0.593,
      "step": 5150
    },
    {
      "epoch": 5.034146341463415,
      "grad_norm": 4.484500408172607,
      "learning_rate": 1.782439024390244e-05,
      "loss": 0.7764,
      "step": 5160
    },
    {
      "epoch": 5.04390243902439,
      "grad_norm": 4.838986873626709,
      "learning_rate": 1.7795121951219513e-05,
      "loss": 0.7267,
      "step": 5170
    },
    {
      "epoch": 5.053658536585366,
      "grad_norm": 6.066318035125732,
      "learning_rate": 1.7765853658536583e-05,
      "loss": 0.7619,
      "step": 5180
    },
    {
      "epoch": 5.0634146341463415,
      "grad_norm": 5.30183744430542,
      "learning_rate": 1.7736585365853657e-05,
      "loss": 0.7879,
      "step": 5190
    },
    {
      "epoch": 5.073170731707317,
      "grad_norm": 4.23823356628418,
      "learning_rate": 1.770731707317073e-05,
      "loss": 0.5579,
      "step": 5200
    },
    {
      "epoch": 5.082926829268293,
      "grad_norm": 4.114034175872803,
      "learning_rate": 1.7678048780487803e-05,
      "loss": 0.6488,
      "step": 5210
    },
    {
      "epoch": 5.092682926829268,
      "grad_norm": 4.802909851074219,
      "learning_rate": 1.7648780487804876e-05,
      "loss": 0.7336,
      "step": 5220
    },
    {
      "epoch": 5.102439024390244,
      "grad_norm": 5.424414157867432,
      "learning_rate": 1.761951219512195e-05,
      "loss": 0.7588,
      "step": 5230
    },
    {
      "epoch": 5.11219512195122,
      "grad_norm": 4.068822860717773,
      "learning_rate": 1.7590243902439023e-05,
      "loss": 0.7469,
      "step": 5240
    },
    {
      "epoch": 5.121951219512195,
      "grad_norm": 6.455063819885254,
      "learning_rate": 1.7560975609756096e-05,
      "loss": 0.6819,
      "step": 5250
    },
    {
      "epoch": 5.131707317073171,
      "grad_norm": 7.5815558433532715,
      "learning_rate": 1.753170731707317e-05,
      "loss": 0.7416,
      "step": 5260
    },
    {
      "epoch": 5.1414634146341465,
      "grad_norm": 7.357218265533447,
      "learning_rate": 1.7502439024390243e-05,
      "loss": 0.6783,
      "step": 5270
    },
    {
      "epoch": 5.151219512195122,
      "grad_norm": 8.588905334472656,
      "learning_rate": 1.7473170731707316e-05,
      "loss": 0.7753,
      "step": 5280
    },
    {
      "epoch": 5.160975609756098,
      "grad_norm": 4.671985149383545,
      "learning_rate": 1.744390243902439e-05,
      "loss": 0.9412,
      "step": 5290
    },
    {
      "epoch": 5.170731707317073,
      "grad_norm": 4.299088954925537,
      "learning_rate": 1.7414634146341463e-05,
      "loss": 0.7174,
      "step": 5300
    },
    {
      "epoch": 5.180487804878049,
      "grad_norm": 5.61989164352417,
      "learning_rate": 1.7385365853658536e-05,
      "loss": 0.7528,
      "step": 5310
    },
    {
      "epoch": 5.190243902439025,
      "grad_norm": 6.408153057098389,
      "learning_rate": 1.735609756097561e-05,
      "loss": 0.8085,
      "step": 5320
    },
    {
      "epoch": 5.2,
      "grad_norm": 10.116849899291992,
      "learning_rate": 1.7326829268292683e-05,
      "loss": 0.6803,
      "step": 5330
    },
    {
      "epoch": 5.209756097560976,
      "grad_norm": 6.330572605133057,
      "learning_rate": 1.7297560975609756e-05,
      "loss": 0.6377,
      "step": 5340
    },
    {
      "epoch": 5.219512195121951,
      "grad_norm": 3.1590795516967773,
      "learning_rate": 1.726829268292683e-05,
      "loss": 0.7222,
      "step": 5350
    },
    {
      "epoch": 5.229268292682927,
      "grad_norm": 7.4421515464782715,
      "learning_rate": 1.7239024390243902e-05,
      "loss": 0.7227,
      "step": 5360
    },
    {
      "epoch": 5.239024390243903,
      "grad_norm": 7.013416767120361,
      "learning_rate": 1.7209756097560976e-05,
      "loss": 0.7524,
      "step": 5370
    },
    {
      "epoch": 5.248780487804878,
      "grad_norm": 4.415230751037598,
      "learning_rate": 1.718048780487805e-05,
      "loss": 0.6975,
      "step": 5380
    },
    {
      "epoch": 5.258536585365854,
      "grad_norm": 3.8694887161254883,
      "learning_rate": 1.7151219512195122e-05,
      "loss": 0.8668,
      "step": 5390
    },
    {
      "epoch": 5.2682926829268295,
      "grad_norm": 5.4841437339782715,
      "learning_rate": 1.7121951219512196e-05,
      "loss": 0.8211,
      "step": 5400
    },
    {
      "epoch": 5.278048780487805,
      "grad_norm": 3.0319440364837646,
      "learning_rate": 1.709268292682927e-05,
      "loss": 0.6874,
      "step": 5410
    },
    {
      "epoch": 5.287804878048781,
      "grad_norm": 4.0496745109558105,
      "learning_rate": 1.706341463414634e-05,
      "loss": 0.6897,
      "step": 5420
    },
    {
      "epoch": 5.297560975609756,
      "grad_norm": 4.827596664428711,
      "learning_rate": 1.7034146341463412e-05,
      "loss": 0.7015,
      "step": 5430
    },
    {
      "epoch": 5.307317073170732,
      "grad_norm": 5.8050456047058105,
      "learning_rate": 1.7004878048780485e-05,
      "loss": 0.8101,
      "step": 5440
    },
    {
      "epoch": 5.317073170731708,
      "grad_norm": 8.042531967163086,
      "learning_rate": 1.697560975609756e-05,
      "loss": 0.7189,
      "step": 5450
    },
    {
      "epoch": 5.326829268292683,
      "grad_norm": 4.308102130889893,
      "learning_rate": 1.6946341463414632e-05,
      "loss": 0.8298,
      "step": 5460
    },
    {
      "epoch": 5.336585365853659,
      "grad_norm": 3.376934289932251,
      "learning_rate": 1.6917073170731705e-05,
      "loss": 0.8317,
      "step": 5470
    },
    {
      "epoch": 5.3463414634146345,
      "grad_norm": 6.0644683837890625,
      "learning_rate": 1.688780487804878e-05,
      "loss": 0.8223,
      "step": 5480
    },
    {
      "epoch": 5.35609756097561,
      "grad_norm": 5.3179850578308105,
      "learning_rate": 1.6858536585365855e-05,
      "loss": 0.7762,
      "step": 5490
    },
    {
      "epoch": 5.365853658536586,
      "grad_norm": 4.06345272064209,
      "learning_rate": 1.682926829268293e-05,
      "loss": 0.6599,
      "step": 5500
    },
    {
      "epoch": 5.375609756097561,
      "grad_norm": 4.488490104675293,
      "learning_rate": 1.6800000000000002e-05,
      "loss": 0.7248,
      "step": 5510
    },
    {
      "epoch": 5.385365853658537,
      "grad_norm": 3.9689767360687256,
      "learning_rate": 1.6770731707317075e-05,
      "loss": 0.6536,
      "step": 5520
    },
    {
      "epoch": 5.3951219512195125,
      "grad_norm": 8.466779708862305,
      "learning_rate": 1.674146341463415e-05,
      "loss": 0.7436,
      "step": 5530
    },
    {
      "epoch": 5.404878048780488,
      "grad_norm": 3.604990243911743,
      "learning_rate": 1.671219512195122e-05,
      "loss": 0.724,
      "step": 5540
    },
    {
      "epoch": 5.414634146341464,
      "grad_norm": 3.9536232948303223,
      "learning_rate": 1.6682926829268295e-05,
      "loss": 0.7504,
      "step": 5550
    },
    {
      "epoch": 5.424390243902439,
      "grad_norm": 5.18770694732666,
      "learning_rate": 1.6653658536585368e-05,
      "loss": 0.7814,
      "step": 5560
    },
    {
      "epoch": 5.434146341463415,
      "grad_norm": 5.767782688140869,
      "learning_rate": 1.662439024390244e-05,
      "loss": 0.4896,
      "step": 5570
    },
    {
      "epoch": 5.443902439024391,
      "grad_norm": 4.559449195861816,
      "learning_rate": 1.6595121951219515e-05,
      "loss": 0.7651,
      "step": 5580
    },
    {
      "epoch": 5.453658536585366,
      "grad_norm": 3.7725157737731934,
      "learning_rate": 1.6565853658536588e-05,
      "loss": 0.7448,
      "step": 5590
    },
    {
      "epoch": 5.463414634146342,
      "grad_norm": 3.535388231277466,
      "learning_rate": 1.653658536585366e-05,
      "loss": 0.5987,
      "step": 5600
    },
    {
      "epoch": 5.473170731707317,
      "grad_norm": 56.87511444091797,
      "learning_rate": 1.6507317073170735e-05,
      "loss": 0.6927,
      "step": 5610
    },
    {
      "epoch": 5.482926829268292,
      "grad_norm": 6.620731353759766,
      "learning_rate": 1.6478048780487808e-05,
      "loss": 0.7719,
      "step": 5620
    },
    {
      "epoch": 5.492682926829268,
      "grad_norm": 3.993675470352173,
      "learning_rate": 1.644878048780488e-05,
      "loss": 0.7382,
      "step": 5630
    },
    {
      "epoch": 5.5024390243902435,
      "grad_norm": 4.109822750091553,
      "learning_rate": 1.6419512195121954e-05,
      "loss": 0.845,
      "step": 5640
    },
    {
      "epoch": 5.512195121951219,
      "grad_norm": 3.359659433364868,
      "learning_rate": 1.6390243902439024e-05,
      "loss": 0.8193,
      "step": 5650
    },
    {
      "epoch": 5.521951219512195,
      "grad_norm": 4.235306262969971,
      "learning_rate": 1.6360975609756098e-05,
      "loss": 0.8642,
      "step": 5660
    },
    {
      "epoch": 5.53170731707317,
      "grad_norm": 3.5567619800567627,
      "learning_rate": 1.633170731707317e-05,
      "loss": 0.6248,
      "step": 5670
    },
    {
      "epoch": 5.541463414634146,
      "grad_norm": 6.443765163421631,
      "learning_rate": 1.6302439024390244e-05,
      "loss": 0.6532,
      "step": 5680
    },
    {
      "epoch": 5.5512195121951216,
      "grad_norm": 4.24997091293335,
      "learning_rate": 1.6273170731707318e-05,
      "loss": 0.5613,
      "step": 5690
    },
    {
      "epoch": 5.560975609756097,
      "grad_norm": 3.7289445400238037,
      "learning_rate": 1.624390243902439e-05,
      "loss": 0.7333,
      "step": 5700
    },
    {
      "epoch": 5.570731707317073,
      "grad_norm": 5.383352279663086,
      "learning_rate": 1.6214634146341464e-05,
      "loss": 0.6538,
      "step": 5710
    },
    {
      "epoch": 5.580487804878048,
      "grad_norm": 3.0919909477233887,
      "learning_rate": 1.6185365853658537e-05,
      "loss": 0.8004,
      "step": 5720
    },
    {
      "epoch": 5.590243902439024,
      "grad_norm": 4.66690731048584,
      "learning_rate": 1.615609756097561e-05,
      "loss": 0.7774,
      "step": 5730
    },
    {
      "epoch": 5.6,
      "grad_norm": 3.7497398853302,
      "learning_rate": 1.6126829268292684e-05,
      "loss": 0.7128,
      "step": 5740
    },
    {
      "epoch": 5.609756097560975,
      "grad_norm": 6.948086738586426,
      "learning_rate": 1.6097560975609757e-05,
      "loss": 0.7164,
      "step": 5750
    },
    {
      "epoch": 5.619512195121951,
      "grad_norm": 3.851604461669922,
      "learning_rate": 1.606829268292683e-05,
      "loss": 0.7219,
      "step": 5760
    },
    {
      "epoch": 5.6292682926829265,
      "grad_norm": 4.249239921569824,
      "learning_rate": 1.6039024390243904e-05,
      "loss": 0.7952,
      "step": 5770
    },
    {
      "epoch": 5.639024390243902,
      "grad_norm": 6.870947360992432,
      "learning_rate": 1.6009756097560977e-05,
      "loss": 0.7292,
      "step": 5780
    },
    {
      "epoch": 5.648780487804878,
      "grad_norm": 3.0629780292510986,
      "learning_rate": 1.598048780487805e-05,
      "loss": 0.5839,
      "step": 5790
    },
    {
      "epoch": 5.658536585365853,
      "grad_norm": 4.488579273223877,
      "learning_rate": 1.5951219512195124e-05,
      "loss": 0.593,
      "step": 5800
    },
    {
      "epoch": 5.668292682926829,
      "grad_norm": 2.9671401977539062,
      "learning_rate": 1.5921951219512197e-05,
      "loss": 0.6674,
      "step": 5810
    },
    {
      "epoch": 5.678048780487805,
      "grad_norm": 3.1851696968078613,
      "learning_rate": 1.589268292682927e-05,
      "loss": 0.6145,
      "step": 5820
    },
    {
      "epoch": 5.68780487804878,
      "grad_norm": 4.926481246948242,
      "learning_rate": 1.5863414634146344e-05,
      "loss": 0.7054,
      "step": 5830
    },
    {
      "epoch": 5.697560975609756,
      "grad_norm": 6.75816535949707,
      "learning_rate": 1.5834146341463417e-05,
      "loss": 0.6918,
      "step": 5840
    },
    {
      "epoch": 5.7073170731707314,
      "grad_norm": 4.282699108123779,
      "learning_rate": 1.580487804878049e-05,
      "loss": 0.7333,
      "step": 5850
    },
    {
      "epoch": 5.717073170731707,
      "grad_norm": 4.245416641235352,
      "learning_rate": 1.5775609756097563e-05,
      "loss": 0.7051,
      "step": 5860
    },
    {
      "epoch": 5.726829268292683,
      "grad_norm": 4.926001071929932,
      "learning_rate": 1.5746341463414637e-05,
      "loss": 0.6946,
      "step": 5870
    },
    {
      "epoch": 5.736585365853658,
      "grad_norm": 4.251893997192383,
      "learning_rate": 1.571707317073171e-05,
      "loss": 1.0311,
      "step": 5880
    },
    {
      "epoch": 5.746341463414634,
      "grad_norm": 4.76591682434082,
      "learning_rate": 1.568780487804878e-05,
      "loss": 0.8017,
      "step": 5890
    },
    {
      "epoch": 5.7560975609756095,
      "grad_norm": 4.897883415222168,
      "learning_rate": 1.5658536585365853e-05,
      "loss": 0.5854,
      "step": 5900
    },
    {
      "epoch": 5.765853658536585,
      "grad_norm": 4.6140217781066895,
      "learning_rate": 1.5629268292682926e-05,
      "loss": 0.6883,
      "step": 5910
    },
    {
      "epoch": 5.775609756097561,
      "grad_norm": 4.728873252868652,
      "learning_rate": 1.56e-05,
      "loss": 0.6514,
      "step": 5920
    },
    {
      "epoch": 5.785365853658536,
      "grad_norm": 4.841724395751953,
      "learning_rate": 1.5570731707317073e-05,
      "loss": 0.7567,
      "step": 5930
    },
    {
      "epoch": 5.795121951219512,
      "grad_norm": 3.0682597160339355,
      "learning_rate": 1.5541463414634146e-05,
      "loss": 0.7109,
      "step": 5940
    },
    {
      "epoch": 5.804878048780488,
      "grad_norm": 5.195882320404053,
      "learning_rate": 1.551219512195122e-05,
      "loss": 0.7758,
      "step": 5950
    },
    {
      "epoch": 5.814634146341463,
      "grad_norm": 5.45112943649292,
      "learning_rate": 1.5482926829268293e-05,
      "loss": 0.7403,
      "step": 5960
    },
    {
      "epoch": 5.824390243902439,
      "grad_norm": 2.462134838104248,
      "learning_rate": 1.5453658536585366e-05,
      "loss": 0.5902,
      "step": 5970
    },
    {
      "epoch": 5.8341463414634145,
      "grad_norm": 6.108086109161377,
      "learning_rate": 1.542439024390244e-05,
      "loss": 0.6407,
      "step": 5980
    },
    {
      "epoch": 5.84390243902439,
      "grad_norm": 3.9486992359161377,
      "learning_rate": 1.5395121951219513e-05,
      "loss": 0.7281,
      "step": 5990
    },
    {
      "epoch": 5.853658536585366,
      "grad_norm": 3.617577314376831,
      "learning_rate": 1.5365853658536586e-05,
      "loss": 0.6969,
      "step": 6000
    },
    {
      "epoch": 5.863414634146341,
      "grad_norm": 4.314629077911377,
      "learning_rate": 1.533658536585366e-05,
      "loss": 0.8115,
      "step": 6010
    },
    {
      "epoch": 5.873170731707317,
      "grad_norm": 11.834199905395508,
      "learning_rate": 1.5307317073170733e-05,
      "loss": 0.739,
      "step": 6020
    },
    {
      "epoch": 5.882926829268293,
      "grad_norm": 5.175218105316162,
      "learning_rate": 1.5278048780487806e-05,
      "loss": 0.6653,
      "step": 6030
    },
    {
      "epoch": 5.892682926829268,
      "grad_norm": 4.505629062652588,
      "learning_rate": 1.5248780487804879e-05,
      "loss": 0.7588,
      "step": 6040
    },
    {
      "epoch": 5.902439024390244,
      "grad_norm": 3.6926400661468506,
      "learning_rate": 1.5219512195121952e-05,
      "loss": 0.7369,
      "step": 6050
    },
    {
      "epoch": 5.912195121951219,
      "grad_norm": 9.578536987304688,
      "learning_rate": 1.5190243902439026e-05,
      "loss": 0.683,
      "step": 6060
    },
    {
      "epoch": 5.921951219512195,
      "grad_norm": 4.114490509033203,
      "learning_rate": 1.5160975609756097e-05,
      "loss": 0.7747,
      "step": 6070
    },
    {
      "epoch": 5.931707317073171,
      "grad_norm": 5.439108848571777,
      "learning_rate": 1.513170731707317e-05,
      "loss": 0.6944,
      "step": 6080
    },
    {
      "epoch": 5.941463414634146,
      "grad_norm": 4.619530200958252,
      "learning_rate": 1.5102439024390244e-05,
      "loss": 0.8312,
      "step": 6090
    },
    {
      "epoch": 5.951219512195122,
      "grad_norm": 4.609347820281982,
      "learning_rate": 1.5073170731707317e-05,
      "loss": 0.7628,
      "step": 6100
    },
    {
      "epoch": 5.9609756097560975,
      "grad_norm": 3.3043630123138428,
      "learning_rate": 1.504390243902439e-05,
      "loss": 0.58,
      "step": 6110
    },
    {
      "epoch": 5.970731707317073,
      "grad_norm": 4.833143711090088,
      "learning_rate": 1.5014634146341464e-05,
      "loss": 0.6214,
      "step": 6120
    },
    {
      "epoch": 5.980487804878049,
      "grad_norm": 7.0934672355651855,
      "learning_rate": 1.4985365853658537e-05,
      "loss": 0.8493,
      "step": 6130
    },
    {
      "epoch": 5.990243902439024,
      "grad_norm": 4.789239883422852,
      "learning_rate": 1.495609756097561e-05,
      "loss": 0.7901,
      "step": 6140
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.5219225883483887,
      "learning_rate": 1.4926829268292684e-05,
      "loss": 0.4748,
      "step": 6150
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.473966509103775,
      "eval_runtime": 2.1983,
      "eval_samples_per_second": 196.058,
      "eval_steps_per_second": 24.564,
      "step": 6150
    },
    {
      "epoch": 6.009756097560976,
      "grad_norm": 5.06523323059082,
      "learning_rate": 1.4897560975609757e-05,
      "loss": 0.648,
      "step": 6160
    },
    {
      "epoch": 6.019512195121951,
      "grad_norm": 5.688313007354736,
      "learning_rate": 1.486829268292683e-05,
      "loss": 0.5528,
      "step": 6170
    },
    {
      "epoch": 6.029268292682927,
      "grad_norm": 6.304387092590332,
      "learning_rate": 1.4839024390243903e-05,
      "loss": 0.7213,
      "step": 6180
    },
    {
      "epoch": 6.0390243902439025,
      "grad_norm": 4.100026607513428,
      "learning_rate": 1.4809756097560975e-05,
      "loss": 0.5695,
      "step": 6190
    },
    {
      "epoch": 6.048780487804878,
      "grad_norm": 4.58293342590332,
      "learning_rate": 1.4780487804878048e-05,
      "loss": 0.684,
      "step": 6200
    },
    {
      "epoch": 6.058536585365854,
      "grad_norm": 5.2741169929504395,
      "learning_rate": 1.4751219512195122e-05,
      "loss": 0.6536,
      "step": 6210
    },
    {
      "epoch": 6.068292682926829,
      "grad_norm": 7.303840637207031,
      "learning_rate": 1.4721951219512195e-05,
      "loss": 0.8571,
      "step": 6220
    },
    {
      "epoch": 6.078048780487805,
      "grad_norm": 4.4702019691467285,
      "learning_rate": 1.4692682926829268e-05,
      "loss": 0.6898,
      "step": 6230
    },
    {
      "epoch": 6.087804878048781,
      "grad_norm": 6.753406524658203,
      "learning_rate": 1.4663414634146341e-05,
      "loss": 0.7323,
      "step": 6240
    },
    {
      "epoch": 6.097560975609756,
      "grad_norm": 3.411266803741455,
      "learning_rate": 1.4634146341463415e-05,
      "loss": 0.6094,
      "step": 6250
    },
    {
      "epoch": 6.107317073170732,
      "grad_norm": 4.200493812561035,
      "learning_rate": 1.4604878048780488e-05,
      "loss": 0.7303,
      "step": 6260
    },
    {
      "epoch": 6.117073170731707,
      "grad_norm": 5.324514389038086,
      "learning_rate": 1.4575609756097561e-05,
      "loss": 0.6611,
      "step": 6270
    },
    {
      "epoch": 6.126829268292683,
      "grad_norm": 5.518704891204834,
      "learning_rate": 1.4546341463414635e-05,
      "loss": 0.7227,
      "step": 6280
    },
    {
      "epoch": 6.136585365853659,
      "grad_norm": 5.404102802276611,
      "learning_rate": 1.4517073170731708e-05,
      "loss": 0.6861,
      "step": 6290
    },
    {
      "epoch": 6.146341463414634,
      "grad_norm": 3.969306707382202,
      "learning_rate": 1.4487804878048781e-05,
      "loss": 0.7313,
      "step": 6300
    },
    {
      "epoch": 6.15609756097561,
      "grad_norm": 9.077821731567383,
      "learning_rate": 1.4458536585365853e-05,
      "loss": 0.6447,
      "step": 6310
    },
    {
      "epoch": 6.1658536585365855,
      "grad_norm": 5.107797145843506,
      "learning_rate": 1.4429268292682926e-05,
      "loss": 0.8077,
      "step": 6320
    },
    {
      "epoch": 6.175609756097561,
      "grad_norm": 3.7669148445129395,
      "learning_rate": 1.44e-05,
      "loss": 0.6853,
      "step": 6330
    },
    {
      "epoch": 6.185365853658537,
      "grad_norm": 4.348043918609619,
      "learning_rate": 1.4370731707317073e-05,
      "loss": 0.6819,
      "step": 6340
    },
    {
      "epoch": 6.195121951219512,
      "grad_norm": 7.0349225997924805,
      "learning_rate": 1.4341463414634146e-05,
      "loss": 0.5778,
      "step": 6350
    },
    {
      "epoch": 6.204878048780488,
      "grad_norm": 6.298891067504883,
      "learning_rate": 1.431219512195122e-05,
      "loss": 0.7486,
      "step": 6360
    },
    {
      "epoch": 6.214634146341464,
      "grad_norm": 5.100589752197266,
      "learning_rate": 1.4282926829268292e-05,
      "loss": 0.5693,
      "step": 6370
    },
    {
      "epoch": 6.224390243902439,
      "grad_norm": 3.437370538711548,
      "learning_rate": 1.4253658536585366e-05,
      "loss": 0.5384,
      "step": 6380
    },
    {
      "epoch": 6.234146341463415,
      "grad_norm": 4.0116963386535645,
      "learning_rate": 1.4224390243902439e-05,
      "loss": 0.743,
      "step": 6390
    },
    {
      "epoch": 6.2439024390243905,
      "grad_norm": 5.169159889221191,
      "learning_rate": 1.4195121951219512e-05,
      "loss": 0.6942,
      "step": 6400
    },
    {
      "epoch": 6.253658536585366,
      "grad_norm": 4.729720115661621,
      "learning_rate": 1.4165853658536586e-05,
      "loss": 0.6763,
      "step": 6410
    },
    {
      "epoch": 6.263414634146342,
      "grad_norm": 5.739103317260742,
      "learning_rate": 1.4136585365853659e-05,
      "loss": 0.6076,
      "step": 6420
    },
    {
      "epoch": 6.273170731707317,
      "grad_norm": 6.2567925453186035,
      "learning_rate": 1.410731707317073e-05,
      "loss": 0.839,
      "step": 6430
    },
    {
      "epoch": 6.282926829268293,
      "grad_norm": 6.127915382385254,
      "learning_rate": 1.4078048780487804e-05,
      "loss": 0.7469,
      "step": 6440
    },
    {
      "epoch": 6.2926829268292686,
      "grad_norm": 5.011312007904053,
      "learning_rate": 1.4048780487804879e-05,
      "loss": 0.7364,
      "step": 6450
    },
    {
      "epoch": 6.302439024390244,
      "grad_norm": 7.836746692657471,
      "learning_rate": 1.4019512195121952e-05,
      "loss": 0.7715,
      "step": 6460
    },
    {
      "epoch": 6.31219512195122,
      "grad_norm": 5.5404534339904785,
      "learning_rate": 1.3990243902439025e-05,
      "loss": 0.7126,
      "step": 6470
    },
    {
      "epoch": 6.321951219512195,
      "grad_norm": 3.688328266143799,
      "learning_rate": 1.3960975609756099e-05,
      "loss": 0.6611,
      "step": 6480
    },
    {
      "epoch": 6.331707317073171,
      "grad_norm": 5.068147659301758,
      "learning_rate": 1.3931707317073172e-05,
      "loss": 0.5599,
      "step": 6490
    },
    {
      "epoch": 6.341463414634147,
      "grad_norm": 4.828679084777832,
      "learning_rate": 1.3902439024390245e-05,
      "loss": 0.7327,
      "step": 6500
    },
    {
      "epoch": 6.351219512195122,
      "grad_norm": 3.7251648902893066,
      "learning_rate": 1.3873170731707318e-05,
      "loss": 0.7372,
      "step": 6510
    },
    {
      "epoch": 6.360975609756098,
      "grad_norm": 4.169531345367432,
      "learning_rate": 1.3843902439024392e-05,
      "loss": 0.6758,
      "step": 6520
    },
    {
      "epoch": 6.3707317073170735,
      "grad_norm": 3.656373977661133,
      "learning_rate": 1.3814634146341465e-05,
      "loss": 0.6041,
      "step": 6530
    },
    {
      "epoch": 6.380487804878049,
      "grad_norm": 3.492795944213867,
      "learning_rate": 1.3785365853658538e-05,
      "loss": 0.5569,
      "step": 6540
    },
    {
      "epoch": 6.390243902439025,
      "grad_norm": 5.078094482421875,
      "learning_rate": 1.375609756097561e-05,
      "loss": 0.6807,
      "step": 6550
    },
    {
      "epoch": 6.4,
      "grad_norm": 5.64828634262085,
      "learning_rate": 1.3726829268292683e-05,
      "loss": 0.6707,
      "step": 6560
    },
    {
      "epoch": 6.409756097560976,
      "grad_norm": 2.6936991214752197,
      "learning_rate": 1.3697560975609757e-05,
      "loss": 0.5923,
      "step": 6570
    },
    {
      "epoch": 6.419512195121952,
      "grad_norm": 6.458296775817871,
      "learning_rate": 1.366829268292683e-05,
      "loss": 0.7361,
      "step": 6580
    },
    {
      "epoch": 6.429268292682927,
      "grad_norm": 4.210754871368408,
      "learning_rate": 1.3639024390243903e-05,
      "loss": 0.5765,
      "step": 6590
    },
    {
      "epoch": 6.439024390243903,
      "grad_norm": 4.499602794647217,
      "learning_rate": 1.3609756097560976e-05,
      "loss": 0.6619,
      "step": 6600
    },
    {
      "epoch": 6.4487804878048784,
      "grad_norm": 4.093859672546387,
      "learning_rate": 1.358048780487805e-05,
      "loss": 0.6238,
      "step": 6610
    },
    {
      "epoch": 6.458536585365854,
      "grad_norm": 3.804525852203369,
      "learning_rate": 1.3551219512195123e-05,
      "loss": 0.6661,
      "step": 6620
    },
    {
      "epoch": 6.46829268292683,
      "grad_norm": 2.7000036239624023,
      "learning_rate": 1.3521951219512196e-05,
      "loss": 0.7177,
      "step": 6630
    },
    {
      "epoch": 6.478048780487805,
      "grad_norm": 3.745054006576538,
      "learning_rate": 1.349268292682927e-05,
      "loss": 0.5807,
      "step": 6640
    },
    {
      "epoch": 6.487804878048781,
      "grad_norm": 4.17600154876709,
      "learning_rate": 1.3463414634146343e-05,
      "loss": 0.5562,
      "step": 6650
    },
    {
      "epoch": 6.4975609756097565,
      "grad_norm": 3.5611443519592285,
      "learning_rate": 1.3434146341463416e-05,
      "loss": 0.6239,
      "step": 6660
    },
    {
      "epoch": 6.507317073170732,
      "grad_norm": 3.2476508617401123,
      "learning_rate": 1.3404878048780488e-05,
      "loss": 0.6793,
      "step": 6670
    },
    {
      "epoch": 6.517073170731708,
      "grad_norm": 6.234369277954102,
      "learning_rate": 1.3375609756097561e-05,
      "loss": 0.654,
      "step": 6680
    },
    {
      "epoch": 6.526829268292683,
      "grad_norm": 3.8418149948120117,
      "learning_rate": 1.3346341463414634e-05,
      "loss": 0.7865,
      "step": 6690
    },
    {
      "epoch": 6.536585365853659,
      "grad_norm": 3.0722084045410156,
      "learning_rate": 1.3317073170731708e-05,
      "loss": 0.6614,
      "step": 6700
    },
    {
      "epoch": 6.546341463414635,
      "grad_norm": 3.6216375827789307,
      "learning_rate": 1.328780487804878e-05,
      "loss": 0.7201,
      "step": 6710
    },
    {
      "epoch": 6.55609756097561,
      "grad_norm": 7.091348171234131,
      "learning_rate": 1.3258536585365854e-05,
      "loss": 0.6523,
      "step": 6720
    },
    {
      "epoch": 6.565853658536585,
      "grad_norm": 3.7751307487487793,
      "learning_rate": 1.3229268292682927e-05,
      "loss": 0.6674,
      "step": 6730
    },
    {
      "epoch": 6.575609756097561,
      "grad_norm": 5.326596260070801,
      "learning_rate": 1.32e-05,
      "loss": 0.6081,
      "step": 6740
    },
    {
      "epoch": 6.585365853658536,
      "grad_norm": 3.007636308670044,
      "learning_rate": 1.3170731707317074e-05,
      "loss": 0.641,
      "step": 6750
    },
    {
      "epoch": 6.595121951219512,
      "grad_norm": 4.0795674324035645,
      "learning_rate": 1.3141463414634147e-05,
      "loss": 0.6781,
      "step": 6760
    },
    {
      "epoch": 6.6048780487804875,
      "grad_norm": 7.353954792022705,
      "learning_rate": 1.311219512195122e-05,
      "loss": 0.5519,
      "step": 6770
    },
    {
      "epoch": 6.614634146341463,
      "grad_norm": 6.874689102172852,
      "learning_rate": 1.3082926829268294e-05,
      "loss": 0.7613,
      "step": 6780
    },
    {
      "epoch": 6.624390243902439,
      "grad_norm": 7.600565433502197,
      "learning_rate": 1.3053658536585365e-05,
      "loss": 0.6966,
      "step": 6790
    },
    {
      "epoch": 6.634146341463414,
      "grad_norm": 5.606077194213867,
      "learning_rate": 1.3024390243902439e-05,
      "loss": 0.6499,
      "step": 6800
    },
    {
      "epoch": 6.64390243902439,
      "grad_norm": 3.9306137561798096,
      "learning_rate": 1.2995121951219512e-05,
      "loss": 0.575,
      "step": 6810
    },
    {
      "epoch": 6.6536585365853655,
      "grad_norm": 3.844566583633423,
      "learning_rate": 1.2965853658536585e-05,
      "loss": 0.714,
      "step": 6820
    },
    {
      "epoch": 6.663414634146341,
      "grad_norm": 5.373045444488525,
      "learning_rate": 1.2936585365853659e-05,
      "loss": 0.7808,
      "step": 6830
    },
    {
      "epoch": 6.673170731707317,
      "grad_norm": 4.264561176300049,
      "learning_rate": 1.2907317073170732e-05,
      "loss": 0.6358,
      "step": 6840
    },
    {
      "epoch": 6.682926829268292,
      "grad_norm": 4.039421081542969,
      "learning_rate": 1.2878048780487805e-05,
      "loss": 0.6142,
      "step": 6850
    },
    {
      "epoch": 6.692682926829268,
      "grad_norm": 5.240413188934326,
      "learning_rate": 1.2848780487804878e-05,
      "loss": 0.6633,
      "step": 6860
    },
    {
      "epoch": 6.702439024390244,
      "grad_norm": 5.538013935089111,
      "learning_rate": 1.2819512195121952e-05,
      "loss": 0.6828,
      "step": 6870
    },
    {
      "epoch": 6.712195121951219,
      "grad_norm": 4.1152191162109375,
      "learning_rate": 1.2790243902439025e-05,
      "loss": 0.8239,
      "step": 6880
    },
    {
      "epoch": 6.721951219512195,
      "grad_norm": 3.3924365043640137,
      "learning_rate": 1.2760975609756098e-05,
      "loss": 0.6473,
      "step": 6890
    },
    {
      "epoch": 6.7317073170731705,
      "grad_norm": 8.744146347045898,
      "learning_rate": 1.2731707317073172e-05,
      "loss": 0.6892,
      "step": 6900
    },
    {
      "epoch": 6.741463414634146,
      "grad_norm": 5.699683666229248,
      "learning_rate": 1.2702439024390243e-05,
      "loss": 0.8201,
      "step": 6910
    },
    {
      "epoch": 6.751219512195122,
      "grad_norm": 4.139947891235352,
      "learning_rate": 1.2673170731707316e-05,
      "loss": 0.728,
      "step": 6920
    },
    {
      "epoch": 6.760975609756097,
      "grad_norm": 3.246797561645508,
      "learning_rate": 1.264390243902439e-05,
      "loss": 0.592,
      "step": 6930
    },
    {
      "epoch": 6.770731707317073,
      "grad_norm": 5.181224346160889,
      "learning_rate": 1.2614634146341463e-05,
      "loss": 0.6757,
      "step": 6940
    },
    {
      "epoch": 6.780487804878049,
      "grad_norm": 6.258457183837891,
      "learning_rate": 1.2585365853658536e-05,
      "loss": 0.9072,
      "step": 6950
    },
    {
      "epoch": 6.790243902439024,
      "grad_norm": 4.8463134765625,
      "learning_rate": 1.255609756097561e-05,
      "loss": 0.5665,
      "step": 6960
    },
    {
      "epoch": 6.8,
      "grad_norm": 4.6658101081848145,
      "learning_rate": 1.2526829268292683e-05,
      "loss": 0.6152,
      "step": 6970
    },
    {
      "epoch": 6.809756097560975,
      "grad_norm": 11.602935791015625,
      "learning_rate": 1.2497560975609756e-05,
      "loss": 0.5946,
      "step": 6980
    },
    {
      "epoch": 6.819512195121951,
      "grad_norm": 4.888242244720459,
      "learning_rate": 1.246829268292683e-05,
      "loss": 0.6528,
      "step": 6990
    },
    {
      "epoch": 6.829268292682927,
      "grad_norm": 3.4636731147766113,
      "learning_rate": 1.2439024390243903e-05,
      "loss": 0.7692,
      "step": 7000
    },
    {
      "epoch": 6.839024390243902,
      "grad_norm": 4.652115345001221,
      "learning_rate": 1.2409756097560976e-05,
      "loss": 0.7423,
      "step": 7010
    },
    {
      "epoch": 6.848780487804878,
      "grad_norm": 4.681759834289551,
      "learning_rate": 1.2380487804878048e-05,
      "loss": 0.6842,
      "step": 7020
    },
    {
      "epoch": 6.8585365853658535,
      "grad_norm": 5.957235336303711,
      "learning_rate": 1.2351219512195121e-05,
      "loss": 0.7833,
      "step": 7030
    },
    {
      "epoch": 6.868292682926829,
      "grad_norm": 5.885193347930908,
      "learning_rate": 1.2321951219512194e-05,
      "loss": 0.6611,
      "step": 7040
    },
    {
      "epoch": 6.878048780487805,
      "grad_norm": 7.2030816078186035,
      "learning_rate": 1.2292682926829267e-05,
      "loss": 0.5776,
      "step": 7050
    },
    {
      "epoch": 6.88780487804878,
      "grad_norm": 2.766643762588501,
      "learning_rate": 1.226341463414634e-05,
      "loss": 0.458,
      "step": 7060
    },
    {
      "epoch": 6.897560975609756,
      "grad_norm": 8.144928932189941,
      "learning_rate": 1.2234146341463414e-05,
      "loss": 0.7128,
      "step": 7070
    },
    {
      "epoch": 6.907317073170732,
      "grad_norm": 4.198520660400391,
      "learning_rate": 1.2204878048780487e-05,
      "loss": 0.5314,
      "step": 7080
    },
    {
      "epoch": 6.917073170731707,
      "grad_norm": 3.3831465244293213,
      "learning_rate": 1.2175609756097562e-05,
      "loss": 0.606,
      "step": 7090
    },
    {
      "epoch": 6.926829268292683,
      "grad_norm": 8.634040832519531,
      "learning_rate": 1.2146341463414636e-05,
      "loss": 0.6743,
      "step": 7100
    },
    {
      "epoch": 6.9365853658536585,
      "grad_norm": 3.859717607498169,
      "learning_rate": 1.2117073170731709e-05,
      "loss": 0.5954,
      "step": 7110
    },
    {
      "epoch": 6.946341463414634,
      "grad_norm": 4.419107913970947,
      "learning_rate": 1.2087804878048782e-05,
      "loss": 0.7214,
      "step": 7120
    },
    {
      "epoch": 6.95609756097561,
      "grad_norm": 4.864830017089844,
      "learning_rate": 1.2058536585365855e-05,
      "loss": 0.6218,
      "step": 7130
    },
    {
      "epoch": 6.965853658536585,
      "grad_norm": 6.9759931564331055,
      "learning_rate": 1.2029268292682927e-05,
      "loss": 0.6085,
      "step": 7140
    },
    {
      "epoch": 6.975609756097561,
      "grad_norm": 5.8233442306518555,
      "learning_rate": 1.2e-05,
      "loss": 0.5775,
      "step": 7150
    },
    {
      "epoch": 6.985365853658537,
      "grad_norm": 5.709436416625977,
      "learning_rate": 1.1970731707317074e-05,
      "loss": 0.7329,
      "step": 7160
    },
    {
      "epoch": 6.995121951219512,
      "grad_norm": 5.146650791168213,
      "learning_rate": 1.1941463414634147e-05,
      "loss": 0.5901,
      "step": 7170
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.4657132625579834,
      "eval_runtime": 2.5098,
      "eval_samples_per_second": 171.728,
      "eval_steps_per_second": 21.516,
      "step": 7175
    },
    {
      "epoch": 7.004878048780488,
      "grad_norm": 4.1296868324279785,
      "learning_rate": 1.191219512195122e-05,
      "loss": 0.9111,
      "step": 7180
    },
    {
      "epoch": 7.014634146341463,
      "grad_norm": 5.049349784851074,
      "learning_rate": 1.1882926829268293e-05,
      "loss": 0.5385,
      "step": 7190
    },
    {
      "epoch": 7.024390243902439,
      "grad_norm": 2.1052489280700684,
      "learning_rate": 1.1853658536585367e-05,
      "loss": 0.4674,
      "step": 7200
    },
    {
      "epoch": 7.034146341463415,
      "grad_norm": 4.8243560791015625,
      "learning_rate": 1.182439024390244e-05,
      "loss": 0.5791,
      "step": 7210
    },
    {
      "epoch": 7.04390243902439,
      "grad_norm": 2.8082518577575684,
      "learning_rate": 1.1795121951219513e-05,
      "loss": 0.654,
      "step": 7220
    },
    {
      "epoch": 7.053658536585366,
      "grad_norm": 3.5041439533233643,
      "learning_rate": 1.1765853658536587e-05,
      "loss": 0.6293,
      "step": 7230
    },
    {
      "epoch": 7.0634146341463415,
      "grad_norm": 8.95784854888916,
      "learning_rate": 1.173658536585366e-05,
      "loss": 0.544,
      "step": 7240
    },
    {
      "epoch": 7.073170731707317,
      "grad_norm": 4.042994022369385,
      "learning_rate": 1.1707317073170733e-05,
      "loss": 0.7037,
      "step": 7250
    },
    {
      "epoch": 7.082926829268293,
      "grad_norm": 3.941117525100708,
      "learning_rate": 1.1678048780487805e-05,
      "loss": 0.6421,
      "step": 7260
    },
    {
      "epoch": 7.092682926829268,
      "grad_norm": 4.891635417938232,
      "learning_rate": 1.1648780487804878e-05,
      "loss": 0.6521,
      "step": 7270
    },
    {
      "epoch": 7.102439024390244,
      "grad_norm": 4.00367546081543,
      "learning_rate": 1.1619512195121951e-05,
      "loss": 0.6289,
      "step": 7280
    },
    {
      "epoch": 7.11219512195122,
      "grad_norm": 4.59088659286499,
      "learning_rate": 1.1590243902439025e-05,
      "loss": 0.738,
      "step": 7290
    },
    {
      "epoch": 7.121951219512195,
      "grad_norm": 4.31188440322876,
      "learning_rate": 1.1560975609756098e-05,
      "loss": 0.5788,
      "step": 7300
    },
    {
      "epoch": 7.131707317073171,
      "grad_norm": 4.182316303253174,
      "learning_rate": 1.1531707317073171e-05,
      "loss": 0.6302,
      "step": 7310
    },
    {
      "epoch": 7.1414634146341465,
      "grad_norm": 4.637821674346924,
      "learning_rate": 1.1502439024390244e-05,
      "loss": 0.746,
      "step": 7320
    },
    {
      "epoch": 7.151219512195122,
      "grad_norm": 6.3886237144470215,
      "learning_rate": 1.1473170731707318e-05,
      "loss": 0.6602,
      "step": 7330
    },
    {
      "epoch": 7.160975609756098,
      "grad_norm": 3.8614938259124756,
      "learning_rate": 1.1443902439024391e-05,
      "loss": 0.7763,
      "step": 7340
    },
    {
      "epoch": 7.170731707317073,
      "grad_norm": 4.7814788818359375,
      "learning_rate": 1.1414634146341464e-05,
      "loss": 0.6995,
      "step": 7350
    },
    {
      "epoch": 7.180487804878049,
      "grad_norm": 4.591709613800049,
      "learning_rate": 1.1385365853658538e-05,
      "loss": 0.6078,
      "step": 7360
    },
    {
      "epoch": 7.190243902439025,
      "grad_norm": 3.810570240020752,
      "learning_rate": 1.1356097560975611e-05,
      "loss": 0.6585,
      "step": 7370
    },
    {
      "epoch": 7.2,
      "grad_norm": 3.5959312915802,
      "learning_rate": 1.1326829268292682e-05,
      "loss": 0.5839,
      "step": 7380
    },
    {
      "epoch": 7.209756097560976,
      "grad_norm": 10.91006088256836,
      "learning_rate": 1.1297560975609756e-05,
      "loss": 0.7314,
      "step": 7390
    },
    {
      "epoch": 7.219512195121951,
      "grad_norm": 3.5339181423187256,
      "learning_rate": 1.1268292682926829e-05,
      "loss": 0.5816,
      "step": 7400
    },
    {
      "epoch": 7.229268292682927,
      "grad_norm": 4.016465663909912,
      "learning_rate": 1.1239024390243902e-05,
      "loss": 0.6103,
      "step": 7410
    },
    {
      "epoch": 7.239024390243903,
      "grad_norm": 5.014120578765869,
      "learning_rate": 1.1209756097560976e-05,
      "loss": 0.4929,
      "step": 7420
    },
    {
      "epoch": 7.248780487804878,
      "grad_norm": 3.3965330123901367,
      "learning_rate": 1.1180487804878049e-05,
      "loss": 0.6161,
      "step": 7430
    },
    {
      "epoch": 7.258536585365854,
      "grad_norm": 3.144028663635254,
      "learning_rate": 1.1151219512195122e-05,
      "loss": 0.591,
      "step": 7440
    },
    {
      "epoch": 7.2682926829268295,
      "grad_norm": 2.5212931632995605,
      "learning_rate": 1.1121951219512195e-05,
      "loss": 0.5294,
      "step": 7450
    },
    {
      "epoch": 7.278048780487805,
      "grad_norm": 7.355587005615234,
      "learning_rate": 1.1092682926829269e-05,
      "loss": 0.8062,
      "step": 7460
    },
    {
      "epoch": 7.287804878048781,
      "grad_norm": 1.9399502277374268,
      "learning_rate": 1.1063414634146342e-05,
      "loss": 0.5846,
      "step": 7470
    },
    {
      "epoch": 7.297560975609756,
      "grad_norm": 4.419597625732422,
      "learning_rate": 1.1034146341463415e-05,
      "loss": 0.4869,
      "step": 7480
    },
    {
      "epoch": 7.307317073170732,
      "grad_norm": 6.391424179077148,
      "learning_rate": 1.1004878048780489e-05,
      "loss": 0.5455,
      "step": 7490
    },
    {
      "epoch": 7.317073170731708,
      "grad_norm": 5.344315528869629,
      "learning_rate": 1.097560975609756e-05,
      "loss": 0.6794,
      "step": 7500
    },
    {
      "epoch": 7.326829268292683,
      "grad_norm": 8.75085163116455,
      "learning_rate": 1.0946341463414634e-05,
      "loss": 0.7524,
      "step": 7510
    },
    {
      "epoch": 7.336585365853659,
      "grad_norm": 5.347619533538818,
      "learning_rate": 1.0917073170731707e-05,
      "loss": 0.658,
      "step": 7520
    },
    {
      "epoch": 7.3463414634146345,
      "grad_norm": 2.4987692832946777,
      "learning_rate": 1.088780487804878e-05,
      "loss": 0.5655,
      "step": 7530
    },
    {
      "epoch": 7.35609756097561,
      "grad_norm": 7.659930229187012,
      "learning_rate": 1.0858536585365853e-05,
      "loss": 0.7252,
      "step": 7540
    },
    {
      "epoch": 7.365853658536586,
      "grad_norm": 3.707486629486084,
      "learning_rate": 1.0829268292682927e-05,
      "loss": 0.5938,
      "step": 7550
    },
    {
      "epoch": 7.375609756097561,
      "grad_norm": 7.064523696899414,
      "learning_rate": 1.08e-05,
      "loss": 0.7335,
      "step": 7560
    },
    {
      "epoch": 7.385365853658537,
      "grad_norm": 4.122854232788086,
      "learning_rate": 1.0770731707317073e-05,
      "loss": 0.5772,
      "step": 7570
    },
    {
      "epoch": 7.3951219512195125,
      "grad_norm": 2.5494613647460938,
      "learning_rate": 1.0741463414634147e-05,
      "loss": 0.6506,
      "step": 7580
    },
    {
      "epoch": 7.404878048780488,
      "grad_norm": 5.613608360290527,
      "learning_rate": 1.071219512195122e-05,
      "loss": 0.5704,
      "step": 7590
    },
    {
      "epoch": 7.414634146341464,
      "grad_norm": 3.909748077392578,
      "learning_rate": 1.0682926829268293e-05,
      "loss": 0.6288,
      "step": 7600
    },
    {
      "epoch": 7.424390243902439,
      "grad_norm": 5.163419723510742,
      "learning_rate": 1.0653658536585366e-05,
      "loss": 0.7519,
      "step": 7610
    },
    {
      "epoch": 7.434146341463415,
      "grad_norm": 6.131266117095947,
      "learning_rate": 1.0624390243902438e-05,
      "loss": 0.5779,
      "step": 7620
    },
    {
      "epoch": 7.443902439024391,
      "grad_norm": 4.058895587921143,
      "learning_rate": 1.0595121951219511e-05,
      "loss": 0.6468,
      "step": 7630
    },
    {
      "epoch": 7.453658536585366,
      "grad_norm": 3.3045406341552734,
      "learning_rate": 1.0565853658536585e-05,
      "loss": 0.8602,
      "step": 7640
    },
    {
      "epoch": 7.463414634146342,
      "grad_norm": 5.123964309692383,
      "learning_rate": 1.0536585365853658e-05,
      "loss": 0.5845,
      "step": 7650
    },
    {
      "epoch": 7.473170731707317,
      "grad_norm": 4.213142395019531,
      "learning_rate": 1.0507317073170731e-05,
      "loss": 0.6387,
      "step": 7660
    },
    {
      "epoch": 7.482926829268292,
      "grad_norm": 4.209359169006348,
      "learning_rate": 1.0478048780487804e-05,
      "loss": 0.7408,
      "step": 7670
    },
    {
      "epoch": 7.492682926829268,
      "grad_norm": 4.509215831756592,
      "learning_rate": 1.0448780487804878e-05,
      "loss": 0.6201,
      "step": 7680
    },
    {
      "epoch": 7.5024390243902435,
      "grad_norm": 3.248541831970215,
      "learning_rate": 1.0419512195121951e-05,
      "loss": 0.5805,
      "step": 7690
    },
    {
      "epoch": 7.512195121951219,
      "grad_norm": 1.9974559545516968,
      "learning_rate": 1.0390243902439024e-05,
      "loss": 0.5588,
      "step": 7700
    },
    {
      "epoch": 7.521951219512195,
      "grad_norm": 8.267496109008789,
      "learning_rate": 1.0360975609756098e-05,
      "loss": 0.6155,
      "step": 7710
    },
    {
      "epoch": 7.53170731707317,
      "grad_norm": 2.988499402999878,
      "learning_rate": 1.033170731707317e-05,
      "loss": 0.5714,
      "step": 7720
    },
    {
      "epoch": 7.541463414634146,
      "grad_norm": 5.521251201629639,
      "learning_rate": 1.0302439024390246e-05,
      "loss": 0.5487,
      "step": 7730
    },
    {
      "epoch": 7.5512195121951216,
      "grad_norm": 2.6578218936920166,
      "learning_rate": 1.0273170731707317e-05,
      "loss": 0.6086,
      "step": 7740
    },
    {
      "epoch": 7.560975609756097,
      "grad_norm": 5.24428129196167,
      "learning_rate": 1.024390243902439e-05,
      "loss": 0.6684,
      "step": 7750
    },
    {
      "epoch": 7.570731707317073,
      "grad_norm": 2.2795679569244385,
      "learning_rate": 1.0214634146341464e-05,
      "loss": 0.6518,
      "step": 7760
    },
    {
      "epoch": 7.580487804878048,
      "grad_norm": 3.9373955726623535,
      "learning_rate": 1.0185365853658537e-05,
      "loss": 0.5848,
      "step": 7770
    },
    {
      "epoch": 7.590243902439024,
      "grad_norm": 4.502900123596191,
      "learning_rate": 1.015609756097561e-05,
      "loss": 0.5075,
      "step": 7780
    },
    {
      "epoch": 7.6,
      "grad_norm": 3.619361162185669,
      "learning_rate": 1.0126829268292684e-05,
      "loss": 0.6447,
      "step": 7790
    },
    {
      "epoch": 7.609756097560975,
      "grad_norm": 3.2975900173187256,
      "learning_rate": 1.0097560975609757e-05,
      "loss": 0.486,
      "step": 7800
    },
    {
      "epoch": 7.619512195121951,
      "grad_norm": 5.468411922454834,
      "learning_rate": 1.006829268292683e-05,
      "loss": 0.6892,
      "step": 7810
    },
    {
      "epoch": 7.6292682926829265,
      "grad_norm": 4.101294040679932,
      "learning_rate": 1.0039024390243904e-05,
      "loss": 0.6265,
      "step": 7820
    },
    {
      "epoch": 7.639024390243902,
      "grad_norm": 5.246140956878662,
      "learning_rate": 1.0009756097560977e-05,
      "loss": 0.6809,
      "step": 7830
    },
    {
      "epoch": 7.648780487804878,
      "grad_norm": 4.760993003845215,
      "learning_rate": 9.98048780487805e-06,
      "loss": 0.4963,
      "step": 7840
    },
    {
      "epoch": 7.658536585365853,
      "grad_norm": 12.068758010864258,
      "learning_rate": 9.951219512195124e-06,
      "loss": 0.6607,
      "step": 7850
    },
    {
      "epoch": 7.668292682926829,
      "grad_norm": 3.8533356189727783,
      "learning_rate": 9.921951219512195e-06,
      "loss": 0.6719,
      "step": 7860
    },
    {
      "epoch": 7.678048780487805,
      "grad_norm": 5.536848545074463,
      "learning_rate": 9.892682926829268e-06,
      "loss": 0.4254,
      "step": 7870
    },
    {
      "epoch": 7.68780487804878,
      "grad_norm": 4.395762920379639,
      "learning_rate": 9.863414634146342e-06,
      "loss": 0.5421,
      "step": 7880
    },
    {
      "epoch": 7.697560975609756,
      "grad_norm": 9.025278091430664,
      "learning_rate": 9.834146341463415e-06,
      "loss": 0.6008,
      "step": 7890
    },
    {
      "epoch": 7.7073170731707314,
      "grad_norm": 6.105113506317139,
      "learning_rate": 9.804878048780488e-06,
      "loss": 0.7202,
      "step": 7900
    },
    {
      "epoch": 7.717073170731707,
      "grad_norm": 6.603391647338867,
      "learning_rate": 9.775609756097562e-06,
      "loss": 0.7287,
      "step": 7910
    },
    {
      "epoch": 7.726829268292683,
      "grad_norm": 3.368487596511841,
      "learning_rate": 9.746341463414635e-06,
      "loss": 0.5893,
      "step": 7920
    },
    {
      "epoch": 7.736585365853658,
      "grad_norm": 4.316356182098389,
      "learning_rate": 9.717073170731708e-06,
      "loss": 0.5363,
      "step": 7930
    },
    {
      "epoch": 7.746341463414634,
      "grad_norm": 4.047075271606445,
      "learning_rate": 9.687804878048781e-06,
      "loss": 0.5491,
      "step": 7940
    },
    {
      "epoch": 7.7560975609756095,
      "grad_norm": 4.509027004241943,
      "learning_rate": 9.658536585365855e-06,
      "loss": 0.7209,
      "step": 7950
    },
    {
      "epoch": 7.765853658536585,
      "grad_norm": 4.859107494354248,
      "learning_rate": 9.629268292682928e-06,
      "loss": 0.624,
      "step": 7960
    },
    {
      "epoch": 7.775609756097561,
      "grad_norm": 4.087185382843018,
      "learning_rate": 9.600000000000001e-06,
      "loss": 0.6187,
      "step": 7970
    },
    {
      "epoch": 7.785365853658536,
      "grad_norm": 3.0721399784088135,
      "learning_rate": 9.570731707317073e-06,
      "loss": 0.6606,
      "step": 7980
    },
    {
      "epoch": 7.795121951219512,
      "grad_norm": 4.4683451652526855,
      "learning_rate": 9.541463414634146e-06,
      "loss": 0.7384,
      "step": 7990
    },
    {
      "epoch": 7.804878048780488,
      "grad_norm": 2.895198106765747,
      "learning_rate": 9.51219512195122e-06,
      "loss": 0.7019,
      "step": 8000
    },
    {
      "epoch": 7.814634146341463,
      "grad_norm": 4.3073601722717285,
      "learning_rate": 9.482926829268293e-06,
      "loss": 0.5414,
      "step": 8010
    },
    {
      "epoch": 7.824390243902439,
      "grad_norm": 4.815942764282227,
      "learning_rate": 9.453658536585366e-06,
      "loss": 0.5525,
      "step": 8020
    },
    {
      "epoch": 7.8341463414634145,
      "grad_norm": 4.1675944328308105,
      "learning_rate": 9.42439024390244e-06,
      "loss": 0.5605,
      "step": 8030
    },
    {
      "epoch": 7.84390243902439,
      "grad_norm": 3.390562057495117,
      "learning_rate": 9.395121951219513e-06,
      "loss": 0.6371,
      "step": 8040
    },
    {
      "epoch": 7.853658536585366,
      "grad_norm": 5.7269697189331055,
      "learning_rate": 9.365853658536586e-06,
      "loss": 0.5626,
      "step": 8050
    },
    {
      "epoch": 7.863414634146341,
      "grad_norm": 2.4021458625793457,
      "learning_rate": 9.336585365853659e-06,
      "loss": 0.6804,
      "step": 8060
    },
    {
      "epoch": 7.873170731707317,
      "grad_norm": 2.920414686203003,
      "learning_rate": 9.307317073170732e-06,
      "loss": 0.7024,
      "step": 8070
    },
    {
      "epoch": 7.882926829268293,
      "grad_norm": 2.4982850551605225,
      "learning_rate": 9.278048780487806e-06,
      "loss": 0.6295,
      "step": 8080
    },
    {
      "epoch": 7.892682926829268,
      "grad_norm": 4.1175665855407715,
      "learning_rate": 9.248780487804879e-06,
      "loss": 0.5396,
      "step": 8090
    },
    {
      "epoch": 7.902439024390244,
      "grad_norm": 3.229264497756958,
      "learning_rate": 9.21951219512195e-06,
      "loss": 0.6297,
      "step": 8100
    },
    {
      "epoch": 7.912195121951219,
      "grad_norm": 8.033349990844727,
      "learning_rate": 9.190243902439024e-06,
      "loss": 0.6132,
      "step": 8110
    },
    {
      "epoch": 7.921951219512195,
      "grad_norm": 3.4795916080474854,
      "learning_rate": 9.160975609756097e-06,
      "loss": 0.6933,
      "step": 8120
    },
    {
      "epoch": 7.931707317073171,
      "grad_norm": 3.020948648452759,
      "learning_rate": 9.13170731707317e-06,
      "loss": 0.5292,
      "step": 8130
    },
    {
      "epoch": 7.941463414634146,
      "grad_norm": 3.250713348388672,
      "learning_rate": 9.102439024390244e-06,
      "loss": 0.5032,
      "step": 8140
    },
    {
      "epoch": 7.951219512195122,
      "grad_norm": 5.15635871887207,
      "learning_rate": 9.073170731707317e-06,
      "loss": 0.6269,
      "step": 8150
    },
    {
      "epoch": 7.9609756097560975,
      "grad_norm": 3.5582032203674316,
      "learning_rate": 9.04390243902439e-06,
      "loss": 0.5364,
      "step": 8160
    },
    {
      "epoch": 7.970731707317073,
      "grad_norm": 3.943507432937622,
      "learning_rate": 9.014634146341464e-06,
      "loss": 0.6739,
      "step": 8170
    },
    {
      "epoch": 7.980487804878049,
      "grad_norm": 4.123219013214111,
      "learning_rate": 8.985365853658537e-06,
      "loss": 0.6654,
      "step": 8180
    },
    {
      "epoch": 7.990243902439024,
      "grad_norm": 5.391843318939209,
      "learning_rate": 8.95609756097561e-06,
      "loss": 0.766,
      "step": 8190
    },
    {
      "epoch": 8.0,
      "grad_norm": 29.277982711791992,
      "learning_rate": 8.926829268292683e-06,
      "loss": 0.704,
      "step": 8200
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.4482246935367584,
      "eval_runtime": 2.2292,
      "eval_samples_per_second": 193.342,
      "eval_steps_per_second": 24.224,
      "step": 8200
    },
    {
      "epoch": 8.009756097560976,
      "grad_norm": 4.185688495635986,
      "learning_rate": 8.897560975609757e-06,
      "loss": 0.5152,
      "step": 8210
    },
    {
      "epoch": 8.019512195121951,
      "grad_norm": 3.5324039459228516,
      "learning_rate": 8.868292682926828e-06,
      "loss": 0.5478,
      "step": 8220
    },
    {
      "epoch": 8.029268292682927,
      "grad_norm": 3.420534372329712,
      "learning_rate": 8.839024390243902e-06,
      "loss": 0.6451,
      "step": 8230
    },
    {
      "epoch": 8.039024390243902,
      "grad_norm": 4.4556732177734375,
      "learning_rate": 8.809756097560975e-06,
      "loss": 0.658,
      "step": 8240
    },
    {
      "epoch": 8.048780487804878,
      "grad_norm": 5.670925140380859,
      "learning_rate": 8.780487804878048e-06,
      "loss": 0.651,
      "step": 8250
    },
    {
      "epoch": 8.058536585365854,
      "grad_norm": 4.88292932510376,
      "learning_rate": 8.751219512195121e-06,
      "loss": 0.6693,
      "step": 8260
    },
    {
      "epoch": 8.06829268292683,
      "grad_norm": 2.771818161010742,
      "learning_rate": 8.721951219512195e-06,
      "loss": 0.6187,
      "step": 8270
    },
    {
      "epoch": 8.078048780487805,
      "grad_norm": 3.209458589553833,
      "learning_rate": 8.692682926829268e-06,
      "loss": 0.7449,
      "step": 8280
    },
    {
      "epoch": 8.08780487804878,
      "grad_norm": 6.561764717102051,
      "learning_rate": 8.663414634146341e-06,
      "loss": 0.5647,
      "step": 8290
    },
    {
      "epoch": 8.097560975609756,
      "grad_norm": 8.430827140808105,
      "learning_rate": 8.634146341463415e-06,
      "loss": 0.5593,
      "step": 8300
    },
    {
      "epoch": 8.107317073170732,
      "grad_norm": 3.9187514781951904,
      "learning_rate": 8.604878048780488e-06,
      "loss": 0.5574,
      "step": 8310
    },
    {
      "epoch": 8.117073170731707,
      "grad_norm": 7.682411193847656,
      "learning_rate": 8.575609756097561e-06,
      "loss": 0.6722,
      "step": 8320
    },
    {
      "epoch": 8.126829268292683,
      "grad_norm": 4.001801013946533,
      "learning_rate": 8.546341463414634e-06,
      "loss": 0.639,
      "step": 8330
    },
    {
      "epoch": 8.136585365853659,
      "grad_norm": 6.082708358764648,
      "learning_rate": 8.517073170731706e-06,
      "loss": 0.5561,
      "step": 8340
    },
    {
      "epoch": 8.146341463414634,
      "grad_norm": 7.244412899017334,
      "learning_rate": 8.48780487804878e-06,
      "loss": 0.7166,
      "step": 8350
    },
    {
      "epoch": 8.15609756097561,
      "grad_norm": 4.5268754959106445,
      "learning_rate": 8.458536585365853e-06,
      "loss": 0.6163,
      "step": 8360
    },
    {
      "epoch": 8.165853658536586,
      "grad_norm": 3.30344820022583,
      "learning_rate": 8.429268292682928e-06,
      "loss": 0.4833,
      "step": 8370
    },
    {
      "epoch": 8.175609756097561,
      "grad_norm": 3.568103075027466,
      "learning_rate": 8.400000000000001e-06,
      "loss": 0.5987,
      "step": 8380
    },
    {
      "epoch": 8.185365853658537,
      "grad_norm": 4.54812479019165,
      "learning_rate": 8.370731707317074e-06,
      "loss": 0.6309,
      "step": 8390
    },
    {
      "epoch": 8.195121951219512,
      "grad_norm": 7.755340576171875,
      "learning_rate": 8.341463414634147e-06,
      "loss": 0.5998,
      "step": 8400
    },
    {
      "epoch": 8.204878048780488,
      "grad_norm": 2.281748056411743,
      "learning_rate": 8.31219512195122e-06,
      "loss": 0.5848,
      "step": 8410
    },
    {
      "epoch": 8.214634146341464,
      "grad_norm": 2.271923303604126,
      "learning_rate": 8.282926829268294e-06,
      "loss": 0.65,
      "step": 8420
    },
    {
      "epoch": 8.22439024390244,
      "grad_norm": 3.452735185623169,
      "learning_rate": 8.253658536585367e-06,
      "loss": 0.6027,
      "step": 8430
    },
    {
      "epoch": 8.234146341463415,
      "grad_norm": 3.713121175765991,
      "learning_rate": 8.22439024390244e-06,
      "loss": 0.5004,
      "step": 8440
    },
    {
      "epoch": 8.24390243902439,
      "grad_norm": 5.011720657348633,
      "learning_rate": 8.195121951219512e-06,
      "loss": 0.6598,
      "step": 8450
    },
    {
      "epoch": 8.253658536585366,
      "grad_norm": 4.004508972167969,
      "learning_rate": 8.165853658536585e-06,
      "loss": 0.5803,
      "step": 8460
    },
    {
      "epoch": 8.263414634146342,
      "grad_norm": 4.135370254516602,
      "learning_rate": 8.136585365853659e-06,
      "loss": 0.5465,
      "step": 8470
    },
    {
      "epoch": 8.273170731707317,
      "grad_norm": 4.371450424194336,
      "learning_rate": 8.107317073170732e-06,
      "loss": 0.6015,
      "step": 8480
    },
    {
      "epoch": 8.282926829268293,
      "grad_norm": 4.1804118156433105,
      "learning_rate": 8.078048780487805e-06,
      "loss": 0.5305,
      "step": 8490
    },
    {
      "epoch": 8.292682926829269,
      "grad_norm": 5.929856777191162,
      "learning_rate": 8.048780487804879e-06,
      "loss": 0.6345,
      "step": 8500
    },
    {
      "epoch": 8.302439024390244,
      "grad_norm": 4.163134574890137,
      "learning_rate": 8.019512195121952e-06,
      "loss": 0.505,
      "step": 8510
    },
    {
      "epoch": 8.31219512195122,
      "grad_norm": 5.8905487060546875,
      "learning_rate": 7.990243902439025e-06,
      "loss": 0.538,
      "step": 8520
    },
    {
      "epoch": 8.321951219512195,
      "grad_norm": 10.37294864654541,
      "learning_rate": 7.960975609756098e-06,
      "loss": 0.7072,
      "step": 8530
    },
    {
      "epoch": 8.331707317073171,
      "grad_norm": 5.870882034301758,
      "learning_rate": 7.931707317073172e-06,
      "loss": 0.5665,
      "step": 8540
    },
    {
      "epoch": 8.341463414634147,
      "grad_norm": 4.63503360748291,
      "learning_rate": 7.902439024390245e-06,
      "loss": 0.7843,
      "step": 8550
    },
    {
      "epoch": 8.351219512195122,
      "grad_norm": 4.751898765563965,
      "learning_rate": 7.873170731707318e-06,
      "loss": 0.4496,
      "step": 8560
    },
    {
      "epoch": 8.360975609756098,
      "grad_norm": 5.088692665100098,
      "learning_rate": 7.84390243902439e-06,
      "loss": 0.6478,
      "step": 8570
    },
    {
      "epoch": 8.370731707317074,
      "grad_norm": 3.3325328826904297,
      "learning_rate": 7.814634146341463e-06,
      "loss": 0.5772,
      "step": 8580
    },
    {
      "epoch": 8.38048780487805,
      "grad_norm": 4.125387191772461,
      "learning_rate": 7.785365853658537e-06,
      "loss": 0.5956,
      "step": 8590
    },
    {
      "epoch": 8.390243902439025,
      "grad_norm": 3.8221664428710938,
      "learning_rate": 7.75609756097561e-06,
      "loss": 0.7198,
      "step": 8600
    },
    {
      "epoch": 8.4,
      "grad_norm": 3.779600143432617,
      "learning_rate": 7.726829268292683e-06,
      "loss": 0.5689,
      "step": 8610
    },
    {
      "epoch": 8.409756097560976,
      "grad_norm": 7.149824142456055,
      "learning_rate": 7.697560975609756e-06,
      "loss": 0.6412,
      "step": 8620
    },
    {
      "epoch": 8.419512195121952,
      "grad_norm": 3.7842295169830322,
      "learning_rate": 7.66829268292683e-06,
      "loss": 0.5234,
      "step": 8630
    },
    {
      "epoch": 8.429268292682927,
      "grad_norm": 2.5754199028015137,
      "learning_rate": 7.639024390243903e-06,
      "loss": 0.6979,
      "step": 8640
    },
    {
      "epoch": 8.439024390243903,
      "grad_norm": 14.018174171447754,
      "learning_rate": 7.609756097560976e-06,
      "loss": 0.637,
      "step": 8650
    },
    {
      "epoch": 8.448780487804878,
      "grad_norm": 3.514477252960205,
      "learning_rate": 7.580487804878049e-06,
      "loss": 0.6048,
      "step": 8660
    },
    {
      "epoch": 8.458536585365854,
      "grad_norm": 3.4635424613952637,
      "learning_rate": 7.551219512195122e-06,
      "loss": 0.6832,
      "step": 8670
    },
    {
      "epoch": 8.46829268292683,
      "grad_norm": 3.980592966079712,
      "learning_rate": 7.521951219512195e-06,
      "loss": 0.5533,
      "step": 8680
    },
    {
      "epoch": 8.478048780487805,
      "grad_norm": 3.582655191421509,
      "learning_rate": 7.4926829268292685e-06,
      "loss": 0.483,
      "step": 8690
    },
    {
      "epoch": 8.487804878048781,
      "grad_norm": 5.283839225769043,
      "learning_rate": 7.463414634146342e-06,
      "loss": 0.58,
      "step": 8700
    },
    {
      "epoch": 8.497560975609757,
      "grad_norm": 4.65653133392334,
      "learning_rate": 7.434146341463415e-06,
      "loss": 0.7177,
      "step": 8710
    },
    {
      "epoch": 8.507317073170732,
      "grad_norm": 4.406381607055664,
      "learning_rate": 7.4048780487804875e-06,
      "loss": 0.7377,
      "step": 8720
    },
    {
      "epoch": 8.517073170731708,
      "grad_norm": 3.949678421020508,
      "learning_rate": 7.375609756097561e-06,
      "loss": 0.5553,
      "step": 8730
    },
    {
      "epoch": 8.526829268292683,
      "grad_norm": 2.7733750343322754,
      "learning_rate": 7.346341463414634e-06,
      "loss": 0.5924,
      "step": 8740
    },
    {
      "epoch": 8.536585365853659,
      "grad_norm": 5.037448883056641,
      "learning_rate": 7.317073170731707e-06,
      "loss": 0.594,
      "step": 8750
    },
    {
      "epoch": 8.546341463414635,
      "grad_norm": 6.415115833282471,
      "learning_rate": 7.287804878048781e-06,
      "loss": 0.6751,
      "step": 8760
    },
    {
      "epoch": 8.55609756097561,
      "grad_norm": 3.482362747192383,
      "learning_rate": 7.258536585365854e-06,
      "loss": 0.5537,
      "step": 8770
    },
    {
      "epoch": 8.565853658536586,
      "grad_norm": 2.9793906211853027,
      "learning_rate": 7.229268292682926e-06,
      "loss": 0.5128,
      "step": 8780
    },
    {
      "epoch": 8.575609756097561,
      "grad_norm": 4.067510604858398,
      "learning_rate": 7.2e-06,
      "loss": 0.7036,
      "step": 8790
    },
    {
      "epoch": 8.585365853658537,
      "grad_norm": 6.46417760848999,
      "learning_rate": 7.170731707317073e-06,
      "loss": 0.5616,
      "step": 8800
    },
    {
      "epoch": 8.595121951219513,
      "grad_norm": 4.502617359161377,
      "learning_rate": 7.141463414634146e-06,
      "loss": 0.4678,
      "step": 8810
    },
    {
      "epoch": 8.604878048780488,
      "grad_norm": 3.0298714637756348,
      "learning_rate": 7.1121951219512195e-06,
      "loss": 0.5087,
      "step": 8820
    },
    {
      "epoch": 8.614634146341464,
      "grad_norm": 2.881958246231079,
      "learning_rate": 7.082926829268293e-06,
      "loss": 0.5834,
      "step": 8830
    },
    {
      "epoch": 8.62439024390244,
      "grad_norm": 5.700189113616943,
      "learning_rate": 7.053658536585365e-06,
      "loss": 0.5812,
      "step": 8840
    },
    {
      "epoch": 8.634146341463415,
      "grad_norm": 4.127130031585693,
      "learning_rate": 7.024390243902439e-06,
      "loss": 0.5839,
      "step": 8850
    },
    {
      "epoch": 8.64390243902439,
      "grad_norm": 4.542916774749756,
      "learning_rate": 6.995121951219513e-06,
      "loss": 0.7146,
      "step": 8860
    },
    {
      "epoch": 8.653658536585366,
      "grad_norm": 2.7864110469818115,
      "learning_rate": 6.965853658536586e-06,
      "loss": 0.6467,
      "step": 8870
    },
    {
      "epoch": 8.663414634146342,
      "grad_norm": 4.752138137817383,
      "learning_rate": 6.936585365853659e-06,
      "loss": 0.7565,
      "step": 8880
    },
    {
      "epoch": 8.673170731707318,
      "grad_norm": 4.541611671447754,
      "learning_rate": 6.9073170731707325e-06,
      "loss": 0.6912,
      "step": 8890
    },
    {
      "epoch": 8.682926829268293,
      "grad_norm": 3.7866101264953613,
      "learning_rate": 6.878048780487805e-06,
      "loss": 0.6834,
      "step": 8900
    },
    {
      "epoch": 8.692682926829269,
      "grad_norm": 3.6312482357025146,
      "learning_rate": 6.848780487804878e-06,
      "loss": 0.672,
      "step": 8910
    },
    {
      "epoch": 8.702439024390245,
      "grad_norm": 6.6034932136535645,
      "learning_rate": 6.8195121951219515e-06,
      "loss": 0.6046,
      "step": 8920
    },
    {
      "epoch": 8.71219512195122,
      "grad_norm": 4.370355606079102,
      "learning_rate": 6.790243902439025e-06,
      "loss": 0.7222,
      "step": 8930
    },
    {
      "epoch": 8.721951219512196,
      "grad_norm": 4.573836803436279,
      "learning_rate": 6.760975609756098e-06,
      "loss": 0.5562,
      "step": 8940
    },
    {
      "epoch": 8.731707317073171,
      "grad_norm": 3.4819419384002686,
      "learning_rate": 6.731707317073171e-06,
      "loss": 0.7908,
      "step": 8950
    },
    {
      "epoch": 8.741463414634147,
      "grad_norm": 6.17144250869751,
      "learning_rate": 6.702439024390244e-06,
      "loss": 0.6697,
      "step": 8960
    },
    {
      "epoch": 8.751219512195123,
      "grad_norm": 4.160793304443359,
      "learning_rate": 6.673170731707317e-06,
      "loss": 0.5589,
      "step": 8970
    },
    {
      "epoch": 8.760975609756098,
      "grad_norm": 3.855858564376831,
      "learning_rate": 6.64390243902439e-06,
      "loss": 0.5312,
      "step": 8980
    },
    {
      "epoch": 8.770731707317074,
      "grad_norm": 8.019378662109375,
      "learning_rate": 6.614634146341464e-06,
      "loss": 0.4854,
      "step": 8990
    },
    {
      "epoch": 8.78048780487805,
      "grad_norm": 6.384222030639648,
      "learning_rate": 6.585365853658537e-06,
      "loss": 0.5941,
      "step": 9000
    },
    {
      "epoch": 8.790243902439025,
      "grad_norm": 7.527706146240234,
      "learning_rate": 6.55609756097561e-06,
      "loss": 0.5976,
      "step": 9010
    },
    {
      "epoch": 8.8,
      "grad_norm": 4.475605487823486,
      "learning_rate": 6.526829268292683e-06,
      "loss": 0.738,
      "step": 9020
    },
    {
      "epoch": 8.809756097560976,
      "grad_norm": 5.550478935241699,
      "learning_rate": 6.497560975609756e-06,
      "loss": 0.6893,
      "step": 9030
    },
    {
      "epoch": 8.819512195121952,
      "grad_norm": 4.58347225189209,
      "learning_rate": 6.468292682926829e-06,
      "loss": 0.5572,
      "step": 9040
    },
    {
      "epoch": 8.829268292682928,
      "grad_norm": 4.857528209686279,
      "learning_rate": 6.4390243902439026e-06,
      "loss": 0.6132,
      "step": 9050
    },
    {
      "epoch": 8.839024390243903,
      "grad_norm": 37.09995651245117,
      "learning_rate": 6.409756097560976e-06,
      "loss": 0.5398,
      "step": 9060
    },
    {
      "epoch": 8.848780487804879,
      "grad_norm": 5.388534069061279,
      "learning_rate": 6.380487804878049e-06,
      "loss": 0.6523,
      "step": 9070
    },
    {
      "epoch": 8.858536585365854,
      "grad_norm": 5.02443790435791,
      "learning_rate": 6.3512195121951216e-06,
      "loss": 0.714,
      "step": 9080
    },
    {
      "epoch": 8.86829268292683,
      "grad_norm": 3.7983171939849854,
      "learning_rate": 6.321951219512195e-06,
      "loss": 0.5458,
      "step": 9090
    },
    {
      "epoch": 8.878048780487806,
      "grad_norm": 2.3042564392089844,
      "learning_rate": 6.292682926829268e-06,
      "loss": 0.5721,
      "step": 9100
    },
    {
      "epoch": 8.887804878048781,
      "grad_norm": 3.1553690433502197,
      "learning_rate": 6.2634146341463414e-06,
      "loss": 0.6605,
      "step": 9110
    },
    {
      "epoch": 8.897560975609757,
      "grad_norm": 3.6697916984558105,
      "learning_rate": 6.234146341463415e-06,
      "loss": 0.5391,
      "step": 9120
    },
    {
      "epoch": 8.907317073170733,
      "grad_norm": 3.9344303607940674,
      "learning_rate": 6.204878048780488e-06,
      "loss": 0.5702,
      "step": 9130
    },
    {
      "epoch": 8.917073170731708,
      "grad_norm": 2.269517660140991,
      "learning_rate": 6.1756097560975604e-06,
      "loss": 0.5061,
      "step": 9140
    },
    {
      "epoch": 8.926829268292684,
      "grad_norm": 4.018985748291016,
      "learning_rate": 6.146341463414634e-06,
      "loss": 0.5698,
      "step": 9150
    },
    {
      "epoch": 8.93658536585366,
      "grad_norm": 1.7956593036651611,
      "learning_rate": 6.117073170731707e-06,
      "loss": 0.6204,
      "step": 9160
    },
    {
      "epoch": 8.946341463414633,
      "grad_norm": 4.797246932983398,
      "learning_rate": 6.087804878048781e-06,
      "loss": 0.6505,
      "step": 9170
    },
    {
      "epoch": 8.95609756097561,
      "grad_norm": 5.2820024490356445,
      "learning_rate": 6.0585365853658544e-06,
      "loss": 0.6146,
      "step": 9180
    },
    {
      "epoch": 8.965853658536584,
      "grad_norm": 3.434919834136963,
      "learning_rate": 6.029268292682928e-06,
      "loss": 0.6574,
      "step": 9190
    },
    {
      "epoch": 8.975609756097562,
      "grad_norm": 4.0793280601501465,
      "learning_rate": 6e-06,
      "loss": 0.5729,
      "step": 9200
    },
    {
      "epoch": 8.985365853658536,
      "grad_norm": 8.577259063720703,
      "learning_rate": 5.9707317073170734e-06,
      "loss": 0.5124,
      "step": 9210
    },
    {
      "epoch": 8.995121951219513,
      "grad_norm": 3.693110227584839,
      "learning_rate": 5.941463414634147e-06,
      "loss": 0.5959,
      "step": 9220
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.4489249885082245,
      "eval_runtime": 2.6513,
      "eval_samples_per_second": 162.561,
      "eval_steps_per_second": 20.367,
      "step": 9225
    },
    {
      "epoch": 9.004878048780487,
      "grad_norm": 3.2537283897399902,
      "learning_rate": 5.91219512195122e-06,
      "loss": 0.5675,
      "step": 9230
    },
    {
      "epoch": 9.014634146341463,
      "grad_norm": 3.4730441570281982,
      "learning_rate": 5.882926829268293e-06,
      "loss": 0.4897,
      "step": 9240
    },
    {
      "epoch": 9.024390243902438,
      "grad_norm": 3.7693991661071777,
      "learning_rate": 5.853658536585367e-06,
      "loss": 0.6275,
      "step": 9250
    },
    {
      "epoch": 9.034146341463414,
      "grad_norm": 2.9174487590789795,
      "learning_rate": 5.824390243902439e-06,
      "loss": 0.5165,
      "step": 9260
    },
    {
      "epoch": 9.04390243902439,
      "grad_norm": 10.328730583190918,
      "learning_rate": 5.795121951219512e-06,
      "loss": 0.6921,
      "step": 9270
    },
    {
      "epoch": 9.053658536585365,
      "grad_norm": 5.130741119384766,
      "learning_rate": 5.765853658536586e-06,
      "loss": 0.5835,
      "step": 9280
    },
    {
      "epoch": 9.06341463414634,
      "grad_norm": 5.816221237182617,
      "learning_rate": 5.736585365853659e-06,
      "loss": 0.5274,
      "step": 9290
    },
    {
      "epoch": 9.073170731707316,
      "grad_norm": 5.754615783691406,
      "learning_rate": 5.707317073170732e-06,
      "loss": 0.6077,
      "step": 9300
    },
    {
      "epoch": 9.082926829268292,
      "grad_norm": 2.1349895000457764,
      "learning_rate": 5.6780487804878054e-06,
      "loss": 0.5522,
      "step": 9310
    },
    {
      "epoch": 9.092682926829267,
      "grad_norm": 4.563711643218994,
      "learning_rate": 5.648780487804878e-06,
      "loss": 0.6811,
      "step": 9320
    },
    {
      "epoch": 9.102439024390243,
      "grad_norm": 4.679305076599121,
      "learning_rate": 5.619512195121951e-06,
      "loss": 0.6454,
      "step": 9330
    },
    {
      "epoch": 9.112195121951219,
      "grad_norm": 4.478633403778076,
      "learning_rate": 5.5902439024390245e-06,
      "loss": 0.5908,
      "step": 9340
    },
    {
      "epoch": 9.121951219512194,
      "grad_norm": 12.681501388549805,
      "learning_rate": 5.560975609756098e-06,
      "loss": 0.6323,
      "step": 9350
    },
    {
      "epoch": 9.13170731707317,
      "grad_norm": 4.224761009216309,
      "learning_rate": 5.531707317073171e-06,
      "loss": 0.6388,
      "step": 9360
    },
    {
      "epoch": 9.141463414634146,
      "grad_norm": 5.434450149536133,
      "learning_rate": 5.502439024390244e-06,
      "loss": 0.627,
      "step": 9370
    },
    {
      "epoch": 9.151219512195121,
      "grad_norm": 4.003040790557861,
      "learning_rate": 5.473170731707317e-06,
      "loss": 0.5827,
      "step": 9380
    },
    {
      "epoch": 9.160975609756097,
      "grad_norm": 4.092349529266357,
      "learning_rate": 5.44390243902439e-06,
      "loss": 0.5785,
      "step": 9390
    },
    {
      "epoch": 9.170731707317072,
      "grad_norm": 4.98482084274292,
      "learning_rate": 5.414634146341463e-06,
      "loss": 0.5734,
      "step": 9400
    },
    {
      "epoch": 9.180487804878048,
      "grad_norm": 3.4097540378570557,
      "learning_rate": 5.385365853658537e-06,
      "loss": 0.5767,
      "step": 9410
    },
    {
      "epoch": 9.190243902439024,
      "grad_norm": 4.575531959533691,
      "learning_rate": 5.35609756097561e-06,
      "loss": 0.6308,
      "step": 9420
    },
    {
      "epoch": 9.2,
      "grad_norm": 3.5808420181274414,
      "learning_rate": 5.326829268292683e-06,
      "loss": 0.5126,
      "step": 9430
    },
    {
      "epoch": 9.209756097560975,
      "grad_norm": 4.840463161468506,
      "learning_rate": 5.297560975609756e-06,
      "loss": 0.5569,
      "step": 9440
    },
    {
      "epoch": 9.21951219512195,
      "grad_norm": 15.93150520324707,
      "learning_rate": 5.268292682926829e-06,
      "loss": 0.6912,
      "step": 9450
    },
    {
      "epoch": 9.229268292682926,
      "grad_norm": 3.113110303878784,
      "learning_rate": 5.239024390243902e-06,
      "loss": 0.5113,
      "step": 9460
    },
    {
      "epoch": 9.239024390243902,
      "grad_norm": 2.697667360305786,
      "learning_rate": 5.2097560975609755e-06,
      "loss": 0.5697,
      "step": 9470
    },
    {
      "epoch": 9.248780487804877,
      "grad_norm": 5.503433704376221,
      "learning_rate": 5.180487804878049e-06,
      "loss": 0.6683,
      "step": 9480
    },
    {
      "epoch": 9.258536585365853,
      "grad_norm": 5.164898872375488,
      "learning_rate": 5.151219512195123e-06,
      "loss": 0.5572,
      "step": 9490
    },
    {
      "epoch": 9.268292682926829,
      "grad_norm": 4.378481864929199,
      "learning_rate": 5.121951219512195e-06,
      "loss": 0.5314,
      "step": 9500
    },
    {
      "epoch": 9.278048780487804,
      "grad_norm": 3.086904287338257,
      "learning_rate": 5.092682926829269e-06,
      "loss": 0.6433,
      "step": 9510
    },
    {
      "epoch": 9.28780487804878,
      "grad_norm": 6.006469249725342,
      "learning_rate": 5.063414634146342e-06,
      "loss": 0.6389,
      "step": 9520
    },
    {
      "epoch": 9.297560975609755,
      "grad_norm": 3.2921531200408936,
      "learning_rate": 5.034146341463415e-06,
      "loss": 0.5287,
      "step": 9530
    },
    {
      "epoch": 9.307317073170731,
      "grad_norm": 4.117655277252197,
      "learning_rate": 5.0048780487804885e-06,
      "loss": 0.5927,
      "step": 9540
    },
    {
      "epoch": 9.317073170731707,
      "grad_norm": 11.864782333374023,
      "learning_rate": 4.975609756097562e-06,
      "loss": 0.5341,
      "step": 9550
    },
    {
      "epoch": 9.326829268292682,
      "grad_norm": 3.3114075660705566,
      "learning_rate": 4.946341463414634e-06,
      "loss": 0.6501,
      "step": 9560
    },
    {
      "epoch": 9.336585365853658,
      "grad_norm": 4.1997270584106445,
      "learning_rate": 4.9170731707317075e-06,
      "loss": 0.6719,
      "step": 9570
    },
    {
      "epoch": 9.346341463414634,
      "grad_norm": 3.494992256164551,
      "learning_rate": 4.887804878048781e-06,
      "loss": 0.4541,
      "step": 9580
    },
    {
      "epoch": 9.35609756097561,
      "grad_norm": 3.5198171138763428,
      "learning_rate": 4.858536585365854e-06,
      "loss": 0.4948,
      "step": 9590
    },
    {
      "epoch": 9.365853658536585,
      "grad_norm": 4.274258136749268,
      "learning_rate": 4.829268292682927e-06,
      "loss": 0.5319,
      "step": 9600
    },
    {
      "epoch": 9.37560975609756,
      "grad_norm": 4.41455602645874,
      "learning_rate": 4.800000000000001e-06,
      "loss": 0.5166,
      "step": 9610
    },
    {
      "epoch": 9.385365853658536,
      "grad_norm": 4.107071399688721,
      "learning_rate": 4.770731707317073e-06,
      "loss": 0.5781,
      "step": 9620
    },
    {
      "epoch": 9.395121951219512,
      "grad_norm": 5.146185398101807,
      "learning_rate": 4.741463414634146e-06,
      "loss": 0.5441,
      "step": 9630
    },
    {
      "epoch": 9.404878048780487,
      "grad_norm": 4.547103404998779,
      "learning_rate": 4.71219512195122e-06,
      "loss": 0.588,
      "step": 9640
    },
    {
      "epoch": 9.414634146341463,
      "grad_norm": 5.644989490509033,
      "learning_rate": 4.682926829268293e-06,
      "loss": 0.5914,
      "step": 9650
    },
    {
      "epoch": 9.424390243902439,
      "grad_norm": 7.191071033477783,
      "learning_rate": 4.653658536585366e-06,
      "loss": 0.5792,
      "step": 9660
    },
    {
      "epoch": 9.434146341463414,
      "grad_norm": 4.432190418243408,
      "learning_rate": 4.6243902439024395e-06,
      "loss": 0.4714,
      "step": 9670
    },
    {
      "epoch": 9.44390243902439,
      "grad_norm": 8.801451683044434,
      "learning_rate": 4.595121951219512e-06,
      "loss": 0.6918,
      "step": 9680
    },
    {
      "epoch": 9.453658536585365,
      "grad_norm": 5.516130447387695,
      "learning_rate": 4.565853658536585e-06,
      "loss": 0.7817,
      "step": 9690
    },
    {
      "epoch": 9.463414634146341,
      "grad_norm": 21.661928176879883,
      "learning_rate": 4.5365853658536585e-06,
      "loss": 0.6807,
      "step": 9700
    },
    {
      "epoch": 9.473170731707317,
      "grad_norm": 5.630960941314697,
      "learning_rate": 4.507317073170732e-06,
      "loss": 0.6141,
      "step": 9710
    },
    {
      "epoch": 9.482926829268292,
      "grad_norm": 2.8707501888275146,
      "learning_rate": 4.478048780487805e-06,
      "loss": 0.6743,
      "step": 9720
    },
    {
      "epoch": 9.492682926829268,
      "grad_norm": 5.473480224609375,
      "learning_rate": 4.448780487804878e-06,
      "loss": 0.5414,
      "step": 9730
    },
    {
      "epoch": 9.502439024390243,
      "grad_norm": 4.272770404815674,
      "learning_rate": 4.419512195121951e-06,
      "loss": 0.636,
      "step": 9740
    },
    {
      "epoch": 9.512195121951219,
      "grad_norm": 3.8749048709869385,
      "learning_rate": 4.390243902439024e-06,
      "loss": 0.5829,
      "step": 9750
    },
    {
      "epoch": 9.521951219512195,
      "grad_norm": 3.718302011489868,
      "learning_rate": 4.360975609756097e-06,
      "loss": 0.4905,
      "step": 9760
    },
    {
      "epoch": 9.53170731707317,
      "grad_norm": 2.89564847946167,
      "learning_rate": 4.331707317073171e-06,
      "loss": 0.5742,
      "step": 9770
    },
    {
      "epoch": 9.541463414634146,
      "grad_norm": 2.8058931827545166,
      "learning_rate": 4.302439024390244e-06,
      "loss": 0.6279,
      "step": 9780
    },
    {
      "epoch": 9.551219512195122,
      "grad_norm": 3.956881046295166,
      "learning_rate": 4.273170731707317e-06,
      "loss": 0.5699,
      "step": 9790
    },
    {
      "epoch": 9.560975609756097,
      "grad_norm": 5.771266937255859,
      "learning_rate": 4.24390243902439e-06,
      "loss": 0.6645,
      "step": 9800
    },
    {
      "epoch": 9.570731707317073,
      "grad_norm": 5.8729352951049805,
      "learning_rate": 4.214634146341464e-06,
      "loss": 0.612,
      "step": 9810
    },
    {
      "epoch": 9.580487804878048,
      "grad_norm": 2.7457473278045654,
      "learning_rate": 4.185365853658537e-06,
      "loss": 0.5554,
      "step": 9820
    },
    {
      "epoch": 9.590243902439024,
      "grad_norm": 2.921177387237549,
      "learning_rate": 4.15609756097561e-06,
      "loss": 0.5387,
      "step": 9830
    },
    {
      "epoch": 9.6,
      "grad_norm": 3.2051663398742676,
      "learning_rate": 4.126829268292684e-06,
      "loss": 0.5437,
      "step": 9840
    },
    {
      "epoch": 9.609756097560975,
      "grad_norm": 3.7505204677581787,
      "learning_rate": 4.097560975609756e-06,
      "loss": 0.5418,
      "step": 9850
    },
    {
      "epoch": 9.61951219512195,
      "grad_norm": 3.1144518852233887,
      "learning_rate": 4.068292682926829e-06,
      "loss": 0.5382,
      "step": 9860
    },
    {
      "epoch": 9.629268292682926,
      "grad_norm": 4.079824924468994,
      "learning_rate": 4.039024390243903e-06,
      "loss": 0.4986,
      "step": 9870
    },
    {
      "epoch": 9.639024390243902,
      "grad_norm": 5.595481872558594,
      "learning_rate": 4.009756097560976e-06,
      "loss": 0.6231,
      "step": 9880
    },
    {
      "epoch": 9.648780487804878,
      "grad_norm": 5.110833644866943,
      "learning_rate": 3.980487804878049e-06,
      "loss": 0.707,
      "step": 9890
    },
    {
      "epoch": 9.658536585365853,
      "grad_norm": 4.554283142089844,
      "learning_rate": 3.9512195121951225e-06,
      "loss": 0.449,
      "step": 9900
    },
    {
      "epoch": 9.668292682926829,
      "grad_norm": 6.009799957275391,
      "learning_rate": 3.921951219512195e-06,
      "loss": 0.6109,
      "step": 9910
    },
    {
      "epoch": 9.678048780487805,
      "grad_norm": 3.911194086074829,
      "learning_rate": 3.892682926829268e-06,
      "loss": 0.7502,
      "step": 9920
    },
    {
      "epoch": 9.68780487804878,
      "grad_norm": 4.772467613220215,
      "learning_rate": 3.8634146341463415e-06,
      "loss": 0.5569,
      "step": 9930
    },
    {
      "epoch": 9.697560975609756,
      "grad_norm": 3.984408378601074,
      "learning_rate": 3.834146341463415e-06,
      "loss": 0.6636,
      "step": 9940
    },
    {
      "epoch": 9.707317073170731,
      "grad_norm": 2.5957396030426025,
      "learning_rate": 3.804878048780488e-06,
      "loss": 0.5226,
      "step": 9950
    },
    {
      "epoch": 9.717073170731707,
      "grad_norm": 3.566663980484009,
      "learning_rate": 3.775609756097561e-06,
      "loss": 0.4884,
      "step": 9960
    },
    {
      "epoch": 9.726829268292683,
      "grad_norm": 3.544820547103882,
      "learning_rate": 3.7463414634146343e-06,
      "loss": 0.5091,
      "step": 9970
    },
    {
      "epoch": 9.736585365853658,
      "grad_norm": 6.208227634429932,
      "learning_rate": 3.7170731707317075e-06,
      "loss": 0.5498,
      "step": 9980
    },
    {
      "epoch": 9.746341463414634,
      "grad_norm": 2.9035089015960693,
      "learning_rate": 3.6878048780487804e-06,
      "loss": 0.6177,
      "step": 9990
    },
    {
      "epoch": 9.75609756097561,
      "grad_norm": 4.602689266204834,
      "learning_rate": 3.6585365853658537e-06,
      "loss": 0.5217,
      "step": 10000
    },
    {
      "epoch": 9.765853658536585,
      "grad_norm": 4.039586067199707,
      "learning_rate": 3.629268292682927e-06,
      "loss": 0.597,
      "step": 10010
    },
    {
      "epoch": 9.77560975609756,
      "grad_norm": 9.101693153381348,
      "learning_rate": 3.6e-06,
      "loss": 0.4919,
      "step": 10020
    },
    {
      "epoch": 9.785365853658536,
      "grad_norm": 3.617281436920166,
      "learning_rate": 3.570731707317073e-06,
      "loss": 0.6354,
      "step": 10030
    },
    {
      "epoch": 9.795121951219512,
      "grad_norm": 2.883925199508667,
      "learning_rate": 3.5414634146341464e-06,
      "loss": 0.5161,
      "step": 10040
    },
    {
      "epoch": 9.804878048780488,
      "grad_norm": 4.74945068359375,
      "learning_rate": 3.5121951219512197e-06,
      "loss": 0.5598,
      "step": 10050
    },
    {
      "epoch": 9.814634146341463,
      "grad_norm": 5.065260410308838,
      "learning_rate": 3.482926829268293e-06,
      "loss": 0.5591,
      "step": 10060
    },
    {
      "epoch": 9.824390243902439,
      "grad_norm": 6.8348069190979,
      "learning_rate": 3.4536585365853663e-06,
      "loss": 0.6039,
      "step": 10070
    },
    {
      "epoch": 9.834146341463414,
      "grad_norm": 4.529128551483154,
      "learning_rate": 3.424390243902439e-06,
      "loss": 0.5859,
      "step": 10080
    },
    {
      "epoch": 9.84390243902439,
      "grad_norm": 4.0121636390686035,
      "learning_rate": 3.3951219512195124e-06,
      "loss": 0.5225,
      "step": 10090
    },
    {
      "epoch": 9.853658536585366,
      "grad_norm": 2.7388737201690674,
      "learning_rate": 3.3658536585365857e-06,
      "loss": 0.5461,
      "step": 10100
    },
    {
      "epoch": 9.863414634146341,
      "grad_norm": 3.0614852905273438,
      "learning_rate": 3.3365853658536586e-06,
      "loss": 0.4574,
      "step": 10110
    },
    {
      "epoch": 9.873170731707317,
      "grad_norm": 2.933802366256714,
      "learning_rate": 3.307317073170732e-06,
      "loss": 0.5175,
      "step": 10120
    },
    {
      "epoch": 9.882926829268293,
      "grad_norm": 2.399904727935791,
      "learning_rate": 3.278048780487805e-06,
      "loss": 0.4337,
      "step": 10130
    },
    {
      "epoch": 9.892682926829268,
      "grad_norm": 3.0476434230804443,
      "learning_rate": 3.248780487804878e-06,
      "loss": 0.7059,
      "step": 10140
    },
    {
      "epoch": 9.902439024390244,
      "grad_norm": 3.6341309547424316,
      "learning_rate": 3.2195121951219513e-06,
      "loss": 0.5517,
      "step": 10150
    },
    {
      "epoch": 9.91219512195122,
      "grad_norm": 4.938133239746094,
      "learning_rate": 3.1902439024390246e-06,
      "loss": 0.567,
      "step": 10160
    },
    {
      "epoch": 9.921951219512195,
      "grad_norm": 3.057703971862793,
      "learning_rate": 3.1609756097560974e-06,
      "loss": 0.6634,
      "step": 10170
    },
    {
      "epoch": 9.93170731707317,
      "grad_norm": 3.257108449935913,
      "learning_rate": 3.1317073170731707e-06,
      "loss": 0.5944,
      "step": 10180
    },
    {
      "epoch": 9.941463414634146,
      "grad_norm": 2.5088064670562744,
      "learning_rate": 3.102439024390244e-06,
      "loss": 0.6107,
      "step": 10190
    },
    {
      "epoch": 9.951219512195122,
      "grad_norm": 3.80769681930542,
      "learning_rate": 3.073170731707317e-06,
      "loss": 0.6295,
      "step": 10200
    },
    {
      "epoch": 9.960975609756098,
      "grad_norm": 6.694754600524902,
      "learning_rate": 3.0439024390243906e-06,
      "loss": 0.5307,
      "step": 10210
    },
    {
      "epoch": 9.970731707317073,
      "grad_norm": 4.071231842041016,
      "learning_rate": 3.014634146341464e-06,
      "loss": 0.5775,
      "step": 10220
    },
    {
      "epoch": 9.980487804878049,
      "grad_norm": 4.113222122192383,
      "learning_rate": 2.9853658536585367e-06,
      "loss": 0.6681,
      "step": 10230
    },
    {
      "epoch": 9.990243902439024,
      "grad_norm": 2.604295253753662,
      "learning_rate": 2.95609756097561e-06,
      "loss": 0.5814,
      "step": 10240
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.7303089499473572,
      "learning_rate": 2.9268292682926833e-06,
      "loss": 0.5011,
      "step": 10250
    }
  ],
  "logging_steps": 10,
  "max_steps": 10250,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3069932799098880.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
