{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 10.0,
  "eval_steps": 500,
  "global_step": 11590,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008628127696289905,
      "grad_norm": 1611.1785888671875,
      "learning_rate": 2.997411561691113e-05,
      "loss": 23.4588,
      "step": 10
    },
    {
      "epoch": 0.01725625539257981,
      "grad_norm": 1346.677001953125,
      "learning_rate": 2.994823123382226e-05,
      "loss": 21.5537,
      "step": 20
    },
    {
      "epoch": 0.025884383088869714,
      "grad_norm": 1424.337646484375,
      "learning_rate": 2.992234685073339e-05,
      "loss": 20.5831,
      "step": 30
    },
    {
      "epoch": 0.03451251078515962,
      "grad_norm": 1812.0107421875,
      "learning_rate": 2.9896462467644522e-05,
      "loss": 18.5146,
      "step": 40
    },
    {
      "epoch": 0.04314063848144953,
      "grad_norm": 724.4116821289062,
      "learning_rate": 2.987057808455565e-05,
      "loss": 17.9376,
      "step": 50
    },
    {
      "epoch": 0.05176876617773943,
      "grad_norm": 1902.954345703125,
      "learning_rate": 2.984469370146678e-05,
      "loss": 17.7711,
      "step": 60
    },
    {
      "epoch": 0.060396893874029335,
      "grad_norm": 1052.405517578125,
      "learning_rate": 2.981880931837791e-05,
      "loss": 15.8877,
      "step": 70
    },
    {
      "epoch": 0.06902502157031924,
      "grad_norm": 3851.638427734375,
      "learning_rate": 2.9792924935289043e-05,
      "loss": 15.2982,
      "step": 80
    },
    {
      "epoch": 0.07765314926660914,
      "grad_norm": 396.05218505859375,
      "learning_rate": 2.9767040552200173e-05,
      "loss": 13.5395,
      "step": 90
    },
    {
      "epoch": 0.08628127696289906,
      "grad_norm": 2612.902099609375,
      "learning_rate": 2.9741156169111302e-05,
      "loss": 13.2343,
      "step": 100
    },
    {
      "epoch": 0.09490940465918896,
      "grad_norm": 50974.79296875,
      "learning_rate": 2.9715271786022432e-05,
      "loss": 12.9605,
      "step": 110
    },
    {
      "epoch": 0.10353753235547886,
      "grad_norm": 755.5878295898438,
      "learning_rate": 2.9689387402933565e-05,
      "loss": 12.0287,
      "step": 120
    },
    {
      "epoch": 0.11216566005176877,
      "grad_norm": 718.585205078125,
      "learning_rate": 2.9663503019844694e-05,
      "loss": 11.9107,
      "step": 130
    },
    {
      "epoch": 0.12079378774805867,
      "grad_norm": 213.56773376464844,
      "learning_rate": 2.9637618636755824e-05,
      "loss": 11.2405,
      "step": 140
    },
    {
      "epoch": 0.12942191544434858,
      "grad_norm": 216.77194213867188,
      "learning_rate": 2.9611734253666953e-05,
      "loss": 10.885,
      "step": 150
    },
    {
      "epoch": 0.13805004314063848,
      "grad_norm": 325.8002014160156,
      "learning_rate": 2.9585849870578086e-05,
      "loss": 10.1043,
      "step": 160
    },
    {
      "epoch": 0.14667817083692838,
      "grad_norm": 115.71402740478516,
      "learning_rate": 2.9559965487489215e-05,
      "loss": 9.7571,
      "step": 170
    },
    {
      "epoch": 0.15530629853321828,
      "grad_norm": 1107.6322021484375,
      "learning_rate": 2.9534081104400345e-05,
      "loss": 9.4101,
      "step": 180
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 221.99838256835938,
      "learning_rate": 2.9508196721311474e-05,
      "loss": 8.6902,
      "step": 190
    },
    {
      "epoch": 0.1725625539257981,
      "grad_norm": 80.05264282226562,
      "learning_rate": 2.9482312338222607e-05,
      "loss": 8.217,
      "step": 200
    },
    {
      "epoch": 0.181190681622088,
      "grad_norm": 51.318328857421875,
      "learning_rate": 2.9456427955133737e-05,
      "loss": 7.7812,
      "step": 210
    },
    {
      "epoch": 0.1898188093183779,
      "grad_norm": 65.29530334472656,
      "learning_rate": 2.9430543572044866e-05,
      "loss": 7.7178,
      "step": 220
    },
    {
      "epoch": 0.1984469370146678,
      "grad_norm": 621.398681640625,
      "learning_rate": 2.9404659188955996e-05,
      "loss": 7.2109,
      "step": 230
    },
    {
      "epoch": 0.2070750647109577,
      "grad_norm": 20.492935180664062,
      "learning_rate": 2.937877480586713e-05,
      "loss": 6.895,
      "step": 240
    },
    {
      "epoch": 0.21570319240724764,
      "grad_norm": 176.45736694335938,
      "learning_rate": 2.9352890422778258e-05,
      "loss": 6.5428,
      "step": 250
    },
    {
      "epoch": 0.22433132010353754,
      "grad_norm": 2055.09912109375,
      "learning_rate": 2.9327006039689388e-05,
      "loss": 6.1076,
      "step": 260
    },
    {
      "epoch": 0.23295944779982744,
      "grad_norm": 63.115196228027344,
      "learning_rate": 2.9301121656600517e-05,
      "loss": 5.7377,
      "step": 270
    },
    {
      "epoch": 0.24158757549611734,
      "grad_norm": 50.83503723144531,
      "learning_rate": 2.927523727351165e-05,
      "loss": 5.8146,
      "step": 280
    },
    {
      "epoch": 0.25021570319240727,
      "grad_norm": 28.3941650390625,
      "learning_rate": 2.924935289042278e-05,
      "loss": 5.4843,
      "step": 290
    },
    {
      "epoch": 0.25884383088869717,
      "grad_norm": 53.945396423339844,
      "learning_rate": 2.922346850733391e-05,
      "loss": 5.3836,
      "step": 300
    },
    {
      "epoch": 0.26747195858498707,
      "grad_norm": 23.696733474731445,
      "learning_rate": 2.919758412424504e-05,
      "loss": 4.7356,
      "step": 310
    },
    {
      "epoch": 0.27610008628127697,
      "grad_norm": 72.5272445678711,
      "learning_rate": 2.917169974115617e-05,
      "loss": 5.3798,
      "step": 320
    },
    {
      "epoch": 0.28472821397756687,
      "grad_norm": 257.24884033203125,
      "learning_rate": 2.91458153580673e-05,
      "loss": 4.7993,
      "step": 330
    },
    {
      "epoch": 0.29335634167385677,
      "grad_norm": 16.56666374206543,
      "learning_rate": 2.911993097497843e-05,
      "loss": 4.9596,
      "step": 340
    },
    {
      "epoch": 0.30198446937014667,
      "grad_norm": 38.44278335571289,
      "learning_rate": 2.909404659188956e-05,
      "loss": 4.8459,
      "step": 350
    },
    {
      "epoch": 0.31061259706643657,
      "grad_norm": 21.38633155822754,
      "learning_rate": 2.9068162208800693e-05,
      "loss": 4.467,
      "step": 360
    },
    {
      "epoch": 0.31924072476272647,
      "grad_norm": 127.22117614746094,
      "learning_rate": 2.9042277825711822e-05,
      "loss": 4.5277,
      "step": 370
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 20.243011474609375,
      "learning_rate": 2.901639344262295e-05,
      "loss": 4.1718,
      "step": 380
    },
    {
      "epoch": 0.3364969801553063,
      "grad_norm": 17.071258544921875,
      "learning_rate": 2.899050905953408e-05,
      "loss": 4.2082,
      "step": 390
    },
    {
      "epoch": 0.3451251078515962,
      "grad_norm": 14.369948387145996,
      "learning_rate": 2.8964624676445214e-05,
      "loss": 3.7611,
      "step": 400
    },
    {
      "epoch": 0.3537532355478861,
      "grad_norm": 15.942354202270508,
      "learning_rate": 2.8938740293356343e-05,
      "loss": 3.9986,
      "step": 410
    },
    {
      "epoch": 0.362381363244176,
      "grad_norm": 16.408802032470703,
      "learning_rate": 2.8912855910267473e-05,
      "loss": 3.8547,
      "step": 420
    },
    {
      "epoch": 0.3710094909404659,
      "grad_norm": 32.612220764160156,
      "learning_rate": 2.8886971527178602e-05,
      "loss": 3.6178,
      "step": 430
    },
    {
      "epoch": 0.3796376186367558,
      "grad_norm": 12.077070236206055,
      "learning_rate": 2.8861087144089735e-05,
      "loss": 3.6951,
      "step": 440
    },
    {
      "epoch": 0.3882657463330457,
      "grad_norm": 73.24039459228516,
      "learning_rate": 2.8835202761000865e-05,
      "loss": 3.632,
      "step": 450
    },
    {
      "epoch": 0.3968938740293356,
      "grad_norm": 26.115360260009766,
      "learning_rate": 2.8809318377911994e-05,
      "loss": 3.9022,
      "step": 460
    },
    {
      "epoch": 0.4055220017256255,
      "grad_norm": 36.119205474853516,
      "learning_rate": 2.8783433994823124e-05,
      "loss": 3.5776,
      "step": 470
    },
    {
      "epoch": 0.4141501294219154,
      "grad_norm": 15.902210235595703,
      "learning_rate": 2.8757549611734256e-05,
      "loss": 3.4228,
      "step": 480
    },
    {
      "epoch": 0.4227782571182053,
      "grad_norm": 161.64479064941406,
      "learning_rate": 2.8731665228645386e-05,
      "loss": 3.4346,
      "step": 490
    },
    {
      "epoch": 0.4314063848144953,
      "grad_norm": 9.728084564208984,
      "learning_rate": 2.8705780845556515e-05,
      "loss": 3.7705,
      "step": 500
    },
    {
      "epoch": 0.4400345125107852,
      "grad_norm": 11.695751190185547,
      "learning_rate": 2.8679896462467645e-05,
      "loss": 3.1752,
      "step": 510
    },
    {
      "epoch": 0.4486626402070751,
      "grad_norm": 16.904739379882812,
      "learning_rate": 2.8654012079378778e-05,
      "loss": 3.4358,
      "step": 520
    },
    {
      "epoch": 0.457290767903365,
      "grad_norm": 51.04827117919922,
      "learning_rate": 2.8628127696289907e-05,
      "loss": 2.9479,
      "step": 530
    },
    {
      "epoch": 0.4659188955996549,
      "grad_norm": 31.71221351623535,
      "learning_rate": 2.8602243313201037e-05,
      "loss": 2.669,
      "step": 540
    },
    {
      "epoch": 0.4745470232959448,
      "grad_norm": 18.423744201660156,
      "learning_rate": 2.8576358930112166e-05,
      "loss": 3.0984,
      "step": 550
    },
    {
      "epoch": 0.4831751509922347,
      "grad_norm": 12.26468276977539,
      "learning_rate": 2.85504745470233e-05,
      "loss": 3.1457,
      "step": 560
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 10.608569145202637,
      "learning_rate": 2.8524590163934425e-05,
      "loss": 3.2164,
      "step": 570
    },
    {
      "epoch": 0.5004314063848145,
      "grad_norm": 12.755915641784668,
      "learning_rate": 2.8498705780845558e-05,
      "loss": 2.8646,
      "step": 580
    },
    {
      "epoch": 0.5090595340811044,
      "grad_norm": 59.26004409790039,
      "learning_rate": 2.8472821397756688e-05,
      "loss": 3.031,
      "step": 590
    },
    {
      "epoch": 0.5176876617773943,
      "grad_norm": 15.446871757507324,
      "learning_rate": 2.844693701466782e-05,
      "loss": 3.0349,
      "step": 600
    },
    {
      "epoch": 0.5263157894736842,
      "grad_norm": 9.482240676879883,
      "learning_rate": 2.8421052631578946e-05,
      "loss": 2.9969,
      "step": 610
    },
    {
      "epoch": 0.5349439171699741,
      "grad_norm": 13.548700332641602,
      "learning_rate": 2.8395168248490076e-05,
      "loss": 3.1689,
      "step": 620
    },
    {
      "epoch": 0.543572044866264,
      "grad_norm": 13.112253189086914,
      "learning_rate": 2.836928386540121e-05,
      "loss": 2.6926,
      "step": 630
    },
    {
      "epoch": 0.5522001725625539,
      "grad_norm": 11.312240600585938,
      "learning_rate": 2.834339948231234e-05,
      "loss": 2.93,
      "step": 640
    },
    {
      "epoch": 0.5608283002588438,
      "grad_norm": 10.461165428161621,
      "learning_rate": 2.8317515099223468e-05,
      "loss": 2.959,
      "step": 650
    },
    {
      "epoch": 0.5694564279551337,
      "grad_norm": 9.03032398223877,
      "learning_rate": 2.8291630716134597e-05,
      "loss": 2.918,
      "step": 660
    },
    {
      "epoch": 0.5780845556514237,
      "grad_norm": 12.844351768493652,
      "learning_rate": 2.826574633304573e-05,
      "loss": 2.8902,
      "step": 670
    },
    {
      "epoch": 0.5867126833477135,
      "grad_norm": 33.6794319152832,
      "learning_rate": 2.823986194995686e-05,
      "loss": 2.9339,
      "step": 680
    },
    {
      "epoch": 0.5953408110440035,
      "grad_norm": 7.776472568511963,
      "learning_rate": 2.821397756686799e-05,
      "loss": 2.8553,
      "step": 690
    },
    {
      "epoch": 0.6039689387402933,
      "grad_norm": 7.849714756011963,
      "learning_rate": 2.818809318377912e-05,
      "loss": 2.7912,
      "step": 700
    },
    {
      "epoch": 0.6125970664365833,
      "grad_norm": 7.597228527069092,
      "learning_rate": 2.816220880069025e-05,
      "loss": 2.7338,
      "step": 710
    },
    {
      "epoch": 0.6212251941328731,
      "grad_norm": 15.213830947875977,
      "learning_rate": 2.813632441760138e-05,
      "loss": 2.5763,
      "step": 720
    },
    {
      "epoch": 0.6298533218291631,
      "grad_norm": 67.49362182617188,
      "learning_rate": 2.811044003451251e-05,
      "loss": 2.2355,
      "step": 730
    },
    {
      "epoch": 0.6384814495254529,
      "grad_norm": 17.47430992126465,
      "learning_rate": 2.808455565142364e-05,
      "loss": 2.7791,
      "step": 740
    },
    {
      "epoch": 0.6471095772217429,
      "grad_norm": 9.078857421875,
      "learning_rate": 2.8058671268334773e-05,
      "loss": 2.8407,
      "step": 750
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 16.073314666748047,
      "learning_rate": 2.8032786885245902e-05,
      "loss": 2.5429,
      "step": 760
    },
    {
      "epoch": 0.6643658326143227,
      "grad_norm": 28.346363067626953,
      "learning_rate": 2.8006902502157032e-05,
      "loss": 2.6008,
      "step": 770
    },
    {
      "epoch": 0.6729939603106126,
      "grad_norm": 11.8480224609375,
      "learning_rate": 2.798101811906816e-05,
      "loss": 2.1889,
      "step": 780
    },
    {
      "epoch": 0.6816220880069025,
      "grad_norm": 14.186466217041016,
      "learning_rate": 2.7955133735979294e-05,
      "loss": 2.6072,
      "step": 790
    },
    {
      "epoch": 0.6902502157031924,
      "grad_norm": 10.741659164428711,
      "learning_rate": 2.7929249352890424e-05,
      "loss": 2.3833,
      "step": 800
    },
    {
      "epoch": 0.6988783433994823,
      "grad_norm": 8.368185997009277,
      "learning_rate": 2.7903364969801553e-05,
      "loss": 2.5208,
      "step": 810
    },
    {
      "epoch": 0.7075064710957722,
      "grad_norm": 45.714439392089844,
      "learning_rate": 2.7877480586712683e-05,
      "loss": 2.3551,
      "step": 820
    },
    {
      "epoch": 0.7161345987920621,
      "grad_norm": 10.711389541625977,
      "learning_rate": 2.7851596203623815e-05,
      "loss": 2.4345,
      "step": 830
    },
    {
      "epoch": 0.724762726488352,
      "grad_norm": 10.009029388427734,
      "learning_rate": 2.7825711820534945e-05,
      "loss": 2.5711,
      "step": 840
    },
    {
      "epoch": 0.7333908541846419,
      "grad_norm": 6.669869899749756,
      "learning_rate": 2.7799827437446074e-05,
      "loss": 2.3161,
      "step": 850
    },
    {
      "epoch": 0.7420189818809318,
      "grad_norm": 7.437397480010986,
      "learning_rate": 2.7773943054357204e-05,
      "loss": 2.3243,
      "step": 860
    },
    {
      "epoch": 0.7506471095772217,
      "grad_norm": 6.825283527374268,
      "learning_rate": 2.7748058671268337e-05,
      "loss": 2.47,
      "step": 870
    },
    {
      "epoch": 0.7592752372735116,
      "grad_norm": 8.945544242858887,
      "learning_rate": 2.7722174288179466e-05,
      "loss": 2.6577,
      "step": 880
    },
    {
      "epoch": 0.7679033649698016,
      "grad_norm": 14.798147201538086,
      "learning_rate": 2.7696289905090596e-05,
      "loss": 2.427,
      "step": 890
    },
    {
      "epoch": 0.7765314926660914,
      "grad_norm": 11.005062103271484,
      "learning_rate": 2.7670405522001725e-05,
      "loss": 2.2595,
      "step": 900
    },
    {
      "epoch": 0.7851596203623814,
      "grad_norm": 6.581026554107666,
      "learning_rate": 2.7644521138912858e-05,
      "loss": 2.3416,
      "step": 910
    },
    {
      "epoch": 0.7937877480586712,
      "grad_norm": 11.897897720336914,
      "learning_rate": 2.7618636755823987e-05,
      "loss": 2.1759,
      "step": 920
    },
    {
      "epoch": 0.8024158757549612,
      "grad_norm": 9.87186050415039,
      "learning_rate": 2.7592752372735117e-05,
      "loss": 2.251,
      "step": 930
    },
    {
      "epoch": 0.811044003451251,
      "grad_norm": 10.450788497924805,
      "learning_rate": 2.7566867989646246e-05,
      "loss": 2.3915,
      "step": 940
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 24.39588165283203,
      "learning_rate": 2.754098360655738e-05,
      "loss": 2.5148,
      "step": 950
    },
    {
      "epoch": 0.8283002588438308,
      "grad_norm": 16.458454132080078,
      "learning_rate": 2.751509922346851e-05,
      "loss": 2.3658,
      "step": 960
    },
    {
      "epoch": 0.8369283865401208,
      "grad_norm": 13.174307823181152,
      "learning_rate": 2.7489214840379638e-05,
      "loss": 2.1263,
      "step": 970
    },
    {
      "epoch": 0.8455565142364107,
      "grad_norm": 7.65854549407959,
      "learning_rate": 2.7463330457290768e-05,
      "loss": 2.2837,
      "step": 980
    },
    {
      "epoch": 0.8541846419327006,
      "grad_norm": 27.979938507080078,
      "learning_rate": 2.74374460742019e-05,
      "loss": 2.1062,
      "step": 990
    },
    {
      "epoch": 0.8628127696289906,
      "grad_norm": 151.30169677734375,
      "learning_rate": 2.741156169111303e-05,
      "loss": 2.528,
      "step": 1000
    },
    {
      "epoch": 0.8714408973252804,
      "grad_norm": 59.85219192504883,
      "learning_rate": 2.738567730802416e-05,
      "loss": 2.2947,
      "step": 1010
    },
    {
      "epoch": 0.8800690250215704,
      "grad_norm": 10.714910507202148,
      "learning_rate": 2.735979292493529e-05,
      "loss": 2.273,
      "step": 1020
    },
    {
      "epoch": 0.8886971527178602,
      "grad_norm": 223.82359313964844,
      "learning_rate": 2.7333908541846422e-05,
      "loss": 2.2311,
      "step": 1030
    },
    {
      "epoch": 0.8973252804141502,
      "grad_norm": 6.191257953643799,
      "learning_rate": 2.730802415875755e-05,
      "loss": 2.1423,
      "step": 1040
    },
    {
      "epoch": 0.90595340811044,
      "grad_norm": 8.265118598937988,
      "learning_rate": 2.728213977566868e-05,
      "loss": 2.27,
      "step": 1050
    },
    {
      "epoch": 0.91458153580673,
      "grad_norm": 45.64952850341797,
      "learning_rate": 2.725625539257981e-05,
      "loss": 2.2185,
      "step": 1060
    },
    {
      "epoch": 0.9232096635030198,
      "grad_norm": 7.81273889541626,
      "learning_rate": 2.7230371009490943e-05,
      "loss": 2.2265,
      "step": 1070
    },
    {
      "epoch": 0.9318377911993098,
      "grad_norm": 21.471721649169922,
      "learning_rate": 2.7204486626402073e-05,
      "loss": 2.2766,
      "step": 1080
    },
    {
      "epoch": 0.9404659188955996,
      "grad_norm": 8.191720962524414,
      "learning_rate": 2.7178602243313202e-05,
      "loss": 2.0214,
      "step": 1090
    },
    {
      "epoch": 0.9490940465918896,
      "grad_norm": 16.811010360717773,
      "learning_rate": 2.715271786022433e-05,
      "loss": 2.1013,
      "step": 1100
    },
    {
      "epoch": 0.9577221742881795,
      "grad_norm": 14.321645736694336,
      "learning_rate": 2.7126833477135465e-05,
      "loss": 2.119,
      "step": 1110
    },
    {
      "epoch": 0.9663503019844694,
      "grad_norm": 46.80204391479492,
      "learning_rate": 2.710094909404659e-05,
      "loss": 2.2411,
      "step": 1120
    },
    {
      "epoch": 0.9749784296807593,
      "grad_norm": 7.399768829345703,
      "learning_rate": 2.7075064710957723e-05,
      "loss": 2.0746,
      "step": 1130
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 7.989836692810059,
      "learning_rate": 2.7049180327868853e-05,
      "loss": 2.2652,
      "step": 1140
    },
    {
      "epoch": 0.9922346850733391,
      "grad_norm": 11.654426574707031,
      "learning_rate": 2.7023295944779986e-05,
      "loss": 2.0594,
      "step": 1150
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.134108304977417,
      "eval_runtime": 2.3836,
      "eval_samples_per_second": 208.089,
      "eval_steps_per_second": 26.011,
      "step": 1159
    },
    {
      "epoch": 1.000862812769629,
      "grad_norm": 89.1153564453125,
      "learning_rate": 2.6997411561691112e-05,
      "loss": 2.0267,
      "step": 1160
    },
    {
      "epoch": 1.009490940465919,
      "grad_norm": 10.078282356262207,
      "learning_rate": 2.6971527178602245e-05,
      "loss": 1.8715,
      "step": 1170
    },
    {
      "epoch": 1.0181190681622088,
      "grad_norm": 9.997578620910645,
      "learning_rate": 2.6945642795513374e-05,
      "loss": 2.1161,
      "step": 1180
    },
    {
      "epoch": 1.0267471958584986,
      "grad_norm": 8.007933616638184,
      "learning_rate": 2.6919758412424507e-05,
      "loss": 1.9174,
      "step": 1190
    },
    {
      "epoch": 1.0353753235547887,
      "grad_norm": 19.07499885559082,
      "learning_rate": 2.6893874029335633e-05,
      "loss": 1.9371,
      "step": 1200
    },
    {
      "epoch": 1.0440034512510785,
      "grad_norm": 8.295398712158203,
      "learning_rate": 2.6867989646246766e-05,
      "loss": 1.9425,
      "step": 1210
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 67.6395263671875,
      "learning_rate": 2.6842105263157896e-05,
      "loss": 1.7489,
      "step": 1220
    },
    {
      "epoch": 1.0612597066436584,
      "grad_norm": 12.037497520446777,
      "learning_rate": 2.6816220880069025e-05,
      "loss": 1.9568,
      "step": 1230
    },
    {
      "epoch": 1.0698878343399483,
      "grad_norm": 5.62046480178833,
      "learning_rate": 2.6790336496980155e-05,
      "loss": 1.8129,
      "step": 1240
    },
    {
      "epoch": 1.0785159620362381,
      "grad_norm": 55.346214294433594,
      "learning_rate": 2.6764452113891284e-05,
      "loss": 1.8951,
      "step": 1250
    },
    {
      "epoch": 1.087144089732528,
      "grad_norm": 12.107487678527832,
      "learning_rate": 2.6738567730802417e-05,
      "loss": 1.9087,
      "step": 1260
    },
    {
      "epoch": 1.095772217428818,
      "grad_norm": 7.308590412139893,
      "learning_rate": 2.6712683347713546e-05,
      "loss": 1.7608,
      "step": 1270
    },
    {
      "epoch": 1.1044003451251079,
      "grad_norm": 12.88768482208252,
      "learning_rate": 2.6686798964624676e-05,
      "loss": 1.8461,
      "step": 1280
    },
    {
      "epoch": 1.1130284728213977,
      "grad_norm": 6.3864665031433105,
      "learning_rate": 2.6660914581535805e-05,
      "loss": 1.8499,
      "step": 1290
    },
    {
      "epoch": 1.1216566005176876,
      "grad_norm": 6.429502010345459,
      "learning_rate": 2.6635030198446938e-05,
      "loss": 1.9388,
      "step": 1300
    },
    {
      "epoch": 1.1302847282139776,
      "grad_norm": 8.877351760864258,
      "learning_rate": 2.6609145815358068e-05,
      "loss": 1.8521,
      "step": 1310
    },
    {
      "epoch": 1.1389128559102675,
      "grad_norm": 8.931160926818848,
      "learning_rate": 2.6583261432269197e-05,
      "loss": 1.8164,
      "step": 1320
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 33.71826934814453,
      "learning_rate": 2.6557377049180327e-05,
      "loss": 1.8473,
      "step": 1330
    },
    {
      "epoch": 1.1561691113028472,
      "grad_norm": 10.050326347351074,
      "learning_rate": 2.653149266609146e-05,
      "loss": 2.0251,
      "step": 1340
    },
    {
      "epoch": 1.1647972389991372,
      "grad_norm": 10.82668685913086,
      "learning_rate": 2.650560828300259e-05,
      "loss": 1.8355,
      "step": 1350
    },
    {
      "epoch": 1.173425366695427,
      "grad_norm": 5.326560020446777,
      "learning_rate": 2.647972389991372e-05,
      "loss": 1.5333,
      "step": 1360
    },
    {
      "epoch": 1.182053494391717,
      "grad_norm": 8.264891624450684,
      "learning_rate": 2.6453839516824848e-05,
      "loss": 1.7426,
      "step": 1370
    },
    {
      "epoch": 1.190681622088007,
      "grad_norm": 7.672389984130859,
      "learning_rate": 2.642795513373598e-05,
      "loss": 1.5816,
      "step": 1380
    },
    {
      "epoch": 1.1993097497842968,
      "grad_norm": 13.872163772583008,
      "learning_rate": 2.640207075064711e-05,
      "loss": 1.8851,
      "step": 1390
    },
    {
      "epoch": 1.2079378774805867,
      "grad_norm": 6.059316635131836,
      "learning_rate": 2.637618636755824e-05,
      "loss": 1.8395,
      "step": 1400
    },
    {
      "epoch": 1.2165660051768765,
      "grad_norm": 9.762899398803711,
      "learning_rate": 2.635030198446937e-05,
      "loss": 1.8025,
      "step": 1410
    },
    {
      "epoch": 1.2251941328731666,
      "grad_norm": 50.29071044921875,
      "learning_rate": 2.6324417601380502e-05,
      "loss": 2.0532,
      "step": 1420
    },
    {
      "epoch": 1.2338222605694564,
      "grad_norm": 12.585947036743164,
      "learning_rate": 2.629853321829163e-05,
      "loss": 1.8369,
      "step": 1430
    },
    {
      "epoch": 1.2424503882657463,
      "grad_norm": 7.799862861633301,
      "learning_rate": 2.627264883520276e-05,
      "loss": 1.9144,
      "step": 1440
    },
    {
      "epoch": 1.2510785159620363,
      "grad_norm": 8.881339073181152,
      "learning_rate": 2.624676445211389e-05,
      "loss": 1.8684,
      "step": 1450
    },
    {
      "epoch": 1.2597066436583262,
      "grad_norm": 5.881706237792969,
      "learning_rate": 2.6220880069025023e-05,
      "loss": 1.9603,
      "step": 1460
    },
    {
      "epoch": 1.268334771354616,
      "grad_norm": 10.035075187683105,
      "learning_rate": 2.6194995685936153e-05,
      "loss": 1.829,
      "step": 1470
    },
    {
      "epoch": 1.2769628990509059,
      "grad_norm": 13.3865966796875,
      "learning_rate": 2.6169111302847282e-05,
      "loss": 1.6915,
      "step": 1480
    },
    {
      "epoch": 1.2855910267471957,
      "grad_norm": 14.182516098022461,
      "learning_rate": 2.6143226919758412e-05,
      "loss": 1.765,
      "step": 1490
    },
    {
      "epoch": 1.2942191544434858,
      "grad_norm": 21.645435333251953,
      "learning_rate": 2.6117342536669545e-05,
      "loss": 1.7213,
      "step": 1500
    },
    {
      "epoch": 1.3028472821397756,
      "grad_norm": 13456.8486328125,
      "learning_rate": 2.6091458153580674e-05,
      "loss": 1.9195,
      "step": 1510
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 24.727739334106445,
      "learning_rate": 2.6065573770491804e-05,
      "loss": 1.773,
      "step": 1520
    },
    {
      "epoch": 1.3201035375323555,
      "grad_norm": 14.126154899597168,
      "learning_rate": 2.6039689387402933e-05,
      "loss": 1.7158,
      "step": 1530
    },
    {
      "epoch": 1.3287316652286454,
      "grad_norm": 12.421586036682129,
      "learning_rate": 2.6013805004314066e-05,
      "loss": 1.6306,
      "step": 1540
    },
    {
      "epoch": 1.3373597929249352,
      "grad_norm": 8.854684829711914,
      "learning_rate": 2.5987920621225196e-05,
      "loss": 1.7014,
      "step": 1550
    },
    {
      "epoch": 1.345987920621225,
      "grad_norm": 8.957001686096191,
      "learning_rate": 2.5962036238136325e-05,
      "loss": 1.545,
      "step": 1560
    },
    {
      "epoch": 1.3546160483175151,
      "grad_norm": 8.83320140838623,
      "learning_rate": 2.5936151855047455e-05,
      "loss": 1.5981,
      "step": 1570
    },
    {
      "epoch": 1.363244176013805,
      "grad_norm": 6.80540657043457,
      "learning_rate": 2.5910267471958587e-05,
      "loss": 1.644,
      "step": 1580
    },
    {
      "epoch": 1.3718723037100948,
      "grad_norm": 6.894855499267578,
      "learning_rate": 2.5884383088869717e-05,
      "loss": 1.7088,
      "step": 1590
    },
    {
      "epoch": 1.380500431406385,
      "grad_norm": 31.133745193481445,
      "learning_rate": 2.5858498705780846e-05,
      "loss": 1.7425,
      "step": 1600
    },
    {
      "epoch": 1.3891285591026747,
      "grad_norm": 6.632116794586182,
      "learning_rate": 2.5832614322691976e-05,
      "loss": 1.5735,
      "step": 1610
    },
    {
      "epoch": 1.3977566867989646,
      "grad_norm": 7.241219997406006,
      "learning_rate": 2.580672993960311e-05,
      "loss": 1.8004,
      "step": 1620
    },
    {
      "epoch": 1.4063848144952544,
      "grad_norm": 7.445039749145508,
      "learning_rate": 2.5780845556514238e-05,
      "loss": 1.5554,
      "step": 1630
    },
    {
      "epoch": 1.4150129421915445,
      "grad_norm": 8.450106620788574,
      "learning_rate": 2.5754961173425368e-05,
      "loss": 1.6807,
      "step": 1640
    },
    {
      "epoch": 1.4236410698878343,
      "grad_norm": 6.38145112991333,
      "learning_rate": 2.5729076790336497e-05,
      "loss": 1.9478,
      "step": 1650
    },
    {
      "epoch": 1.4322691975841242,
      "grad_norm": 49.67905807495117,
      "learning_rate": 2.570319240724763e-05,
      "loss": 1.7631,
      "step": 1660
    },
    {
      "epoch": 1.4408973252804143,
      "grad_norm": 86.6795883178711,
      "learning_rate": 2.5677308024158756e-05,
      "loss": 1.5038,
      "step": 1670
    },
    {
      "epoch": 1.449525452976704,
      "grad_norm": 7.316893577575684,
      "learning_rate": 2.565142364106989e-05,
      "loss": 1.7682,
      "step": 1680
    },
    {
      "epoch": 1.458153580672994,
      "grad_norm": 5.97777795791626,
      "learning_rate": 2.562553925798102e-05,
      "loss": 1.6942,
      "step": 1690
    },
    {
      "epoch": 1.4667817083692838,
      "grad_norm": 128.07574462890625,
      "learning_rate": 2.559965487489215e-05,
      "loss": 1.6631,
      "step": 1700
    },
    {
      "epoch": 1.4754098360655736,
      "grad_norm": 10.68877124786377,
      "learning_rate": 2.5573770491803277e-05,
      "loss": 1.7182,
      "step": 1710
    },
    {
      "epoch": 1.4840379637618637,
      "grad_norm": 7.537623405456543,
      "learning_rate": 2.554788610871441e-05,
      "loss": 1.6163,
      "step": 1720
    },
    {
      "epoch": 1.4926660914581535,
      "grad_norm": 20.54130744934082,
      "learning_rate": 2.552200172562554e-05,
      "loss": 1.5746,
      "step": 1730
    },
    {
      "epoch": 1.5012942191544436,
      "grad_norm": 12.928511619567871,
      "learning_rate": 2.5496117342536673e-05,
      "loss": 1.6308,
      "step": 1740
    },
    {
      "epoch": 1.5099223468507335,
      "grad_norm": 9.859183311462402,
      "learning_rate": 2.54702329594478e-05,
      "loss": 1.6121,
      "step": 1750
    },
    {
      "epoch": 1.5185504745470233,
      "grad_norm": 81.01475524902344,
      "learning_rate": 2.544434857635893e-05,
      "loss": 1.5736,
      "step": 1760
    },
    {
      "epoch": 1.5271786022433131,
      "grad_norm": 6.830519676208496,
      "learning_rate": 2.541846419327006e-05,
      "loss": 1.6272,
      "step": 1770
    },
    {
      "epoch": 1.535806729939603,
      "grad_norm": 5.9912614822387695,
      "learning_rate": 2.5392579810181194e-05,
      "loss": 1.4047,
      "step": 1780
    },
    {
      "epoch": 1.544434857635893,
      "grad_norm": 6.846096038818359,
      "learning_rate": 2.536669542709232e-05,
      "loss": 1.6938,
      "step": 1790
    },
    {
      "epoch": 1.553062985332183,
      "grad_norm": 14.697179794311523,
      "learning_rate": 2.5340811044003453e-05,
      "loss": 1.398,
      "step": 1800
    },
    {
      "epoch": 1.561691113028473,
      "grad_norm": 19.513072967529297,
      "learning_rate": 2.5314926660914582e-05,
      "loss": 1.8825,
      "step": 1810
    },
    {
      "epoch": 1.5703192407247628,
      "grad_norm": 6.020417213439941,
      "learning_rate": 2.5289042277825715e-05,
      "loss": 1.5655,
      "step": 1820
    },
    {
      "epoch": 1.5789473684210527,
      "grad_norm": 10.565171241760254,
      "learning_rate": 2.526315789473684e-05,
      "loss": 1.4478,
      "step": 1830
    },
    {
      "epoch": 1.5875754961173425,
      "grad_norm": 13.357294082641602,
      "learning_rate": 2.523727351164797e-05,
      "loss": 1.5236,
      "step": 1840
    },
    {
      "epoch": 1.5962036238136323,
      "grad_norm": 6.293725490570068,
      "learning_rate": 2.5211389128559104e-05,
      "loss": 1.4483,
      "step": 1850
    },
    {
      "epoch": 1.6048317515099222,
      "grad_norm": 9.626404762268066,
      "learning_rate": 2.5185504745470233e-05,
      "loss": 1.4723,
      "step": 1860
    },
    {
      "epoch": 1.6134598792062123,
      "grad_norm": 7.798983573913574,
      "learning_rate": 2.5159620362381363e-05,
      "loss": 1.4487,
      "step": 1870
    },
    {
      "epoch": 1.6220880069025023,
      "grad_norm": 10.429298400878906,
      "learning_rate": 2.5133735979292492e-05,
      "loss": 1.7969,
      "step": 1880
    },
    {
      "epoch": 1.6307161345987922,
      "grad_norm": 10.036585807800293,
      "learning_rate": 2.5107851596203625e-05,
      "loss": 1.49,
      "step": 1890
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 7.652751922607422,
      "learning_rate": 2.5081967213114754e-05,
      "loss": 1.1691,
      "step": 1900
    },
    {
      "epoch": 1.6479723899913719,
      "grad_norm": 8.47938346862793,
      "learning_rate": 2.5056082830025884e-05,
      "loss": 1.5752,
      "step": 1910
    },
    {
      "epoch": 1.6566005176876617,
      "grad_norm": 6.094216823577881,
      "learning_rate": 2.5030198446937013e-05,
      "loss": 1.5,
      "step": 1920
    },
    {
      "epoch": 1.6652286453839515,
      "grad_norm": 9.005218505859375,
      "learning_rate": 2.5004314063848146e-05,
      "loss": 1.4184,
      "step": 1930
    },
    {
      "epoch": 1.6738567730802416,
      "grad_norm": 10.289375305175781,
      "learning_rate": 2.4978429680759276e-05,
      "loss": 1.4898,
      "step": 1940
    },
    {
      "epoch": 1.6824849007765315,
      "grad_norm": 6.6216864585876465,
      "learning_rate": 2.4952545297670405e-05,
      "loss": 1.3479,
      "step": 1950
    },
    {
      "epoch": 1.6911130284728215,
      "grad_norm": 6.357244968414307,
      "learning_rate": 2.4926660914581535e-05,
      "loss": 1.5405,
      "step": 1960
    },
    {
      "epoch": 1.6997411561691114,
      "grad_norm": 5.508429527282715,
      "learning_rate": 2.4900776531492668e-05,
      "loss": 1.5503,
      "step": 1970
    },
    {
      "epoch": 1.7083692838654012,
      "grad_norm": 9.138825416564941,
      "learning_rate": 2.4874892148403797e-05,
      "loss": 1.3605,
      "step": 1980
    },
    {
      "epoch": 1.716997411561691,
      "grad_norm": 7.424917697906494,
      "learning_rate": 2.4849007765314927e-05,
      "loss": 1.6433,
      "step": 1990
    },
    {
      "epoch": 1.725625539257981,
      "grad_norm": 23.203533172607422,
      "learning_rate": 2.4823123382226056e-05,
      "loss": 1.2999,
      "step": 2000
    },
    {
      "epoch": 1.734253666954271,
      "grad_norm": 7.636025428771973,
      "learning_rate": 2.479723899913719e-05,
      "loss": 1.742,
      "step": 2010
    },
    {
      "epoch": 1.7428817946505608,
      "grad_norm": 7.443998336791992,
      "learning_rate": 2.477135461604832e-05,
      "loss": 1.5058,
      "step": 2020
    },
    {
      "epoch": 1.7515099223468509,
      "grad_norm": 7.4948954582214355,
      "learning_rate": 2.4745470232959448e-05,
      "loss": 1.1988,
      "step": 2030
    },
    {
      "epoch": 1.7601380500431407,
      "grad_norm": 25.565855026245117,
      "learning_rate": 2.4719585849870577e-05,
      "loss": 1.5423,
      "step": 2040
    },
    {
      "epoch": 1.7687661777394306,
      "grad_norm": 44.71356201171875,
      "learning_rate": 2.469370146678171e-05,
      "loss": 1.4778,
      "step": 2050
    },
    {
      "epoch": 1.7773943054357204,
      "grad_norm": 22.066511154174805,
      "learning_rate": 2.466781708369284e-05,
      "loss": 1.7414,
      "step": 2060
    },
    {
      "epoch": 1.7860224331320103,
      "grad_norm": 6.305901527404785,
      "learning_rate": 2.464193270060397e-05,
      "loss": 1.5877,
      "step": 2070
    },
    {
      "epoch": 1.7946505608283,
      "grad_norm": 13.126606941223145,
      "learning_rate": 2.46160483175151e-05,
      "loss": 1.5639,
      "step": 2080
    },
    {
      "epoch": 1.8032786885245902,
      "grad_norm": 6.893477916717529,
      "learning_rate": 2.459016393442623e-05,
      "loss": 1.3965,
      "step": 2090
    },
    {
      "epoch": 1.8119068162208802,
      "grad_norm": 6.365148067474365,
      "learning_rate": 2.456427955133736e-05,
      "loss": 1.3212,
      "step": 2100
    },
    {
      "epoch": 1.82053494391717,
      "grad_norm": 7.883370876312256,
      "learning_rate": 2.453839516824849e-05,
      "loss": 1.4368,
      "step": 2110
    },
    {
      "epoch": 1.82916307161346,
      "grad_norm": 6.836297988891602,
      "learning_rate": 2.451251078515962e-05,
      "loss": 1.4884,
      "step": 2120
    },
    {
      "epoch": 1.8377911993097498,
      "grad_norm": 5.644900798797607,
      "learning_rate": 2.4486626402070753e-05,
      "loss": 1.4419,
      "step": 2130
    },
    {
      "epoch": 1.8464193270060396,
      "grad_norm": 9.614646911621094,
      "learning_rate": 2.4460742018981882e-05,
      "loss": 1.4483,
      "step": 2140
    },
    {
      "epoch": 1.8550474547023295,
      "grad_norm": 8.060343742370605,
      "learning_rate": 2.4434857635893012e-05,
      "loss": 1.5212,
      "step": 2150
    },
    {
      "epoch": 1.8636755823986195,
      "grad_norm": 10.414228439331055,
      "learning_rate": 2.440897325280414e-05,
      "loss": 1.3548,
      "step": 2160
    },
    {
      "epoch": 1.8723037100949094,
      "grad_norm": 74.3147201538086,
      "learning_rate": 2.4383088869715274e-05,
      "loss": 1.4671,
      "step": 2170
    },
    {
      "epoch": 1.8809318377911994,
      "grad_norm": 5.692623615264893,
      "learning_rate": 2.4357204486626404e-05,
      "loss": 1.4106,
      "step": 2180
    },
    {
      "epoch": 1.8895599654874893,
      "grad_norm": 8.846437454223633,
      "learning_rate": 2.4331320103537533e-05,
      "loss": 1.4713,
      "step": 2190
    },
    {
      "epoch": 1.8981880931837791,
      "grad_norm": 6.808058261871338,
      "learning_rate": 2.4305435720448663e-05,
      "loss": 1.4683,
      "step": 2200
    },
    {
      "epoch": 1.906816220880069,
      "grad_norm": 8.639785766601562,
      "learning_rate": 2.4279551337359795e-05,
      "loss": 1.2813,
      "step": 2210
    },
    {
      "epoch": 1.9154443485763588,
      "grad_norm": 6.628626823425293,
      "learning_rate": 2.425366695427092e-05,
      "loss": 1.4857,
      "step": 2220
    },
    {
      "epoch": 1.9240724762726489,
      "grad_norm": 16.902395248413086,
      "learning_rate": 2.4227782571182054e-05,
      "loss": 1.4624,
      "step": 2230
    },
    {
      "epoch": 1.9327006039689387,
      "grad_norm": 12.580864906311035,
      "learning_rate": 2.4201898188093184e-05,
      "loss": 1.3581,
      "step": 2240
    },
    {
      "epoch": 1.9413287316652288,
      "grad_norm": 6.581643581390381,
      "learning_rate": 2.4176013805004317e-05,
      "loss": 1.3442,
      "step": 2250
    },
    {
      "epoch": 1.9499568593615186,
      "grad_norm": 7.187539100646973,
      "learning_rate": 2.4150129421915443e-05,
      "loss": 1.5357,
      "step": 2260
    },
    {
      "epoch": 1.9585849870578085,
      "grad_norm": 10.964424133300781,
      "learning_rate": 2.4124245038826576e-05,
      "loss": 1.3456,
      "step": 2270
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 7.286751747131348,
      "learning_rate": 2.4098360655737705e-05,
      "loss": 1.3881,
      "step": 2280
    },
    {
      "epoch": 1.9758412424503882,
      "grad_norm": 7.921073913574219,
      "learning_rate": 2.4072476272648838e-05,
      "loss": 1.3025,
      "step": 2290
    },
    {
      "epoch": 1.984469370146678,
      "grad_norm": 4.943100452423096,
      "learning_rate": 2.4046591889559964e-05,
      "loss": 1.517,
      "step": 2300
    },
    {
      "epoch": 1.993097497842968,
      "grad_norm": 6.17785120010376,
      "learning_rate": 2.4020707506471097e-05,
      "loss": 1.2047,
      "step": 2310
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.7386850118637085,
      "eval_runtime": 2.5824,
      "eval_samples_per_second": 192.07,
      "eval_steps_per_second": 24.009,
      "step": 2318
    },
    {
      "epoch": 2.001725625539258,
      "grad_norm": 9.696246147155762,
      "learning_rate": 2.3994823123382227e-05,
      "loss": 1.3599,
      "step": 2320
    },
    {
      "epoch": 2.010353753235548,
      "grad_norm": 6.25351619720459,
      "learning_rate": 2.396893874029336e-05,
      "loss": 1.4787,
      "step": 2330
    },
    {
      "epoch": 2.018981880931838,
      "grad_norm": 7.294634819030762,
      "learning_rate": 2.3943054357204485e-05,
      "loss": 1.2632,
      "step": 2340
    },
    {
      "epoch": 2.0276100086281277,
      "grad_norm": 11.416504859924316,
      "learning_rate": 2.391716997411562e-05,
      "loss": 1.4493,
      "step": 2350
    },
    {
      "epoch": 2.0362381363244175,
      "grad_norm": 11.002779960632324,
      "learning_rate": 2.3891285591026748e-05,
      "loss": 1.3522,
      "step": 2360
    },
    {
      "epoch": 2.0448662640207074,
      "grad_norm": 5.741383075714111,
      "learning_rate": 2.386540120793788e-05,
      "loss": 1.2924,
      "step": 2370
    },
    {
      "epoch": 2.053494391716997,
      "grad_norm": 4.8727498054504395,
      "learning_rate": 2.3839516824849007e-05,
      "loss": 1.1698,
      "step": 2380
    },
    {
      "epoch": 2.0621225194132875,
      "grad_norm": 8.68979549407959,
      "learning_rate": 2.381363244176014e-05,
      "loss": 1.4591,
      "step": 2390
    },
    {
      "epoch": 2.0707506471095773,
      "grad_norm": 5.7548112869262695,
      "learning_rate": 2.378774805867127e-05,
      "loss": 1.2911,
      "step": 2400
    },
    {
      "epoch": 2.079378774805867,
      "grad_norm": 15.738356590270996,
      "learning_rate": 2.3761863675582402e-05,
      "loss": 1.4574,
      "step": 2410
    },
    {
      "epoch": 2.088006902502157,
      "grad_norm": 10.980202674865723,
      "learning_rate": 2.3735979292493528e-05,
      "loss": 1.2675,
      "step": 2420
    },
    {
      "epoch": 2.096635030198447,
      "grad_norm": 8.618746757507324,
      "learning_rate": 2.371009490940466e-05,
      "loss": 1.3361,
      "step": 2430
    },
    {
      "epoch": 2.1052631578947367,
      "grad_norm": 7.41761589050293,
      "learning_rate": 2.368421052631579e-05,
      "loss": 1.2584,
      "step": 2440
    },
    {
      "epoch": 2.1138912855910266,
      "grad_norm": 4.283700942993164,
      "learning_rate": 2.365832614322692e-05,
      "loss": 1.1781,
      "step": 2450
    },
    {
      "epoch": 2.122519413287317,
      "grad_norm": 5.084280014038086,
      "learning_rate": 2.363244176013805e-05,
      "loss": 1.283,
      "step": 2460
    },
    {
      "epoch": 2.1311475409836067,
      "grad_norm": 35.1397819519043,
      "learning_rate": 2.360655737704918e-05,
      "loss": 1.5453,
      "step": 2470
    },
    {
      "epoch": 2.1397756686798965,
      "grad_norm": 8.53515625,
      "learning_rate": 2.3580672993960312e-05,
      "loss": 1.2325,
      "step": 2480
    },
    {
      "epoch": 2.1484037963761864,
      "grad_norm": 7.607666492462158,
      "learning_rate": 2.355478861087144e-05,
      "loss": 1.2446,
      "step": 2490
    },
    {
      "epoch": 2.1570319240724762,
      "grad_norm": 5.915863037109375,
      "learning_rate": 2.352890422778257e-05,
      "loss": 1.302,
      "step": 2500
    },
    {
      "epoch": 2.165660051768766,
      "grad_norm": 28.91741180419922,
      "learning_rate": 2.35030198446937e-05,
      "loss": 1.4334,
      "step": 2510
    },
    {
      "epoch": 2.174288179465056,
      "grad_norm": 7.441214084625244,
      "learning_rate": 2.3477135461604833e-05,
      "loss": 1.3055,
      "step": 2520
    },
    {
      "epoch": 2.1829163071613458,
      "grad_norm": 7.928105354309082,
      "learning_rate": 2.3451251078515963e-05,
      "loss": 1.1678,
      "step": 2530
    },
    {
      "epoch": 2.191544434857636,
      "grad_norm": 72.15815734863281,
      "learning_rate": 2.3425366695427092e-05,
      "loss": 1.4805,
      "step": 2540
    },
    {
      "epoch": 2.200172562553926,
      "grad_norm": 6.493410587310791,
      "learning_rate": 2.339948231233822e-05,
      "loss": 1.1744,
      "step": 2550
    },
    {
      "epoch": 2.2088006902502157,
      "grad_norm": 29.801326751708984,
      "learning_rate": 2.3373597929249354e-05,
      "loss": 1.3464,
      "step": 2560
    },
    {
      "epoch": 2.2174288179465056,
      "grad_norm": 8.92724323272705,
      "learning_rate": 2.3347713546160484e-05,
      "loss": 1.0677,
      "step": 2570
    },
    {
      "epoch": 2.2260569456427954,
      "grad_norm": 5.579065799713135,
      "learning_rate": 2.3321829163071613e-05,
      "loss": 1.2856,
      "step": 2580
    },
    {
      "epoch": 2.2346850733390853,
      "grad_norm": 137.35647583007812,
      "learning_rate": 2.3295944779982743e-05,
      "loss": 1.2691,
      "step": 2590
    },
    {
      "epoch": 2.243313201035375,
      "grad_norm": 5.719179630279541,
      "learning_rate": 2.3270060396893876e-05,
      "loss": 1.3654,
      "step": 2600
    },
    {
      "epoch": 2.2519413287316654,
      "grad_norm": 16.967098236083984,
      "learning_rate": 2.3244176013805005e-05,
      "loss": 1.2608,
      "step": 2610
    },
    {
      "epoch": 2.2605694564279553,
      "grad_norm": 10.454100608825684,
      "learning_rate": 2.3218291630716135e-05,
      "loss": 1.1882,
      "step": 2620
    },
    {
      "epoch": 2.269197584124245,
      "grad_norm": 11.139822006225586,
      "learning_rate": 2.3192407247627264e-05,
      "loss": 1.1731,
      "step": 2630
    },
    {
      "epoch": 2.277825711820535,
      "grad_norm": 7.189225673675537,
      "learning_rate": 2.3166522864538397e-05,
      "loss": 1.3283,
      "step": 2640
    },
    {
      "epoch": 2.286453839516825,
      "grad_norm": 9.587203979492188,
      "learning_rate": 2.3140638481449526e-05,
      "loss": 1.5955,
      "step": 2650
    },
    {
      "epoch": 2.2950819672131146,
      "grad_norm": 7.59979248046875,
      "learning_rate": 2.3114754098360656e-05,
      "loss": 1.2575,
      "step": 2660
    },
    {
      "epoch": 2.3037100949094045,
      "grad_norm": 10.917623519897461,
      "learning_rate": 2.3088869715271785e-05,
      "loss": 1.2725,
      "step": 2670
    },
    {
      "epoch": 2.3123382226056943,
      "grad_norm": 19.220064163208008,
      "learning_rate": 2.3062985332182918e-05,
      "loss": 1.3257,
      "step": 2680
    },
    {
      "epoch": 2.3209663503019846,
      "grad_norm": 18.920522689819336,
      "learning_rate": 2.3037100949094048e-05,
      "loss": 1.3284,
      "step": 2690
    },
    {
      "epoch": 2.3295944779982745,
      "grad_norm": 11.986990928649902,
      "learning_rate": 2.3011216566005177e-05,
      "loss": 1.2165,
      "step": 2700
    },
    {
      "epoch": 2.3382226056945643,
      "grad_norm": 10.620926856994629,
      "learning_rate": 2.2985332182916307e-05,
      "loss": 1.2273,
      "step": 2710
    },
    {
      "epoch": 2.346850733390854,
      "grad_norm": 14.089614868164062,
      "learning_rate": 2.295944779982744e-05,
      "loss": 1.2557,
      "step": 2720
    },
    {
      "epoch": 2.355478861087144,
      "grad_norm": 13.761335372924805,
      "learning_rate": 2.293356341673857e-05,
      "loss": 1.2352,
      "step": 2730
    },
    {
      "epoch": 2.364106988783434,
      "grad_norm": 5.856708526611328,
      "learning_rate": 2.29076790336497e-05,
      "loss": 1.1851,
      "step": 2740
    },
    {
      "epoch": 2.372735116479724,
      "grad_norm": 20.02356719970703,
      "learning_rate": 2.2881794650560828e-05,
      "loss": 1.3125,
      "step": 2750
    },
    {
      "epoch": 2.381363244176014,
      "grad_norm": 8.490787506103516,
      "learning_rate": 2.285591026747196e-05,
      "loss": 1.1911,
      "step": 2760
    },
    {
      "epoch": 2.389991371872304,
      "grad_norm": 6.93238639831543,
      "learning_rate": 2.2830025884383087e-05,
      "loss": 1.3159,
      "step": 2770
    },
    {
      "epoch": 2.3986194995685937,
      "grad_norm": 7.33854866027832,
      "learning_rate": 2.280414150129422e-05,
      "loss": 1.4176,
      "step": 2780
    },
    {
      "epoch": 2.4072476272648835,
      "grad_norm": 11.600375175476074,
      "learning_rate": 2.277825711820535e-05,
      "loss": 1.2766,
      "step": 2790
    },
    {
      "epoch": 2.4158757549611733,
      "grad_norm": 9.240166664123535,
      "learning_rate": 2.2752372735116482e-05,
      "loss": 1.2549,
      "step": 2800
    },
    {
      "epoch": 2.424503882657463,
      "grad_norm": 14.630492210388184,
      "learning_rate": 2.2726488352027608e-05,
      "loss": 1.4128,
      "step": 2810
    },
    {
      "epoch": 2.433132010353753,
      "grad_norm": 4.585143089294434,
      "learning_rate": 2.270060396893874e-05,
      "loss": 1.2716,
      "step": 2820
    },
    {
      "epoch": 2.4417601380500433,
      "grad_norm": 11.982229232788086,
      "learning_rate": 2.267471958584987e-05,
      "loss": 1.219,
      "step": 2830
    },
    {
      "epoch": 2.450388265746333,
      "grad_norm": 8.093182563781738,
      "learning_rate": 2.2648835202761004e-05,
      "loss": 1.2535,
      "step": 2840
    },
    {
      "epoch": 2.459016393442623,
      "grad_norm": 8.462604522705078,
      "learning_rate": 2.262295081967213e-05,
      "loss": 1.3256,
      "step": 2850
    },
    {
      "epoch": 2.467644521138913,
      "grad_norm": 10.552114486694336,
      "learning_rate": 2.2597066436583262e-05,
      "loss": 1.1018,
      "step": 2860
    },
    {
      "epoch": 2.4762726488352027,
      "grad_norm": 7.056125640869141,
      "learning_rate": 2.2571182053494392e-05,
      "loss": 1.1498,
      "step": 2870
    },
    {
      "epoch": 2.4849007765314925,
      "grad_norm": 8.379191398620605,
      "learning_rate": 2.2545297670405525e-05,
      "loss": 1.3135,
      "step": 2880
    },
    {
      "epoch": 2.4935289042277824,
      "grad_norm": 8.831942558288574,
      "learning_rate": 2.251941328731665e-05,
      "loss": 1.231,
      "step": 2890
    },
    {
      "epoch": 2.5021570319240727,
      "grad_norm": 36.932899475097656,
      "learning_rate": 2.2493528904227784e-05,
      "loss": 1.4287,
      "step": 2900
    },
    {
      "epoch": 2.5107851596203625,
      "grad_norm": 8.782482147216797,
      "learning_rate": 2.2467644521138913e-05,
      "loss": 1.1496,
      "step": 2910
    },
    {
      "epoch": 2.5194132873166524,
      "grad_norm": 5.703675270080566,
      "learning_rate": 2.2441760138050046e-05,
      "loss": 1.1704,
      "step": 2920
    },
    {
      "epoch": 2.528041415012942,
      "grad_norm": 6.630077362060547,
      "learning_rate": 2.2415875754961172e-05,
      "loss": 1.1868,
      "step": 2930
    },
    {
      "epoch": 2.536669542709232,
      "grad_norm": 281.8606872558594,
      "learning_rate": 2.2389991371872305e-05,
      "loss": 1.4188,
      "step": 2940
    },
    {
      "epoch": 2.545297670405522,
      "grad_norm": 7.36943244934082,
      "learning_rate": 2.2364106988783435e-05,
      "loss": 1.4532,
      "step": 2950
    },
    {
      "epoch": 2.5539257981018118,
      "grad_norm": 7.784780502319336,
      "learning_rate": 2.2338222605694567e-05,
      "loss": 1.2562,
      "step": 2960
    },
    {
      "epoch": 2.5625539257981016,
      "grad_norm": 9.80643367767334,
      "learning_rate": 2.2312338222605694e-05,
      "loss": 1.2751,
      "step": 2970
    },
    {
      "epoch": 2.5711820534943914,
      "grad_norm": 6.832348346710205,
      "learning_rate": 2.2286453839516826e-05,
      "loss": 1.435,
      "step": 2980
    },
    {
      "epoch": 2.5798101811906817,
      "grad_norm": 15.405223846435547,
      "learning_rate": 2.2260569456427956e-05,
      "loss": 1.2381,
      "step": 2990
    },
    {
      "epoch": 2.5884383088869716,
      "grad_norm": 6.314146041870117,
      "learning_rate": 2.223468507333909e-05,
      "loss": 1.3748,
      "step": 3000
    },
    {
      "epoch": 2.5970664365832614,
      "grad_norm": 5.285006999969482,
      "learning_rate": 2.2208800690250215e-05,
      "loss": 1.0856,
      "step": 3010
    },
    {
      "epoch": 2.6056945642795513,
      "grad_norm": 6.184755325317383,
      "learning_rate": 2.2182916307161348e-05,
      "loss": 1.1943,
      "step": 3020
    },
    {
      "epoch": 2.614322691975841,
      "grad_norm": 9.009148597717285,
      "learning_rate": 2.2157031924072477e-05,
      "loss": 1.1533,
      "step": 3030
    },
    {
      "epoch": 2.6229508196721314,
      "grad_norm": 12.079272270202637,
      "learning_rate": 2.213114754098361e-05,
      "loss": 1.0613,
      "step": 3040
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 6.028000831604004,
      "learning_rate": 2.2105263157894736e-05,
      "loss": 1.292,
      "step": 3050
    },
    {
      "epoch": 2.640207075064711,
      "grad_norm": 6.428757667541504,
      "learning_rate": 2.2079378774805866e-05,
      "loss": 1.141,
      "step": 3060
    },
    {
      "epoch": 2.648835202761001,
      "grad_norm": 22.693065643310547,
      "learning_rate": 2.2053494391717e-05,
      "loss": 1.4538,
      "step": 3070
    },
    {
      "epoch": 2.6574633304572908,
      "grad_norm": 10.089692115783691,
      "learning_rate": 2.2027610008628128e-05,
      "loss": 1.1472,
      "step": 3080
    },
    {
      "epoch": 2.6660914581535806,
      "grad_norm": 6.781190395355225,
      "learning_rate": 2.2001725625539257e-05,
      "loss": 1.1527,
      "step": 3090
    },
    {
      "epoch": 2.6747195858498705,
      "grad_norm": 5.898265838623047,
      "learning_rate": 2.1975841242450387e-05,
      "loss": 0.9799,
      "step": 3100
    },
    {
      "epoch": 2.6833477135461603,
      "grad_norm": 5.235626697540283,
      "learning_rate": 2.194995685936152e-05,
      "loss": 1.0495,
      "step": 3110
    },
    {
      "epoch": 2.69197584124245,
      "grad_norm": 6.496643543243408,
      "learning_rate": 2.192407247627265e-05,
      "loss": 1.1082,
      "step": 3120
    },
    {
      "epoch": 2.7006039689387404,
      "grad_norm": 4.9831390380859375,
      "learning_rate": 2.189818809318378e-05,
      "loss": 1.0243,
      "step": 3130
    },
    {
      "epoch": 2.7092320966350303,
      "grad_norm": 7.168632507324219,
      "learning_rate": 2.1872303710094908e-05,
      "loss": 1.1128,
      "step": 3140
    },
    {
      "epoch": 2.71786022433132,
      "grad_norm": 9.249313354492188,
      "learning_rate": 2.184641932700604e-05,
      "loss": 1.2599,
      "step": 3150
    },
    {
      "epoch": 2.72648835202761,
      "grad_norm": 7.809959888458252,
      "learning_rate": 2.182053494391717e-05,
      "loss": 1.1253,
      "step": 3160
    },
    {
      "epoch": 2.7351164797239,
      "grad_norm": 5.825734615325928,
      "learning_rate": 2.17946505608283e-05,
      "loss": 1.1692,
      "step": 3170
    },
    {
      "epoch": 2.7437446074201897,
      "grad_norm": 9.881051063537598,
      "learning_rate": 2.176876617773943e-05,
      "loss": 1.1442,
      "step": 3180
    },
    {
      "epoch": 2.75237273511648,
      "grad_norm": 15.14118766784668,
      "learning_rate": 2.1742881794650562e-05,
      "loss": 1.3486,
      "step": 3190
    },
    {
      "epoch": 2.76100086281277,
      "grad_norm": 8.337268829345703,
      "learning_rate": 2.1716997411561692e-05,
      "loss": 1.3258,
      "step": 3200
    },
    {
      "epoch": 2.7696289905090596,
      "grad_norm": 6.791463375091553,
      "learning_rate": 2.169111302847282e-05,
      "loss": 1.1286,
      "step": 3210
    },
    {
      "epoch": 2.7782571182053495,
      "grad_norm": 6.884166240692139,
      "learning_rate": 2.166522864538395e-05,
      "loss": 1.185,
      "step": 3220
    },
    {
      "epoch": 2.7868852459016393,
      "grad_norm": 83.66976165771484,
      "learning_rate": 2.1639344262295084e-05,
      "loss": 1.208,
      "step": 3230
    },
    {
      "epoch": 2.795513373597929,
      "grad_norm": 56.75058364868164,
      "learning_rate": 2.1613459879206213e-05,
      "loss": 1.1081,
      "step": 3240
    },
    {
      "epoch": 2.804141501294219,
      "grad_norm": 11.336503028869629,
      "learning_rate": 2.1587575496117343e-05,
      "loss": 1.4023,
      "step": 3250
    },
    {
      "epoch": 2.812769628990509,
      "grad_norm": 8.212331771850586,
      "learning_rate": 2.1561691113028472e-05,
      "loss": 1.0464,
      "step": 3260
    },
    {
      "epoch": 2.8213977566867987,
      "grad_norm": 210.0343017578125,
      "learning_rate": 2.1535806729939605e-05,
      "loss": 1.2861,
      "step": 3270
    },
    {
      "epoch": 2.830025884383089,
      "grad_norm": 6.4173808097839355,
      "learning_rate": 2.1509922346850735e-05,
      "loss": 1.1835,
      "step": 3280
    },
    {
      "epoch": 2.838654012079379,
      "grad_norm": 6.797337532043457,
      "learning_rate": 2.1484037963761864e-05,
      "loss": 1.145,
      "step": 3290
    },
    {
      "epoch": 2.8472821397756687,
      "grad_norm": 4.722661972045898,
      "learning_rate": 2.1458153580672993e-05,
      "loss": 1.1514,
      "step": 3300
    },
    {
      "epoch": 2.8559102674719585,
      "grad_norm": 5.724203586578369,
      "learning_rate": 2.1432269197584126e-05,
      "loss": 1.155,
      "step": 3310
    },
    {
      "epoch": 2.8645383951682484,
      "grad_norm": 6.008789539337158,
      "learning_rate": 2.1406384814495252e-05,
      "loss": 1.2733,
      "step": 3320
    },
    {
      "epoch": 2.8731665228645387,
      "grad_norm": 5.547825336456299,
      "learning_rate": 2.1380500431406385e-05,
      "loss": 1.2803,
      "step": 3330
    },
    {
      "epoch": 2.8817946505608285,
      "grad_norm": 6.595117568969727,
      "learning_rate": 2.1354616048317515e-05,
      "loss": 1.0253,
      "step": 3340
    },
    {
      "epoch": 2.8904227782571184,
      "grad_norm": 9.838805198669434,
      "learning_rate": 2.1328731665228648e-05,
      "loss": 1.0696,
      "step": 3350
    },
    {
      "epoch": 2.899050905953408,
      "grad_norm": 14.960533142089844,
      "learning_rate": 2.1302847282139774e-05,
      "loss": 1.2125,
      "step": 3360
    },
    {
      "epoch": 2.907679033649698,
      "grad_norm": 4.978977203369141,
      "learning_rate": 2.1276962899050907e-05,
      "loss": 1.0809,
      "step": 3370
    },
    {
      "epoch": 2.916307161345988,
      "grad_norm": 8.063459396362305,
      "learning_rate": 2.1251078515962036e-05,
      "loss": 1.1537,
      "step": 3380
    },
    {
      "epoch": 2.9249352890422777,
      "grad_norm": 6.728475093841553,
      "learning_rate": 2.122519413287317e-05,
      "loss": 1.3473,
      "step": 3390
    },
    {
      "epoch": 2.9335634167385676,
      "grad_norm": 6.5154805183410645,
      "learning_rate": 2.1199309749784295e-05,
      "loss": 1.2034,
      "step": 3400
    },
    {
      "epoch": 2.9421915444348574,
      "grad_norm": 6.692643165588379,
      "learning_rate": 2.1173425366695428e-05,
      "loss": 1.1605,
      "step": 3410
    },
    {
      "epoch": 2.9508196721311473,
      "grad_norm": 7.154376983642578,
      "learning_rate": 2.1147540983606557e-05,
      "loss": 1.2034,
      "step": 3420
    },
    {
      "epoch": 2.9594477998274376,
      "grad_norm": 11.833710670471191,
      "learning_rate": 2.112165660051769e-05,
      "loss": 0.971,
      "step": 3430
    },
    {
      "epoch": 2.9680759275237274,
      "grad_norm": 10.075153350830078,
      "learning_rate": 2.1095772217428816e-05,
      "loss": 1.4094,
      "step": 3440
    },
    {
      "epoch": 2.9767040552200172,
      "grad_norm": 7.488197326660156,
      "learning_rate": 2.106988783433995e-05,
      "loss": 1.1001,
      "step": 3450
    },
    {
      "epoch": 2.985332182916307,
      "grad_norm": 7.988064289093018,
      "learning_rate": 2.104400345125108e-05,
      "loss": 1.3086,
      "step": 3460
    },
    {
      "epoch": 2.993960310612597,
      "grad_norm": 11.36455249786377,
      "learning_rate": 2.101811906816221e-05,
      "loss": 1.3297,
      "step": 3470
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.6429275870323181,
      "eval_runtime": 2.4125,
      "eval_samples_per_second": 205.592,
      "eval_steps_per_second": 25.699,
      "step": 3477
    },
    {
      "epoch": 3.0025884383088868,
      "grad_norm": 4.4626078605651855,
      "learning_rate": 2.0992234685073338e-05,
      "loss": 1.0818,
      "step": 3480
    },
    {
      "epoch": 3.011216566005177,
      "grad_norm": 7.220885753631592,
      "learning_rate": 2.096635030198447e-05,
      "loss": 1.0415,
      "step": 3490
    },
    {
      "epoch": 3.019844693701467,
      "grad_norm": 7.854397296905518,
      "learning_rate": 2.09404659188956e-05,
      "loss": 1.0307,
      "step": 3500
    },
    {
      "epoch": 3.0284728213977568,
      "grad_norm": 12.538569450378418,
      "learning_rate": 2.0914581535806733e-05,
      "loss": 1.1667,
      "step": 3510
    },
    {
      "epoch": 3.0371009490940466,
      "grad_norm": 6.6463823318481445,
      "learning_rate": 2.088869715271786e-05,
      "loss": 1.1358,
      "step": 3520
    },
    {
      "epoch": 3.0457290767903364,
      "grad_norm": 8.020078659057617,
      "learning_rate": 2.0862812769628992e-05,
      "loss": 1.0207,
      "step": 3530
    },
    {
      "epoch": 3.0543572044866263,
      "grad_norm": 9.722121238708496,
      "learning_rate": 2.083692838654012e-05,
      "loss": 1.2559,
      "step": 3540
    },
    {
      "epoch": 3.062985332182916,
      "grad_norm": 6.862792015075684,
      "learning_rate": 2.0811044003451254e-05,
      "loss": 1.1173,
      "step": 3550
    },
    {
      "epoch": 3.0716134598792064,
      "grad_norm": 5.130026817321777,
      "learning_rate": 2.078515962036238e-05,
      "loss": 1.0128,
      "step": 3560
    },
    {
      "epoch": 3.0802415875754963,
      "grad_norm": 7.016564846038818,
      "learning_rate": 2.0759275237273513e-05,
      "loss": 1.0474,
      "step": 3570
    },
    {
      "epoch": 3.088869715271786,
      "grad_norm": 15.21106243133545,
      "learning_rate": 2.0733390854184643e-05,
      "loss": 1.1101,
      "step": 3580
    },
    {
      "epoch": 3.097497842968076,
      "grad_norm": 7.572747230529785,
      "learning_rate": 2.0707506471095776e-05,
      "loss": 1.0153,
      "step": 3590
    },
    {
      "epoch": 3.106125970664366,
      "grad_norm": 7.7020955085754395,
      "learning_rate": 2.06816220880069e-05,
      "loss": 1.089,
      "step": 3600
    },
    {
      "epoch": 3.1147540983606556,
      "grad_norm": 89.59535217285156,
      "learning_rate": 2.0655737704918034e-05,
      "loss": 1.1586,
      "step": 3610
    },
    {
      "epoch": 3.1233822260569455,
      "grad_norm": 6.980558395385742,
      "learning_rate": 2.0629853321829164e-05,
      "loss": 1.0241,
      "step": 3620
    },
    {
      "epoch": 3.1320103537532358,
      "grad_norm": 5.747226238250732,
      "learning_rate": 2.0603968938740297e-05,
      "loss": 1.347,
      "step": 3630
    },
    {
      "epoch": 3.1406384814495256,
      "grad_norm": 4.317458629608154,
      "learning_rate": 2.0578084555651423e-05,
      "loss": 1.177,
      "step": 3640
    },
    {
      "epoch": 3.1492666091458155,
      "grad_norm": 5.6195878982543945,
      "learning_rate": 2.0552200172562556e-05,
      "loss": 1.1446,
      "step": 3650
    },
    {
      "epoch": 3.1578947368421053,
      "grad_norm": 3.5103023052215576,
      "learning_rate": 2.0526315789473685e-05,
      "loss": 1.028,
      "step": 3660
    },
    {
      "epoch": 3.166522864538395,
      "grad_norm": 9.133496284484863,
      "learning_rate": 2.0500431406384815e-05,
      "loss": 1.1257,
      "step": 3670
    },
    {
      "epoch": 3.175150992234685,
      "grad_norm": 7.187618732452393,
      "learning_rate": 2.0474547023295944e-05,
      "loss": 1.1082,
      "step": 3680
    },
    {
      "epoch": 3.183779119930975,
      "grad_norm": 6.439459800720215,
      "learning_rate": 2.0448662640207074e-05,
      "loss": 0.9939,
      "step": 3690
    },
    {
      "epoch": 3.1924072476272647,
      "grad_norm": 6.67747688293457,
      "learning_rate": 2.0422778257118207e-05,
      "loss": 1.0635,
      "step": 3700
    },
    {
      "epoch": 3.201035375323555,
      "grad_norm": 8.168949127197266,
      "learning_rate": 2.0396893874029336e-05,
      "loss": 1.0134,
      "step": 3710
    },
    {
      "epoch": 3.209663503019845,
      "grad_norm": 6.7854132652282715,
      "learning_rate": 2.0371009490940466e-05,
      "loss": 1.2055,
      "step": 3720
    },
    {
      "epoch": 3.2182916307161347,
      "grad_norm": 3.8445143699645996,
      "learning_rate": 2.0345125107851595e-05,
      "loss": 1.0161,
      "step": 3730
    },
    {
      "epoch": 3.2269197584124245,
      "grad_norm": 6.670506477355957,
      "learning_rate": 2.0319240724762728e-05,
      "loss": 1.2201,
      "step": 3740
    },
    {
      "epoch": 3.2355478861087144,
      "grad_norm": 4.730180740356445,
      "learning_rate": 2.0293356341673857e-05,
      "loss": 0.8853,
      "step": 3750
    },
    {
      "epoch": 3.244176013805004,
      "grad_norm": 5.379533290863037,
      "learning_rate": 2.0267471958584987e-05,
      "loss": 1.0967,
      "step": 3760
    },
    {
      "epoch": 3.252804141501294,
      "grad_norm": 12.452579498291016,
      "learning_rate": 2.0241587575496116e-05,
      "loss": 1.1237,
      "step": 3770
    },
    {
      "epoch": 3.2614322691975843,
      "grad_norm": 3.0739967823028564,
      "learning_rate": 2.021570319240725e-05,
      "loss": 0.9728,
      "step": 3780
    },
    {
      "epoch": 3.270060396893874,
      "grad_norm": 12.370359420776367,
      "learning_rate": 2.018981880931838e-05,
      "loss": 1.0863,
      "step": 3790
    },
    {
      "epoch": 3.278688524590164,
      "grad_norm": 11.490286827087402,
      "learning_rate": 2.0163934426229508e-05,
      "loss": 1.1612,
      "step": 3800
    },
    {
      "epoch": 3.287316652286454,
      "grad_norm": 5.184188365936279,
      "learning_rate": 2.0138050043140638e-05,
      "loss": 1.1347,
      "step": 3810
    },
    {
      "epoch": 3.2959447799827437,
      "grad_norm": 13.364709854125977,
      "learning_rate": 2.011216566005177e-05,
      "loss": 1.0902,
      "step": 3820
    },
    {
      "epoch": 3.3045729076790336,
      "grad_norm": 8.078739166259766,
      "learning_rate": 2.00862812769629e-05,
      "loss": 0.8079,
      "step": 3830
    },
    {
      "epoch": 3.3132010353753234,
      "grad_norm": 5.888318061828613,
      "learning_rate": 2.006039689387403e-05,
      "loss": 0.9318,
      "step": 3840
    },
    {
      "epoch": 3.3218291630716132,
      "grad_norm": 8.007412910461426,
      "learning_rate": 2.003451251078516e-05,
      "loss": 1.0328,
      "step": 3850
    },
    {
      "epoch": 3.3304572907679035,
      "grad_norm": 6.004914283752441,
      "learning_rate": 2.0008628127696292e-05,
      "loss": 0.9098,
      "step": 3860
    },
    {
      "epoch": 3.3390854184641934,
      "grad_norm": 5.92441987991333,
      "learning_rate": 1.9982743744607418e-05,
      "loss": 0.9679,
      "step": 3870
    },
    {
      "epoch": 3.3477135461604832,
      "grad_norm": 18.74649429321289,
      "learning_rate": 1.995685936151855e-05,
      "loss": 1.08,
      "step": 3880
    },
    {
      "epoch": 3.356341673856773,
      "grad_norm": 184.65365600585938,
      "learning_rate": 1.993097497842968e-05,
      "loss": 1.0232,
      "step": 3890
    },
    {
      "epoch": 3.364969801553063,
      "grad_norm": 9.870807647705078,
      "learning_rate": 1.9905090595340813e-05,
      "loss": 0.9529,
      "step": 3900
    },
    {
      "epoch": 3.3735979292493528,
      "grad_norm": 35.69107437133789,
      "learning_rate": 1.987920621225194e-05,
      "loss": 1.1665,
      "step": 3910
    },
    {
      "epoch": 3.382226056945643,
      "grad_norm": 7.219395160675049,
      "learning_rate": 1.9853321829163072e-05,
      "loss": 1.0918,
      "step": 3920
    },
    {
      "epoch": 3.390854184641933,
      "grad_norm": 5.178959846496582,
      "learning_rate": 1.98274374460742e-05,
      "loss": 1.0094,
      "step": 3930
    },
    {
      "epoch": 3.3994823123382227,
      "grad_norm": 6.611709117889404,
      "learning_rate": 1.9801553062985334e-05,
      "loss": 1.1262,
      "step": 3940
    },
    {
      "epoch": 3.4081104400345126,
      "grad_norm": 4.443993091583252,
      "learning_rate": 1.977566867989646e-05,
      "loss": 0.9284,
      "step": 3950
    },
    {
      "epoch": 3.4167385677308024,
      "grad_norm": 15.750162124633789,
      "learning_rate": 1.9749784296807593e-05,
      "loss": 1.1307,
      "step": 3960
    },
    {
      "epoch": 3.4253666954270923,
      "grad_norm": 5.106734752655029,
      "learning_rate": 1.9723899913718723e-05,
      "loss": 0.8431,
      "step": 3970
    },
    {
      "epoch": 3.433994823123382,
      "grad_norm": 14.316387176513672,
      "learning_rate": 1.9698015530629856e-05,
      "loss": 1.0698,
      "step": 3980
    },
    {
      "epoch": 3.442622950819672,
      "grad_norm": 13.038710594177246,
      "learning_rate": 1.9672131147540982e-05,
      "loss": 1.0936,
      "step": 3990
    },
    {
      "epoch": 3.451251078515962,
      "grad_norm": 8.65680980682373,
      "learning_rate": 1.9646246764452115e-05,
      "loss": 1.3537,
      "step": 4000
    },
    {
      "epoch": 3.459879206212252,
      "grad_norm": 6.979463577270508,
      "learning_rate": 1.9620362381363244e-05,
      "loss": 1.1904,
      "step": 4010
    },
    {
      "epoch": 3.468507333908542,
      "grad_norm": 21.372108459472656,
      "learning_rate": 1.9594477998274377e-05,
      "loss": 1.0843,
      "step": 4020
    },
    {
      "epoch": 3.477135461604832,
      "grad_norm": 7.650877475738525,
      "learning_rate": 1.9568593615185503e-05,
      "loss": 1.0619,
      "step": 4030
    },
    {
      "epoch": 3.4857635893011216,
      "grad_norm": 9.682539939880371,
      "learning_rate": 1.9542709232096636e-05,
      "loss": 1.1464,
      "step": 4040
    },
    {
      "epoch": 3.4943917169974115,
      "grad_norm": 26.72654914855957,
      "learning_rate": 1.9516824849007765e-05,
      "loss": 1.1428,
      "step": 4050
    },
    {
      "epoch": 3.5030198446937013,
      "grad_norm": 10.035561561584473,
      "learning_rate": 1.94909404659189e-05,
      "loss": 0.9979,
      "step": 4060
    },
    {
      "epoch": 3.5116479723899916,
      "grad_norm": 7.7983479499816895,
      "learning_rate": 1.9465056082830024e-05,
      "loss": 1.1039,
      "step": 4070
    },
    {
      "epoch": 3.5202761000862814,
      "grad_norm": 5.461968421936035,
      "learning_rate": 1.9439171699741157e-05,
      "loss": 1.0196,
      "step": 4080
    },
    {
      "epoch": 3.5289042277825713,
      "grad_norm": 9.36749267578125,
      "learning_rate": 1.9413287316652287e-05,
      "loss": 1.0397,
      "step": 4090
    },
    {
      "epoch": 3.537532355478861,
      "grad_norm": 7.295554161071777,
      "learning_rate": 1.938740293356342e-05,
      "loss": 0.9529,
      "step": 4100
    },
    {
      "epoch": 3.546160483175151,
      "grad_norm": 6.037779808044434,
      "learning_rate": 1.9361518550474546e-05,
      "loss": 0.9156,
      "step": 4110
    },
    {
      "epoch": 3.554788610871441,
      "grad_norm": 10.217443466186523,
      "learning_rate": 1.933563416738568e-05,
      "loss": 1.0039,
      "step": 4120
    },
    {
      "epoch": 3.5634167385677307,
      "grad_norm": 6.064477443695068,
      "learning_rate": 1.9309749784296808e-05,
      "loss": 0.9253,
      "step": 4130
    },
    {
      "epoch": 3.5720448662640205,
      "grad_norm": 6.372175693511963,
      "learning_rate": 1.928386540120794e-05,
      "loss": 1.0773,
      "step": 4140
    },
    {
      "epoch": 3.5806729939603104,
      "grad_norm": 3.771261692047119,
      "learning_rate": 1.9257981018119067e-05,
      "loss": 1.1068,
      "step": 4150
    },
    {
      "epoch": 3.5893011216566006,
      "grad_norm": 5.5793538093566895,
      "learning_rate": 1.92320966350302e-05,
      "loss": 0.9761,
      "step": 4160
    },
    {
      "epoch": 3.5979292493528905,
      "grad_norm": 41.99201965332031,
      "learning_rate": 1.920621225194133e-05,
      "loss": 0.9087,
      "step": 4170
    },
    {
      "epoch": 3.6065573770491803,
      "grad_norm": 6.977038383483887,
      "learning_rate": 1.9180327868852462e-05,
      "loss": 1.0408,
      "step": 4180
    },
    {
      "epoch": 3.61518550474547,
      "grad_norm": 7.03787899017334,
      "learning_rate": 1.915444348576359e-05,
      "loss": 1.0156,
      "step": 4190
    },
    {
      "epoch": 3.62381363244176,
      "grad_norm": 5.589576244354248,
      "learning_rate": 1.912855910267472e-05,
      "loss": 1.0885,
      "step": 4200
    },
    {
      "epoch": 3.6324417601380503,
      "grad_norm": 4.364057540893555,
      "learning_rate": 1.910267471958585e-05,
      "loss": 1.0912,
      "step": 4210
    },
    {
      "epoch": 3.64106988783434,
      "grad_norm": 7.7219953536987305,
      "learning_rate": 1.9076790336496984e-05,
      "loss": 0.8106,
      "step": 4220
    },
    {
      "epoch": 3.64969801553063,
      "grad_norm": 8.735573768615723,
      "learning_rate": 1.905090595340811e-05,
      "loss": 0.9404,
      "step": 4230
    },
    {
      "epoch": 3.65832614322692,
      "grad_norm": 11.007935523986816,
      "learning_rate": 1.9025021570319243e-05,
      "loss": 1.1304,
      "step": 4240
    },
    {
      "epoch": 3.6669542709232097,
      "grad_norm": 8.517608642578125,
      "learning_rate": 1.8999137187230372e-05,
      "loss": 0.9058,
      "step": 4250
    },
    {
      "epoch": 3.6755823986194995,
      "grad_norm": 4.48835563659668,
      "learning_rate": 1.8973252804141505e-05,
      "loss": 1.2735,
      "step": 4260
    },
    {
      "epoch": 3.6842105263157894,
      "grad_norm": 10.240647315979004,
      "learning_rate": 1.894736842105263e-05,
      "loss": 1.2085,
      "step": 4270
    },
    {
      "epoch": 3.6928386540120792,
      "grad_norm": 12.54617977142334,
      "learning_rate": 1.892148403796376e-05,
      "loss": 0.9375,
      "step": 4280
    },
    {
      "epoch": 3.701466781708369,
      "grad_norm": 5.758557319641113,
      "learning_rate": 1.8895599654874893e-05,
      "loss": 1.0088,
      "step": 4290
    },
    {
      "epoch": 3.710094909404659,
      "grad_norm": 5.122608661651611,
      "learning_rate": 1.8869715271786023e-05,
      "loss": 0.7994,
      "step": 4300
    },
    {
      "epoch": 3.718723037100949,
      "grad_norm": 21.93495750427246,
      "learning_rate": 1.8843830888697152e-05,
      "loss": 1.1056,
      "step": 4310
    },
    {
      "epoch": 3.727351164797239,
      "grad_norm": 105.48977661132812,
      "learning_rate": 1.8817946505608282e-05,
      "loss": 1.0909,
      "step": 4320
    },
    {
      "epoch": 3.735979292493529,
      "grad_norm": 5.715173721313477,
      "learning_rate": 1.8792062122519415e-05,
      "loss": 1.1948,
      "step": 4330
    },
    {
      "epoch": 3.7446074201898187,
      "grad_norm": 3.755371332168579,
      "learning_rate": 1.8766177739430544e-05,
      "loss": 1.1181,
      "step": 4340
    },
    {
      "epoch": 3.7532355478861086,
      "grad_norm": 10.415861129760742,
      "learning_rate": 1.8740293356341674e-05,
      "loss": 0.9548,
      "step": 4350
    },
    {
      "epoch": 3.761863675582399,
      "grad_norm": 5.237427711486816,
      "learning_rate": 1.8714408973252803e-05,
      "loss": 1.049,
      "step": 4360
    },
    {
      "epoch": 3.7704918032786887,
      "grad_norm": 10.889799118041992,
      "learning_rate": 1.8688524590163936e-05,
      "loss": 0.9307,
      "step": 4370
    },
    {
      "epoch": 3.7791199309749786,
      "grad_norm": 34.586647033691406,
      "learning_rate": 1.8662640207075065e-05,
      "loss": 1.1485,
      "step": 4380
    },
    {
      "epoch": 3.7877480586712684,
      "grad_norm": 3.5604605674743652,
      "learning_rate": 1.8636755823986195e-05,
      "loss": 0.9805,
      "step": 4390
    },
    {
      "epoch": 3.7963761863675582,
      "grad_norm": 8.132851600646973,
      "learning_rate": 1.8610871440897324e-05,
      "loss": 0.8852,
      "step": 4400
    },
    {
      "epoch": 3.805004314063848,
      "grad_norm": 14.169961929321289,
      "learning_rate": 1.8584987057808457e-05,
      "loss": 1.2142,
      "step": 4410
    },
    {
      "epoch": 3.813632441760138,
      "grad_norm": 4.807948112487793,
      "learning_rate": 1.8559102674719583e-05,
      "loss": 0.9434,
      "step": 4420
    },
    {
      "epoch": 3.822260569456428,
      "grad_norm": 584.7919921875,
      "learning_rate": 1.8533218291630716e-05,
      "loss": 0.8874,
      "step": 4430
    },
    {
      "epoch": 3.8308886971527176,
      "grad_norm": 5.567625999450684,
      "learning_rate": 1.8507333908541846e-05,
      "loss": 1.1053,
      "step": 4440
    },
    {
      "epoch": 3.839516824849008,
      "grad_norm": 50.26548385620117,
      "learning_rate": 1.848144952545298e-05,
      "loss": 0.8536,
      "step": 4450
    },
    {
      "epoch": 3.8481449525452978,
      "grad_norm": 5.884154796600342,
      "learning_rate": 1.8455565142364105e-05,
      "loss": 0.9369,
      "step": 4460
    },
    {
      "epoch": 3.8567730802415876,
      "grad_norm": 43.490543365478516,
      "learning_rate": 1.8429680759275238e-05,
      "loss": 0.7433,
      "step": 4470
    },
    {
      "epoch": 3.8654012079378774,
      "grad_norm": 8.73164176940918,
      "learning_rate": 1.8403796376186367e-05,
      "loss": 1.1746,
      "step": 4480
    },
    {
      "epoch": 3.8740293356341673,
      "grad_norm": 5.914668560028076,
      "learning_rate": 1.83779119930975e-05,
      "loss": 0.9275,
      "step": 4490
    },
    {
      "epoch": 3.882657463330457,
      "grad_norm": 6.266617774963379,
      "learning_rate": 1.8352027610008626e-05,
      "loss": 1.0591,
      "step": 4500
    },
    {
      "epoch": 3.8912855910267474,
      "grad_norm": 32.86287307739258,
      "learning_rate": 1.832614322691976e-05,
      "loss": 1.1157,
      "step": 4510
    },
    {
      "epoch": 3.8999137187230373,
      "grad_norm": 5.1150689125061035,
      "learning_rate": 1.830025884383089e-05,
      "loss": 1.085,
      "step": 4520
    },
    {
      "epoch": 3.908541846419327,
      "grad_norm": 6.120386600494385,
      "learning_rate": 1.827437446074202e-05,
      "loss": 0.8799,
      "step": 4530
    },
    {
      "epoch": 3.917169974115617,
      "grad_norm": 5.8577165603637695,
      "learning_rate": 1.8248490077653147e-05,
      "loss": 0.8854,
      "step": 4540
    },
    {
      "epoch": 3.925798101811907,
      "grad_norm": 5.203388690948486,
      "learning_rate": 1.822260569456428e-05,
      "loss": 0.9131,
      "step": 4550
    },
    {
      "epoch": 3.9344262295081966,
      "grad_norm": 8.572028160095215,
      "learning_rate": 1.819672131147541e-05,
      "loss": 0.9486,
      "step": 4560
    },
    {
      "epoch": 3.9430543572044865,
      "grad_norm": 4.4774885177612305,
      "learning_rate": 1.8170836928386542e-05,
      "loss": 1.118,
      "step": 4570
    },
    {
      "epoch": 3.9516824849007763,
      "grad_norm": 4.348685264587402,
      "learning_rate": 1.814495254529767e-05,
      "loss": 1.0059,
      "step": 4580
    },
    {
      "epoch": 3.960310612597066,
      "grad_norm": 5.789333820343018,
      "learning_rate": 1.81190681622088e-05,
      "loss": 1.0767,
      "step": 4590
    },
    {
      "epoch": 3.9689387402933565,
      "grad_norm": 6.852180004119873,
      "learning_rate": 1.809318377911993e-05,
      "loss": 0.7346,
      "step": 4600
    },
    {
      "epoch": 3.9775668679896463,
      "grad_norm": 3.7794790267944336,
      "learning_rate": 1.8067299396031064e-05,
      "loss": 1.0619,
      "step": 4610
    },
    {
      "epoch": 3.986194995685936,
      "grad_norm": 4.941560745239258,
      "learning_rate": 1.804141501294219e-05,
      "loss": 0.8792,
      "step": 4620
    },
    {
      "epoch": 3.994823123382226,
      "grad_norm": 39.698997497558594,
      "learning_rate": 1.8015530629853323e-05,
      "loss": 1.1919,
      "step": 4630
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.5646640658378601,
      "eval_runtime": 2.6234,
      "eval_samples_per_second": 189.064,
      "eval_steps_per_second": 23.633,
      "step": 4636
    },
    {
      "epoch": 4.003451251078516,
      "grad_norm": 1.9155956506729126,
      "learning_rate": 1.7989646246764452e-05,
      "loss": 0.8366,
      "step": 4640
    },
    {
      "epoch": 4.012079378774806,
      "grad_norm": 4.5864458084106445,
      "learning_rate": 1.7963761863675585e-05,
      "loss": 0.8957,
      "step": 4650
    },
    {
      "epoch": 4.020707506471096,
      "grad_norm": 7.5829548835754395,
      "learning_rate": 1.793787748058671e-05,
      "loss": 0.8241,
      "step": 4660
    },
    {
      "epoch": 4.029335634167386,
      "grad_norm": 22.001367568969727,
      "learning_rate": 1.7911993097497844e-05,
      "loss": 1.0099,
      "step": 4670
    },
    {
      "epoch": 4.037963761863676,
      "grad_norm": 17.85956382751465,
      "learning_rate": 1.7886108714408974e-05,
      "loss": 0.8759,
      "step": 4680
    },
    {
      "epoch": 4.0465918895599655,
      "grad_norm": 8.739937782287598,
      "learning_rate": 1.7860224331320106e-05,
      "loss": 0.8988,
      "step": 4690
    },
    {
      "epoch": 4.055220017256255,
      "grad_norm": 5.786636829376221,
      "learning_rate": 1.7834339948231233e-05,
      "loss": 1.0064,
      "step": 4700
    },
    {
      "epoch": 4.063848144952545,
      "grad_norm": 5.320204257965088,
      "learning_rate": 1.7808455565142365e-05,
      "loss": 0.797,
      "step": 4710
    },
    {
      "epoch": 4.072476272648835,
      "grad_norm": 15.46491813659668,
      "learning_rate": 1.7782571182053495e-05,
      "loss": 0.9945,
      "step": 4720
    },
    {
      "epoch": 4.081104400345125,
      "grad_norm": 3.8751204013824463,
      "learning_rate": 1.7756686798964628e-05,
      "loss": 0.9683,
      "step": 4730
    },
    {
      "epoch": 4.089732528041415,
      "grad_norm": 5.068087577819824,
      "learning_rate": 1.7730802415875754e-05,
      "loss": 0.8188,
      "step": 4740
    },
    {
      "epoch": 4.098360655737705,
      "grad_norm": 3.8152318000793457,
      "learning_rate": 1.7704918032786887e-05,
      "loss": 1.0837,
      "step": 4750
    },
    {
      "epoch": 4.106988783433994,
      "grad_norm": 9.96385383605957,
      "learning_rate": 1.7679033649698016e-05,
      "loss": 1.1673,
      "step": 4760
    },
    {
      "epoch": 4.115616911130285,
      "grad_norm": 6.0135698318481445,
      "learning_rate": 1.765314926660915e-05,
      "loss": 0.8337,
      "step": 4770
    },
    {
      "epoch": 4.124245038826575,
      "grad_norm": 5.561182975769043,
      "learning_rate": 1.7627264883520275e-05,
      "loss": 0.8776,
      "step": 4780
    },
    {
      "epoch": 4.132873166522865,
      "grad_norm": 230.8825225830078,
      "learning_rate": 1.7601380500431408e-05,
      "loss": 1.0597,
      "step": 4790
    },
    {
      "epoch": 4.141501294219155,
      "grad_norm": 138.51414489746094,
      "learning_rate": 1.7575496117342537e-05,
      "loss": 0.9114,
      "step": 4800
    },
    {
      "epoch": 4.1501294219154445,
      "grad_norm": 4.407142162322998,
      "learning_rate": 1.754961173425367e-05,
      "loss": 0.8423,
      "step": 4810
    },
    {
      "epoch": 4.158757549611734,
      "grad_norm": 11.290745735168457,
      "learning_rate": 1.7523727351164796e-05,
      "loss": 0.9535,
      "step": 4820
    },
    {
      "epoch": 4.167385677308024,
      "grad_norm": 6.342134475708008,
      "learning_rate": 1.749784296807593e-05,
      "loss": 0.8025,
      "step": 4830
    },
    {
      "epoch": 4.176013805004314,
      "grad_norm": 7.913791179656982,
      "learning_rate": 1.747195858498706e-05,
      "loss": 1.1458,
      "step": 4840
    },
    {
      "epoch": 4.184641932700604,
      "grad_norm": 6.971024990081787,
      "learning_rate": 1.744607420189819e-05,
      "loss": 1.1533,
      "step": 4850
    },
    {
      "epoch": 4.193270060396894,
      "grad_norm": 3.0803463459014893,
      "learning_rate": 1.7420189818809318e-05,
      "loss": 1.0267,
      "step": 4860
    },
    {
      "epoch": 4.201898188093184,
      "grad_norm": 4.9562602043151855,
      "learning_rate": 1.739430543572045e-05,
      "loss": 0.8882,
      "step": 4870
    },
    {
      "epoch": 4.2105263157894735,
      "grad_norm": 3.8135011196136475,
      "learning_rate": 1.736842105263158e-05,
      "loss": 0.9437,
      "step": 4880
    },
    {
      "epoch": 4.219154443485763,
      "grad_norm": 16.519033432006836,
      "learning_rate": 1.734253666954271e-05,
      "loss": 1.0041,
      "step": 4890
    },
    {
      "epoch": 4.227782571182053,
      "grad_norm": 5.732445240020752,
      "learning_rate": 1.731665228645384e-05,
      "loss": 0.8924,
      "step": 4900
    },
    {
      "epoch": 4.236410698878343,
      "grad_norm": 10.432077407836914,
      "learning_rate": 1.729076790336497e-05,
      "loss": 1.0576,
      "step": 4910
    },
    {
      "epoch": 4.245038826574634,
      "grad_norm": 6.627712249755859,
      "learning_rate": 1.72648835202761e-05,
      "loss": 1.0397,
      "step": 4920
    },
    {
      "epoch": 4.253666954270924,
      "grad_norm": 5.280622482299805,
      "learning_rate": 1.723899913718723e-05,
      "loss": 0.7597,
      "step": 4930
    },
    {
      "epoch": 4.262295081967213,
      "grad_norm": 12.379067420959473,
      "learning_rate": 1.721311475409836e-05,
      "loss": 0.9251,
      "step": 4940
    },
    {
      "epoch": 4.270923209663503,
      "grad_norm": 5.506321430206299,
      "learning_rate": 1.718723037100949e-05,
      "loss": 0.7639,
      "step": 4950
    },
    {
      "epoch": 4.279551337359793,
      "grad_norm": 5.126827239990234,
      "learning_rate": 1.7161345987920623e-05,
      "loss": 1.0072,
      "step": 4960
    },
    {
      "epoch": 4.288179465056083,
      "grad_norm": 23.426122665405273,
      "learning_rate": 1.713546160483175e-05,
      "loss": 0.9132,
      "step": 4970
    },
    {
      "epoch": 4.296807592752373,
      "grad_norm": 28.170564651489258,
      "learning_rate": 1.710957722174288e-05,
      "loss": 1.054,
      "step": 4980
    },
    {
      "epoch": 4.305435720448663,
      "grad_norm": 6.271197319030762,
      "learning_rate": 1.708369283865401e-05,
      "loss": 1.0253,
      "step": 4990
    },
    {
      "epoch": 4.3140638481449525,
      "grad_norm": 5.576972484588623,
      "learning_rate": 1.7057808455565144e-05,
      "loss": 1.0268,
      "step": 5000
    },
    {
      "epoch": 4.322691975841242,
      "grad_norm": 4.832690715789795,
      "learning_rate": 1.703192407247627e-05,
      "loss": 0.8802,
      "step": 5010
    },
    {
      "epoch": 4.331320103537532,
      "grad_norm": 1338.8826904296875,
      "learning_rate": 1.7006039689387403e-05,
      "loss": 1.016,
      "step": 5020
    },
    {
      "epoch": 4.339948231233822,
      "grad_norm": 4.290777683258057,
      "learning_rate": 1.6980155306298532e-05,
      "loss": 0.9291,
      "step": 5030
    },
    {
      "epoch": 4.348576358930112,
      "grad_norm": 15.808431625366211,
      "learning_rate": 1.6954270923209665e-05,
      "loss": 0.9051,
      "step": 5040
    },
    {
      "epoch": 4.357204486626402,
      "grad_norm": 4.6663665771484375,
      "learning_rate": 1.692838654012079e-05,
      "loss": 0.9104,
      "step": 5050
    },
    {
      "epoch": 4.3658326143226915,
      "grad_norm": 7.34998893737793,
      "learning_rate": 1.6902502157031924e-05,
      "loss": 1.0331,
      "step": 5060
    },
    {
      "epoch": 4.374460742018982,
      "grad_norm": 7.1688055992126465,
      "learning_rate": 1.6876617773943054e-05,
      "loss": 0.9625,
      "step": 5070
    },
    {
      "epoch": 4.383088869715272,
      "grad_norm": 6.470672607421875,
      "learning_rate": 1.6850733390854187e-05,
      "loss": 1.146,
      "step": 5080
    },
    {
      "epoch": 4.391716997411562,
      "grad_norm": 7.000214576721191,
      "learning_rate": 1.6824849007765313e-05,
      "loss": 0.9079,
      "step": 5090
    },
    {
      "epoch": 4.400345125107852,
      "grad_norm": 9.645998001098633,
      "learning_rate": 1.6798964624676446e-05,
      "loss": 1.097,
      "step": 5100
    },
    {
      "epoch": 4.408973252804142,
      "grad_norm": 7.082625389099121,
      "learning_rate": 1.6773080241587575e-05,
      "loss": 0.9877,
      "step": 5110
    },
    {
      "epoch": 4.4176013805004315,
      "grad_norm": 8.403891563415527,
      "learning_rate": 1.6747195858498708e-05,
      "loss": 1.0057,
      "step": 5120
    },
    {
      "epoch": 4.426229508196721,
      "grad_norm": 5.351071834564209,
      "learning_rate": 1.6721311475409834e-05,
      "loss": 0.9493,
      "step": 5130
    },
    {
      "epoch": 4.434857635893011,
      "grad_norm": 4.992123126983643,
      "learning_rate": 1.6695427092320967e-05,
      "loss": 0.8949,
      "step": 5140
    },
    {
      "epoch": 4.443485763589301,
      "grad_norm": 4.803213119506836,
      "learning_rate": 1.6669542709232096e-05,
      "loss": 0.79,
      "step": 5150
    },
    {
      "epoch": 4.452113891285591,
      "grad_norm": 5.368724822998047,
      "learning_rate": 1.664365832614323e-05,
      "loss": 0.914,
      "step": 5160
    },
    {
      "epoch": 4.460742018981881,
      "grad_norm": 13.422857284545898,
      "learning_rate": 1.6617773943054355e-05,
      "loss": 0.7461,
      "step": 5170
    },
    {
      "epoch": 4.469370146678171,
      "grad_norm": 4.143519401550293,
      "learning_rate": 1.6591889559965488e-05,
      "loss": 0.818,
      "step": 5180
    },
    {
      "epoch": 4.47799827437446,
      "grad_norm": 11.99805736541748,
      "learning_rate": 1.6566005176876618e-05,
      "loss": 1.0754,
      "step": 5190
    },
    {
      "epoch": 4.48662640207075,
      "grad_norm": 15.977333068847656,
      "learning_rate": 1.654012079378775e-05,
      "loss": 0.9856,
      "step": 5200
    },
    {
      "epoch": 4.49525452976704,
      "grad_norm": 4.7317214012146,
      "learning_rate": 1.6514236410698877e-05,
      "loss": 0.7476,
      "step": 5210
    },
    {
      "epoch": 4.503882657463331,
      "grad_norm": 8.080069541931152,
      "learning_rate": 1.648835202761001e-05,
      "loss": 0.8563,
      "step": 5220
    },
    {
      "epoch": 4.512510785159621,
      "grad_norm": 5.12052059173584,
      "learning_rate": 1.646246764452114e-05,
      "loss": 1.0221,
      "step": 5230
    },
    {
      "epoch": 4.5211389128559105,
      "grad_norm": 5.958685874938965,
      "learning_rate": 1.6436583261432272e-05,
      "loss": 1.022,
      "step": 5240
    },
    {
      "epoch": 4.5297670405522,
      "grad_norm": 6.192869186401367,
      "learning_rate": 1.6410698878343398e-05,
      "loss": 0.9679,
      "step": 5250
    },
    {
      "epoch": 4.53839516824849,
      "grad_norm": 6.544035911560059,
      "learning_rate": 1.638481449525453e-05,
      "loss": 1.0169,
      "step": 5260
    },
    {
      "epoch": 4.54702329594478,
      "grad_norm": 71.6249771118164,
      "learning_rate": 1.635893011216566e-05,
      "loss": 1.0117,
      "step": 5270
    },
    {
      "epoch": 4.55565142364107,
      "grad_norm": 6.526785373687744,
      "learning_rate": 1.6333045729076793e-05,
      "loss": 1.0414,
      "step": 5280
    },
    {
      "epoch": 4.56427955133736,
      "grad_norm": 19.089967727661133,
      "learning_rate": 1.630716134598792e-05,
      "loss": 0.9647,
      "step": 5290
    },
    {
      "epoch": 4.57290767903365,
      "grad_norm": 5.936202049255371,
      "learning_rate": 1.6281276962899052e-05,
      "loss": 0.883,
      "step": 5300
    },
    {
      "epoch": 4.581535806729939,
      "grad_norm": 3.2533681392669678,
      "learning_rate": 1.625539257981018e-05,
      "loss": 0.8768,
      "step": 5310
    },
    {
      "epoch": 4.590163934426229,
      "grad_norm": 3.826446533203125,
      "learning_rate": 1.6229508196721314e-05,
      "loss": 0.978,
      "step": 5320
    },
    {
      "epoch": 4.598792062122519,
      "grad_norm": 5.428952693939209,
      "learning_rate": 1.620362381363244e-05,
      "loss": 0.8857,
      "step": 5330
    },
    {
      "epoch": 4.607420189818809,
      "grad_norm": 6.32995080947876,
      "learning_rate": 1.6177739430543573e-05,
      "loss": 0.9213,
      "step": 5340
    },
    {
      "epoch": 4.6160483175151,
      "grad_norm": 8.679458618164062,
      "learning_rate": 1.6151855047454703e-05,
      "loss": 0.8374,
      "step": 5350
    },
    {
      "epoch": 4.624676445211389,
      "grad_norm": 5.671743869781494,
      "learning_rate": 1.6125970664365836e-05,
      "loss": 0.8541,
      "step": 5360
    },
    {
      "epoch": 4.633304572907679,
      "grad_norm": 4.419561862945557,
      "learning_rate": 1.6100086281276962e-05,
      "loss": 0.8783,
      "step": 5370
    },
    {
      "epoch": 4.641932700603969,
      "grad_norm": 8.034558296203613,
      "learning_rate": 1.6074201898188095e-05,
      "loss": 0.8236,
      "step": 5380
    },
    {
      "epoch": 4.650560828300259,
      "grad_norm": 6.017619609832764,
      "learning_rate": 1.6048317515099224e-05,
      "loss": 1.1132,
      "step": 5390
    },
    {
      "epoch": 4.659188955996549,
      "grad_norm": 6.249140739440918,
      "learning_rate": 1.6022433132010357e-05,
      "loss": 0.7532,
      "step": 5400
    },
    {
      "epoch": 4.667817083692839,
      "grad_norm": 5.999796390533447,
      "learning_rate": 1.5996548748921483e-05,
      "loss": 0.9704,
      "step": 5410
    },
    {
      "epoch": 4.676445211389129,
      "grad_norm": 4.321425437927246,
      "learning_rate": 1.5970664365832616e-05,
      "loss": 0.9257,
      "step": 5420
    },
    {
      "epoch": 4.6850733390854185,
      "grad_norm": 93.12113952636719,
      "learning_rate": 1.5944779982743746e-05,
      "loss": 0.8044,
      "step": 5430
    },
    {
      "epoch": 4.693701466781708,
      "grad_norm": 5.996488094329834,
      "learning_rate": 1.591889559965488e-05,
      "loss": 0.9096,
      "step": 5440
    },
    {
      "epoch": 4.702329594477998,
      "grad_norm": 4.618721008300781,
      "learning_rate": 1.5893011216566005e-05,
      "loss": 0.8504,
      "step": 5450
    },
    {
      "epoch": 4.710957722174288,
      "grad_norm": 14.919576644897461,
      "learning_rate": 1.5867126833477137e-05,
      "loss": 0.9973,
      "step": 5460
    },
    {
      "epoch": 4.719585849870578,
      "grad_norm": 4.356051445007324,
      "learning_rate": 1.5841242450388267e-05,
      "loss": 0.9043,
      "step": 5470
    },
    {
      "epoch": 4.728213977566868,
      "grad_norm": 5.904837608337402,
      "learning_rate": 1.58153580672994e-05,
      "loss": 0.9348,
      "step": 5480
    },
    {
      "epoch": 4.7368421052631575,
      "grad_norm": 7.151792049407959,
      "learning_rate": 1.5789473684210526e-05,
      "loss": 0.9714,
      "step": 5490
    },
    {
      "epoch": 4.745470232959448,
      "grad_norm": 7.7001166343688965,
      "learning_rate": 1.5763589301121655e-05,
      "loss": 1.0051,
      "step": 5500
    },
    {
      "epoch": 4.754098360655737,
      "grad_norm": 6.355815887451172,
      "learning_rate": 1.5737704918032788e-05,
      "loss": 1.0887,
      "step": 5510
    },
    {
      "epoch": 4.762726488352028,
      "grad_norm": 6.187618732452393,
      "learning_rate": 1.5711820534943914e-05,
      "loss": 0.8491,
      "step": 5520
    },
    {
      "epoch": 4.771354616048318,
      "grad_norm": 5.265688896179199,
      "learning_rate": 1.5685936151855047e-05,
      "loss": 0.8632,
      "step": 5530
    },
    {
      "epoch": 4.779982743744608,
      "grad_norm": 6.530250072479248,
      "learning_rate": 1.5660051768766177e-05,
      "loss": 0.8478,
      "step": 5540
    },
    {
      "epoch": 4.7886108714408975,
      "grad_norm": 7.539025783538818,
      "learning_rate": 1.563416738567731e-05,
      "loss": 1.0797,
      "step": 5550
    },
    {
      "epoch": 4.797238999137187,
      "grad_norm": 4.342657089233398,
      "learning_rate": 1.5608283002588436e-05,
      "loss": 0.8208,
      "step": 5560
    },
    {
      "epoch": 4.805867126833477,
      "grad_norm": 4.135056018829346,
      "learning_rate": 1.558239861949957e-05,
      "loss": 0.9327,
      "step": 5570
    },
    {
      "epoch": 4.814495254529767,
      "grad_norm": 5.131387233734131,
      "learning_rate": 1.5556514236410698e-05,
      "loss": 0.8377,
      "step": 5580
    },
    {
      "epoch": 4.823123382226057,
      "grad_norm": 6.7750678062438965,
      "learning_rate": 1.553062985332183e-05,
      "loss": 0.8919,
      "step": 5590
    },
    {
      "epoch": 4.831751509922347,
      "grad_norm": 9.90138053894043,
      "learning_rate": 1.5504745470232957e-05,
      "loss": 1.0526,
      "step": 5600
    },
    {
      "epoch": 4.8403796376186365,
      "grad_norm": 6.423156261444092,
      "learning_rate": 1.547886108714409e-05,
      "loss": 0.8454,
      "step": 5610
    },
    {
      "epoch": 4.849007765314926,
      "grad_norm": 6.383857250213623,
      "learning_rate": 1.545297670405522e-05,
      "loss": 0.9227,
      "step": 5620
    },
    {
      "epoch": 4.857635893011216,
      "grad_norm": 7.8532843589782715,
      "learning_rate": 1.5427092320966352e-05,
      "loss": 0.9731,
      "step": 5630
    },
    {
      "epoch": 4.866264020707506,
      "grad_norm": 49.17863464355469,
      "learning_rate": 1.5401207937877478e-05,
      "loss": 0.8827,
      "step": 5640
    },
    {
      "epoch": 4.874892148403797,
      "grad_norm": 4.346700191497803,
      "learning_rate": 1.537532355478861e-05,
      "loss": 0.8147,
      "step": 5650
    },
    {
      "epoch": 4.883520276100087,
      "grad_norm": 6.535245418548584,
      "learning_rate": 1.534943917169974e-05,
      "loss": 0.9545,
      "step": 5660
    },
    {
      "epoch": 4.8921484037963765,
      "grad_norm": 15.618147850036621,
      "learning_rate": 1.5323554788610873e-05,
      "loss": 0.7969,
      "step": 5670
    },
    {
      "epoch": 4.900776531492666,
      "grad_norm": 6.607584476470947,
      "learning_rate": 1.5297670405522e-05,
      "loss": 0.8757,
      "step": 5680
    },
    {
      "epoch": 4.909404659188956,
      "grad_norm": 9.31473159790039,
      "learning_rate": 1.5271786022433132e-05,
      "loss": 1.0727,
      "step": 5690
    },
    {
      "epoch": 4.918032786885246,
      "grad_norm": 4.612117767333984,
      "learning_rate": 1.5245901639344264e-05,
      "loss": 0.8367,
      "step": 5700
    },
    {
      "epoch": 4.926660914581536,
      "grad_norm": 5.944296360015869,
      "learning_rate": 1.5220017256255395e-05,
      "loss": 0.956,
      "step": 5710
    },
    {
      "epoch": 4.935289042277826,
      "grad_norm": 6.799117565155029,
      "learning_rate": 1.5194132873166523e-05,
      "loss": 0.8706,
      "step": 5720
    },
    {
      "epoch": 4.943917169974116,
      "grad_norm": 5.867345809936523,
      "learning_rate": 1.5168248490077654e-05,
      "loss": 0.801,
      "step": 5730
    },
    {
      "epoch": 4.952545297670405,
      "grad_norm": 11.804777145385742,
      "learning_rate": 1.5142364106988785e-05,
      "loss": 0.8743,
      "step": 5740
    },
    {
      "epoch": 4.961173425366695,
      "grad_norm": 4.751771926879883,
      "learning_rate": 1.5116479723899916e-05,
      "loss": 0.8073,
      "step": 5750
    },
    {
      "epoch": 4.969801553062985,
      "grad_norm": 5.879622459411621,
      "learning_rate": 1.5090595340811044e-05,
      "loss": 0.7843,
      "step": 5760
    },
    {
      "epoch": 4.978429680759275,
      "grad_norm": 10.663826942443848,
      "learning_rate": 1.5064710957722175e-05,
      "loss": 0.8496,
      "step": 5770
    },
    {
      "epoch": 4.987057808455565,
      "grad_norm": 2.8931758403778076,
      "learning_rate": 1.5038826574633306e-05,
      "loss": 0.951,
      "step": 5780
    },
    {
      "epoch": 4.995685936151855,
      "grad_norm": 4.240981578826904,
      "learning_rate": 1.5012942191544436e-05,
      "loss": 0.8084,
      "step": 5790
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.5286786556243896,
      "eval_runtime": 2.3586,
      "eval_samples_per_second": 210.297,
      "eval_steps_per_second": 26.287,
      "step": 5795
    },
    {
      "epoch": 5.004314063848145,
      "grad_norm": 4.290783882141113,
      "learning_rate": 1.4987057808455565e-05,
      "loss": 0.8306,
      "step": 5800
    },
    {
      "epoch": 5.012942191544435,
      "grad_norm": 64.72456359863281,
      "learning_rate": 1.4961173425366695e-05,
      "loss": 0.7345,
      "step": 5810
    },
    {
      "epoch": 5.021570319240725,
      "grad_norm": 5.027891159057617,
      "learning_rate": 1.4935289042277826e-05,
      "loss": 1.0452,
      "step": 5820
    },
    {
      "epoch": 5.030198446937015,
      "grad_norm": 4.412449359893799,
      "learning_rate": 1.4909404659188955e-05,
      "loss": 0.8601,
      "step": 5830
    },
    {
      "epoch": 5.038826574633305,
      "grad_norm": 5.115176677703857,
      "learning_rate": 1.4883520276100086e-05,
      "loss": 0.8285,
      "step": 5840
    },
    {
      "epoch": 5.047454702329595,
      "grad_norm": 5.386880397796631,
      "learning_rate": 1.4857635893011216e-05,
      "loss": 0.8052,
      "step": 5850
    },
    {
      "epoch": 5.056082830025884,
      "grad_norm": 8.935781478881836,
      "learning_rate": 1.4831751509922347e-05,
      "loss": 0.9016,
      "step": 5860
    },
    {
      "epoch": 5.064710957722174,
      "grad_norm": 60.016273498535156,
      "learning_rate": 1.4805867126833477e-05,
      "loss": 0.8518,
      "step": 5870
    },
    {
      "epoch": 5.073339085418464,
      "grad_norm": 4.018216609954834,
      "learning_rate": 1.4779982743744608e-05,
      "loss": 0.7358,
      "step": 5880
    },
    {
      "epoch": 5.081967213114754,
      "grad_norm": 5.528759956359863,
      "learning_rate": 1.4754098360655737e-05,
      "loss": 0.782,
      "step": 5890
    },
    {
      "epoch": 5.090595340811044,
      "grad_norm": 6.355035781860352,
      "learning_rate": 1.4728213977566868e-05,
      "loss": 1.082,
      "step": 5900
    },
    {
      "epoch": 5.099223468507334,
      "grad_norm": 3.6446022987365723,
      "learning_rate": 1.4702329594477998e-05,
      "loss": 0.7641,
      "step": 5910
    },
    {
      "epoch": 5.1078515962036235,
      "grad_norm": 4.775167942047119,
      "learning_rate": 1.4676445211389129e-05,
      "loss": 0.9083,
      "step": 5920
    },
    {
      "epoch": 5.116479723899913,
      "grad_norm": 12.208880424499512,
      "learning_rate": 1.4650560828300259e-05,
      "loss": 0.8765,
      "step": 5930
    },
    {
      "epoch": 5.125107851596203,
      "grad_norm": 3.46224045753479,
      "learning_rate": 1.462467644521139e-05,
      "loss": 0.8937,
      "step": 5940
    },
    {
      "epoch": 5.133735979292494,
      "grad_norm": 3.9473695755004883,
      "learning_rate": 1.459879206212252e-05,
      "loss": 0.61,
      "step": 5950
    },
    {
      "epoch": 5.142364106988784,
      "grad_norm": 12.221474647521973,
      "learning_rate": 1.457290767903365e-05,
      "loss": 1.0573,
      "step": 5960
    },
    {
      "epoch": 5.150992234685074,
      "grad_norm": 5.807521343231201,
      "learning_rate": 1.454702329594478e-05,
      "loss": 0.7755,
      "step": 5970
    },
    {
      "epoch": 5.1596203623813635,
      "grad_norm": 10.765088081359863,
      "learning_rate": 1.4521138912855911e-05,
      "loss": 0.8371,
      "step": 5980
    },
    {
      "epoch": 5.168248490077653,
      "grad_norm": 33.53635025024414,
      "learning_rate": 1.449525452976704e-05,
      "loss": 0.9031,
      "step": 5990
    },
    {
      "epoch": 5.176876617773943,
      "grad_norm": 7.203587532043457,
      "learning_rate": 1.4469370146678172e-05,
      "loss": 0.8602,
      "step": 6000
    },
    {
      "epoch": 5.185504745470233,
      "grad_norm": 6.2276530265808105,
      "learning_rate": 1.4443485763589301e-05,
      "loss": 0.915,
      "step": 6010
    },
    {
      "epoch": 5.194132873166523,
      "grad_norm": 5.272498607635498,
      "learning_rate": 1.4417601380500432e-05,
      "loss": 0.9386,
      "step": 6020
    },
    {
      "epoch": 5.202761000862813,
      "grad_norm": 4.95573616027832,
      "learning_rate": 1.4391716997411562e-05,
      "loss": 0.7357,
      "step": 6030
    },
    {
      "epoch": 5.2113891285591025,
      "grad_norm": 5.431813716888428,
      "learning_rate": 1.4365832614322693e-05,
      "loss": 0.8382,
      "step": 6040
    },
    {
      "epoch": 5.220017256255392,
      "grad_norm": 65.74891662597656,
      "learning_rate": 1.4339948231233822e-05,
      "loss": 0.7247,
      "step": 6050
    },
    {
      "epoch": 5.228645383951682,
      "grad_norm": 19.701416015625,
      "learning_rate": 1.4314063848144954e-05,
      "loss": 1.0832,
      "step": 6060
    },
    {
      "epoch": 5.237273511647972,
      "grad_norm": 6.751084804534912,
      "learning_rate": 1.4288179465056083e-05,
      "loss": 0.7502,
      "step": 6070
    },
    {
      "epoch": 5.245901639344262,
      "grad_norm": 14.604159355163574,
      "learning_rate": 1.4262295081967213e-05,
      "loss": 0.8782,
      "step": 6080
    },
    {
      "epoch": 5.254529767040552,
      "grad_norm": 4.217592239379883,
      "learning_rate": 1.4236410698878344e-05,
      "loss": 0.6599,
      "step": 6090
    },
    {
      "epoch": 5.2631578947368425,
      "grad_norm": 6.586548328399658,
      "learning_rate": 1.4210526315789473e-05,
      "loss": 0.7951,
      "step": 6100
    },
    {
      "epoch": 5.271786022433132,
      "grad_norm": 20.856273651123047,
      "learning_rate": 1.4184641932700604e-05,
      "loss": 0.7538,
      "step": 6110
    },
    {
      "epoch": 5.280414150129422,
      "grad_norm": 7.6359076499938965,
      "learning_rate": 1.4158757549611734e-05,
      "loss": 0.9575,
      "step": 6120
    },
    {
      "epoch": 5.289042277825712,
      "grad_norm": 5.772523880004883,
      "learning_rate": 1.4132873166522865e-05,
      "loss": 0.7638,
      "step": 6130
    },
    {
      "epoch": 5.297670405522002,
      "grad_norm": 17.628812789916992,
      "learning_rate": 1.4106988783433995e-05,
      "loss": 0.8072,
      "step": 6140
    },
    {
      "epoch": 5.306298533218292,
      "grad_norm": 5.595774173736572,
      "learning_rate": 1.4081104400345126e-05,
      "loss": 0.9258,
      "step": 6150
    },
    {
      "epoch": 5.3149266609145815,
      "grad_norm": 7.633481979370117,
      "learning_rate": 1.4055220017256255e-05,
      "loss": 0.9163,
      "step": 6160
    },
    {
      "epoch": 5.323554788610871,
      "grad_norm": 4.340407848358154,
      "learning_rate": 1.4029335634167386e-05,
      "loss": 0.812,
      "step": 6170
    },
    {
      "epoch": 5.332182916307161,
      "grad_norm": 5.552212238311768,
      "learning_rate": 1.4003451251078516e-05,
      "loss": 0.8215,
      "step": 6180
    },
    {
      "epoch": 5.340811044003451,
      "grad_norm": 10.834436416625977,
      "learning_rate": 1.3977566867989647e-05,
      "loss": 0.87,
      "step": 6190
    },
    {
      "epoch": 5.349439171699741,
      "grad_norm": 9.093689918518066,
      "learning_rate": 1.3951682484900777e-05,
      "loss": 0.944,
      "step": 6200
    },
    {
      "epoch": 5.358067299396031,
      "grad_norm": 8.92431354522705,
      "learning_rate": 1.3925798101811908e-05,
      "loss": 0.7457,
      "step": 6210
    },
    {
      "epoch": 5.366695427092321,
      "grad_norm": 5.225915908813477,
      "learning_rate": 1.3899913718723037e-05,
      "loss": 0.907,
      "step": 6220
    },
    {
      "epoch": 5.3753235547886105,
      "grad_norm": 12.384308815002441,
      "learning_rate": 1.3874029335634168e-05,
      "loss": 0.8064,
      "step": 6230
    },
    {
      "epoch": 5.383951682484901,
      "grad_norm": 498.79925537109375,
      "learning_rate": 1.3848144952545298e-05,
      "loss": 0.9089,
      "step": 6240
    },
    {
      "epoch": 5.392579810181191,
      "grad_norm": 6.787819862365723,
      "learning_rate": 1.3822260569456429e-05,
      "loss": 0.9908,
      "step": 6250
    },
    {
      "epoch": 5.401207937877481,
      "grad_norm": 8.805879592895508,
      "learning_rate": 1.3796376186367558e-05,
      "loss": 0.8555,
      "step": 6260
    },
    {
      "epoch": 5.409836065573771,
      "grad_norm": 27.78957176208496,
      "learning_rate": 1.377049180327869e-05,
      "loss": 0.7775,
      "step": 6270
    },
    {
      "epoch": 5.418464193270061,
      "grad_norm": 11.655118942260742,
      "learning_rate": 1.3744607420189819e-05,
      "loss": 0.9433,
      "step": 6280
    },
    {
      "epoch": 5.42709232096635,
      "grad_norm": 6.4190778732299805,
      "learning_rate": 1.371872303710095e-05,
      "loss": 0.9123,
      "step": 6290
    },
    {
      "epoch": 5.43572044866264,
      "grad_norm": 9.34205150604248,
      "learning_rate": 1.369283865401208e-05,
      "loss": 0.8318,
      "step": 6300
    },
    {
      "epoch": 5.44434857635893,
      "grad_norm": 8.706588745117188,
      "learning_rate": 1.3666954270923211e-05,
      "loss": 0.8181,
      "step": 6310
    },
    {
      "epoch": 5.45297670405522,
      "grad_norm": 5.74956750869751,
      "learning_rate": 1.364106988783434e-05,
      "loss": 0.75,
      "step": 6320
    },
    {
      "epoch": 5.46160483175151,
      "grad_norm": 5.731367588043213,
      "learning_rate": 1.3615185504745472e-05,
      "loss": 0.8279,
      "step": 6330
    },
    {
      "epoch": 5.4702329594478,
      "grad_norm": 3.9199161529541016,
      "learning_rate": 1.3589301121656601e-05,
      "loss": 0.9202,
      "step": 6340
    },
    {
      "epoch": 5.4788610871440895,
      "grad_norm": 4.499319553375244,
      "learning_rate": 1.3563416738567732e-05,
      "loss": 0.7434,
      "step": 6350
    },
    {
      "epoch": 5.487489214840379,
      "grad_norm": 6.856147289276123,
      "learning_rate": 1.3537532355478862e-05,
      "loss": 0.9396,
      "step": 6360
    },
    {
      "epoch": 5.496117342536669,
      "grad_norm": 4.8360161781311035,
      "learning_rate": 1.3511647972389993e-05,
      "loss": 0.7954,
      "step": 6370
    },
    {
      "epoch": 5.50474547023296,
      "grad_norm": 7.1559014320373535,
      "learning_rate": 1.3485763589301122e-05,
      "loss": 0.9274,
      "step": 6380
    },
    {
      "epoch": 5.513373597929249,
      "grad_norm": 9.541759490966797,
      "learning_rate": 1.3459879206212254e-05,
      "loss": 0.779,
      "step": 6390
    },
    {
      "epoch": 5.52200172562554,
      "grad_norm": 4.665480136871338,
      "learning_rate": 1.3433994823123383e-05,
      "loss": 0.86,
      "step": 6400
    },
    {
      "epoch": 5.530629853321829,
      "grad_norm": 4.691307544708252,
      "learning_rate": 1.3408110440034513e-05,
      "loss": 0.8141,
      "step": 6410
    },
    {
      "epoch": 5.539257981018119,
      "grad_norm": 3.777953624725342,
      "learning_rate": 1.3382226056945642e-05,
      "loss": 0.6752,
      "step": 6420
    },
    {
      "epoch": 5.547886108714409,
      "grad_norm": 4.627112865447998,
      "learning_rate": 1.3356341673856773e-05,
      "loss": 0.8334,
      "step": 6430
    },
    {
      "epoch": 5.556514236410699,
      "grad_norm": 5.67430305480957,
      "learning_rate": 1.3330457290767903e-05,
      "loss": 0.9388,
      "step": 6440
    },
    {
      "epoch": 5.565142364106989,
      "grad_norm": 4.843880653381348,
      "learning_rate": 1.3304572907679034e-05,
      "loss": 0.8061,
      "step": 6450
    },
    {
      "epoch": 5.573770491803279,
      "grad_norm": 10.253564834594727,
      "learning_rate": 1.3278688524590163e-05,
      "loss": 0.7913,
      "step": 6460
    },
    {
      "epoch": 5.5823986194995685,
      "grad_norm": 17.530010223388672,
      "learning_rate": 1.3252804141501295e-05,
      "loss": 0.9574,
      "step": 6470
    },
    {
      "epoch": 5.591026747195858,
      "grad_norm": 3.404935121536255,
      "learning_rate": 1.3226919758412424e-05,
      "loss": 0.9227,
      "step": 6480
    },
    {
      "epoch": 5.599654874892148,
      "grad_norm": 100.4137954711914,
      "learning_rate": 1.3201035375323555e-05,
      "loss": 1.0544,
      "step": 6490
    },
    {
      "epoch": 5.608283002588438,
      "grad_norm": 11.240732192993164,
      "learning_rate": 1.3175150992234685e-05,
      "loss": 0.6057,
      "step": 6500
    },
    {
      "epoch": 5.616911130284728,
      "grad_norm": 5.691351413726807,
      "learning_rate": 1.3149266609145816e-05,
      "loss": 0.7735,
      "step": 6510
    },
    {
      "epoch": 5.625539257981018,
      "grad_norm": 21.495943069458008,
      "learning_rate": 1.3123382226056945e-05,
      "loss": 0.8747,
      "step": 6520
    },
    {
      "epoch": 5.6341673856773085,
      "grad_norm": 3.5116758346557617,
      "learning_rate": 1.3097497842968076e-05,
      "loss": 0.6374,
      "step": 6530
    },
    {
      "epoch": 5.642795513373598,
      "grad_norm": 14.798768043518066,
      "learning_rate": 1.3071613459879206e-05,
      "loss": 1.1275,
      "step": 6540
    },
    {
      "epoch": 5.651423641069888,
      "grad_norm": 5.087140083312988,
      "learning_rate": 1.3045729076790337e-05,
      "loss": 0.8844,
      "step": 6550
    },
    {
      "epoch": 5.660051768766178,
      "grad_norm": 5.858088970184326,
      "learning_rate": 1.3019844693701467e-05,
      "loss": 0.7999,
      "step": 6560
    },
    {
      "epoch": 5.668679896462468,
      "grad_norm": 6.190721035003662,
      "learning_rate": 1.2993960310612598e-05,
      "loss": 0.8219,
      "step": 6570
    },
    {
      "epoch": 5.677308024158758,
      "grad_norm": 4.372045040130615,
      "learning_rate": 1.2968075927523727e-05,
      "loss": 0.7746,
      "step": 6580
    },
    {
      "epoch": 5.6859361518550475,
      "grad_norm": 4.496771335601807,
      "learning_rate": 1.2942191544434858e-05,
      "loss": 0.8491,
      "step": 6590
    },
    {
      "epoch": 5.694564279551337,
      "grad_norm": 4.733067512512207,
      "learning_rate": 1.2916307161345988e-05,
      "loss": 0.8195,
      "step": 6600
    },
    {
      "epoch": 5.703192407247627,
      "grad_norm": 4.766611099243164,
      "learning_rate": 1.2890422778257119e-05,
      "loss": 0.7285,
      "step": 6610
    },
    {
      "epoch": 5.711820534943917,
      "grad_norm": 5.441579818725586,
      "learning_rate": 1.2864538395168249e-05,
      "loss": 0.8302,
      "step": 6620
    },
    {
      "epoch": 5.720448662640207,
      "grad_norm": 14.190784454345703,
      "learning_rate": 1.2838654012079378e-05,
      "loss": 0.8696,
      "step": 6630
    },
    {
      "epoch": 5.729076790336497,
      "grad_norm": 6.3146185874938965,
      "learning_rate": 1.281276962899051e-05,
      "loss": 0.9175,
      "step": 6640
    },
    {
      "epoch": 5.737704918032787,
      "grad_norm": 8.280065536499023,
      "learning_rate": 1.2786885245901639e-05,
      "loss": 0.8865,
      "step": 6650
    },
    {
      "epoch": 5.746333045729076,
      "grad_norm": 6.230611324310303,
      "learning_rate": 1.276100086281277e-05,
      "loss": 0.7682,
      "step": 6660
    },
    {
      "epoch": 5.754961173425366,
      "grad_norm": 6.804759979248047,
      "learning_rate": 1.27351164797239e-05,
      "loss": 0.874,
      "step": 6670
    },
    {
      "epoch": 5.763589301121657,
      "grad_norm": 13.052200317382812,
      "learning_rate": 1.270923209663503e-05,
      "loss": 0.7399,
      "step": 6680
    },
    {
      "epoch": 5.772217428817947,
      "grad_norm": 5.6287336349487305,
      "learning_rate": 1.268334771354616e-05,
      "loss": 0.9442,
      "step": 6690
    },
    {
      "epoch": 5.780845556514237,
      "grad_norm": 7.456914901733398,
      "learning_rate": 1.2657463330457291e-05,
      "loss": 0.9206,
      "step": 6700
    },
    {
      "epoch": 5.7894736842105265,
      "grad_norm": 3.8809523582458496,
      "learning_rate": 1.263157894736842e-05,
      "loss": 0.7783,
      "step": 6710
    },
    {
      "epoch": 5.798101811906816,
      "grad_norm": 5.902669429779053,
      "learning_rate": 1.2605694564279552e-05,
      "loss": 0.8105,
      "step": 6720
    },
    {
      "epoch": 5.806729939603106,
      "grad_norm": 2.0187087059020996,
      "learning_rate": 1.2579810181190681e-05,
      "loss": 0.8133,
      "step": 6730
    },
    {
      "epoch": 5.815358067299396,
      "grad_norm": 9.718408584594727,
      "learning_rate": 1.2553925798101812e-05,
      "loss": 0.8545,
      "step": 6740
    },
    {
      "epoch": 5.823986194995686,
      "grad_norm": 33.936309814453125,
      "learning_rate": 1.2528041415012942e-05,
      "loss": 1.0006,
      "step": 6750
    },
    {
      "epoch": 5.832614322691976,
      "grad_norm": 5.8312788009643555,
      "learning_rate": 1.2502157031924073e-05,
      "loss": 0.9865,
      "step": 6760
    },
    {
      "epoch": 5.841242450388266,
      "grad_norm": 5.465548515319824,
      "learning_rate": 1.2476272648835203e-05,
      "loss": 0.8637,
      "step": 6770
    },
    {
      "epoch": 5.8498705780845555,
      "grad_norm": 5.163577556610107,
      "learning_rate": 1.2450388265746334e-05,
      "loss": 0.6818,
      "step": 6780
    },
    {
      "epoch": 5.858498705780845,
      "grad_norm": 7.302801609039307,
      "learning_rate": 1.2424503882657463e-05,
      "loss": 1.0319,
      "step": 6790
    },
    {
      "epoch": 5.867126833477135,
      "grad_norm": 8.42604923248291,
      "learning_rate": 1.2398619499568594e-05,
      "loss": 0.8328,
      "step": 6800
    },
    {
      "epoch": 5.875754961173425,
      "grad_norm": 6.574827194213867,
      "learning_rate": 1.2372735116479724e-05,
      "loss": 0.7453,
      "step": 6810
    },
    {
      "epoch": 5.884383088869715,
      "grad_norm": 3.851970672607422,
      "learning_rate": 1.2346850733390855e-05,
      "loss": 0.795,
      "step": 6820
    },
    {
      "epoch": 5.893011216566006,
      "grad_norm": 5.836277008056641,
      "learning_rate": 1.2320966350301985e-05,
      "loss": 0.8409,
      "step": 6830
    },
    {
      "epoch": 5.901639344262295,
      "grad_norm": 41.575843811035156,
      "learning_rate": 1.2295081967213116e-05,
      "loss": 0.8248,
      "step": 6840
    },
    {
      "epoch": 5.910267471958585,
      "grad_norm": 5.23711633682251,
      "learning_rate": 1.2269197584124245e-05,
      "loss": 0.8853,
      "step": 6850
    },
    {
      "epoch": 5.918895599654875,
      "grad_norm": 12.142868995666504,
      "learning_rate": 1.2243313201035376e-05,
      "loss": 0.9623,
      "step": 6860
    },
    {
      "epoch": 5.927523727351165,
      "grad_norm": 4.840908527374268,
      "learning_rate": 1.2217428817946506e-05,
      "loss": 0.8911,
      "step": 6870
    },
    {
      "epoch": 5.936151855047455,
      "grad_norm": 6.831459045410156,
      "learning_rate": 1.2191544434857637e-05,
      "loss": 0.8621,
      "step": 6880
    },
    {
      "epoch": 5.944779982743745,
      "grad_norm": 5.529226303100586,
      "learning_rate": 1.2165660051768767e-05,
      "loss": 0.9448,
      "step": 6890
    },
    {
      "epoch": 5.9534081104400345,
      "grad_norm": 5.592390537261963,
      "learning_rate": 1.2139775668679898e-05,
      "loss": 0.7552,
      "step": 6900
    },
    {
      "epoch": 5.962036238136324,
      "grad_norm": 6.414905071258545,
      "learning_rate": 1.2113891285591027e-05,
      "loss": 0.8287,
      "step": 6910
    },
    {
      "epoch": 5.970664365832614,
      "grad_norm": 12.883221626281738,
      "learning_rate": 1.2088006902502158e-05,
      "loss": 0.897,
      "step": 6920
    },
    {
      "epoch": 5.979292493528904,
      "grad_norm": 5.259849548339844,
      "learning_rate": 1.2062122519413288e-05,
      "loss": 0.7937,
      "step": 6930
    },
    {
      "epoch": 5.987920621225194,
      "grad_norm": 5.688714981079102,
      "learning_rate": 1.2036238136324419e-05,
      "loss": 0.7024,
      "step": 6940
    },
    {
      "epoch": 5.996548748921484,
      "grad_norm": 10.46086597442627,
      "learning_rate": 1.2010353753235549e-05,
      "loss": 0.8664,
      "step": 6950
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.5184587836265564,
      "eval_runtime": 2.3494,
      "eval_samples_per_second": 211.12,
      "eval_steps_per_second": 26.39,
      "step": 6954
    },
    {
      "epoch": 6.0051768766177736,
      "grad_norm": 21.47927474975586,
      "learning_rate": 1.198446937014668e-05,
      "loss": 0.9769,
      "step": 6960
    },
    {
      "epoch": 6.013805004314064,
      "grad_norm": 5.826367378234863,
      "learning_rate": 1.195858498705781e-05,
      "loss": 0.9166,
      "step": 6970
    },
    {
      "epoch": 6.022433132010354,
      "grad_norm": 161.82530212402344,
      "learning_rate": 1.193270060396894e-05,
      "loss": 0.8386,
      "step": 6980
    },
    {
      "epoch": 6.031061259706644,
      "grad_norm": 19.01866912841797,
      "learning_rate": 1.190681622088007e-05,
      "loss": 0.8229,
      "step": 6990
    },
    {
      "epoch": 6.039689387402934,
      "grad_norm": 6.03926944732666,
      "learning_rate": 1.1880931837791201e-05,
      "loss": 0.7481,
      "step": 7000
    },
    {
      "epoch": 6.048317515099224,
      "grad_norm": 3.8162503242492676,
      "learning_rate": 1.185504745470233e-05,
      "loss": 0.7693,
      "step": 7010
    },
    {
      "epoch": 6.0569456427955135,
      "grad_norm": 4.730118751525879,
      "learning_rate": 1.182916307161346e-05,
      "loss": 0.9694,
      "step": 7020
    },
    {
      "epoch": 6.065573770491803,
      "grad_norm": 5.0827813148498535,
      "learning_rate": 1.180327868852459e-05,
      "loss": 0.808,
      "step": 7030
    },
    {
      "epoch": 6.074201898188093,
      "grad_norm": 3.5114283561706543,
      "learning_rate": 1.177739430543572e-05,
      "loss": 0.8894,
      "step": 7040
    },
    {
      "epoch": 6.082830025884383,
      "grad_norm": 10.170462608337402,
      "learning_rate": 1.175150992234685e-05,
      "loss": 0.7991,
      "step": 7050
    },
    {
      "epoch": 6.091458153580673,
      "grad_norm": 4.579659461975098,
      "learning_rate": 1.1725625539257981e-05,
      "loss": 0.697,
      "step": 7060
    },
    {
      "epoch": 6.100086281276963,
      "grad_norm": 5.734504222869873,
      "learning_rate": 1.169974115616911e-05,
      "loss": 0.8429,
      "step": 7070
    },
    {
      "epoch": 6.108714408973253,
      "grad_norm": 4.644054412841797,
      "learning_rate": 1.1673856773080242e-05,
      "loss": 0.805,
      "step": 7080
    },
    {
      "epoch": 6.117342536669542,
      "grad_norm": 9.775574684143066,
      "learning_rate": 1.1647972389991371e-05,
      "loss": 0.8453,
      "step": 7090
    },
    {
      "epoch": 6.125970664365832,
      "grad_norm": 19.62481117248535,
      "learning_rate": 1.1622088006902503e-05,
      "loss": 0.9691,
      "step": 7100
    },
    {
      "epoch": 6.134598792062122,
      "grad_norm": 7.8998565673828125,
      "learning_rate": 1.1596203623813632e-05,
      "loss": 0.7818,
      "step": 7110
    },
    {
      "epoch": 6.143226919758413,
      "grad_norm": 19.005985260009766,
      "learning_rate": 1.1570319240724763e-05,
      "loss": 0.7829,
      "step": 7120
    },
    {
      "epoch": 6.151855047454703,
      "grad_norm": 7.470883846282959,
      "learning_rate": 1.1544434857635893e-05,
      "loss": 0.8612,
      "step": 7130
    },
    {
      "epoch": 6.1604831751509925,
      "grad_norm": 224.5339813232422,
      "learning_rate": 1.1518550474547024e-05,
      "loss": 0.7882,
      "step": 7140
    },
    {
      "epoch": 6.169111302847282,
      "grad_norm": 6.3508381843566895,
      "learning_rate": 1.1492666091458153e-05,
      "loss": 0.8988,
      "step": 7150
    },
    {
      "epoch": 6.177739430543572,
      "grad_norm": 4.720486164093018,
      "learning_rate": 1.1466781708369285e-05,
      "loss": 0.8778,
      "step": 7160
    },
    {
      "epoch": 6.186367558239862,
      "grad_norm": 4.210746765136719,
      "learning_rate": 1.1440897325280414e-05,
      "loss": 0.7249,
      "step": 7170
    },
    {
      "epoch": 6.194995685936152,
      "grad_norm": 5.582554817199707,
      "learning_rate": 1.1415012942191543e-05,
      "loss": 0.8207,
      "step": 7180
    },
    {
      "epoch": 6.203623813632442,
      "grad_norm": 8.291606903076172,
      "learning_rate": 1.1389128559102675e-05,
      "loss": 0.8644,
      "step": 7190
    },
    {
      "epoch": 6.212251941328732,
      "grad_norm": 4.599273681640625,
      "learning_rate": 1.1363244176013804e-05,
      "loss": 0.7003,
      "step": 7200
    },
    {
      "epoch": 6.220880069025021,
      "grad_norm": 6.054747581481934,
      "learning_rate": 1.1337359792924935e-05,
      "loss": 0.802,
      "step": 7210
    },
    {
      "epoch": 6.229508196721311,
      "grad_norm": 3.83278226852417,
      "learning_rate": 1.1311475409836065e-05,
      "loss": 0.6186,
      "step": 7220
    },
    {
      "epoch": 6.238136324417601,
      "grad_norm": 5.3227715492248535,
      "learning_rate": 1.1285591026747196e-05,
      "loss": 0.8432,
      "step": 7230
    },
    {
      "epoch": 6.246764452113891,
      "grad_norm": 5.627090930938721,
      "learning_rate": 1.1259706643658325e-05,
      "loss": 0.775,
      "step": 7240
    },
    {
      "epoch": 6.255392579810181,
      "grad_norm": 5.639713287353516,
      "learning_rate": 1.1233822260569457e-05,
      "loss": 0.7256,
      "step": 7250
    },
    {
      "epoch": 6.2640207075064716,
      "grad_norm": 5.425751686096191,
      "learning_rate": 1.1207937877480586e-05,
      "loss": 0.8334,
      "step": 7260
    },
    {
      "epoch": 6.272648835202761,
      "grad_norm": 6.323893070220947,
      "learning_rate": 1.1182053494391717e-05,
      "loss": 0.7269,
      "step": 7270
    },
    {
      "epoch": 6.281276962899051,
      "grad_norm": 6.8596954345703125,
      "learning_rate": 1.1156169111302847e-05,
      "loss": 0.9303,
      "step": 7280
    },
    {
      "epoch": 6.289905090595341,
      "grad_norm": 7.010547161102295,
      "learning_rate": 1.1130284728213978e-05,
      "loss": 0.9642,
      "step": 7290
    },
    {
      "epoch": 6.298533218291631,
      "grad_norm": 5.511105060577393,
      "learning_rate": 1.1104400345125107e-05,
      "loss": 0.8039,
      "step": 7300
    },
    {
      "epoch": 6.307161345987921,
      "grad_norm": 5.012686252593994,
      "learning_rate": 1.1078515962036239e-05,
      "loss": 0.7795,
      "step": 7310
    },
    {
      "epoch": 6.315789473684211,
      "grad_norm": 8.766256332397461,
      "learning_rate": 1.1052631578947368e-05,
      "loss": 0.7323,
      "step": 7320
    },
    {
      "epoch": 6.3244176013805005,
      "grad_norm": 7.590869903564453,
      "learning_rate": 1.10267471958585e-05,
      "loss": 0.6983,
      "step": 7330
    },
    {
      "epoch": 6.33304572907679,
      "grad_norm": 117.50701904296875,
      "learning_rate": 1.1000862812769629e-05,
      "loss": 0.837,
      "step": 7340
    },
    {
      "epoch": 6.34167385677308,
      "grad_norm": 10.463208198547363,
      "learning_rate": 1.097497842968076e-05,
      "loss": 0.7275,
      "step": 7350
    },
    {
      "epoch": 6.35030198446937,
      "grad_norm": 4.176485538482666,
      "learning_rate": 1.094909404659189e-05,
      "loss": 0.6607,
      "step": 7360
    },
    {
      "epoch": 6.35893011216566,
      "grad_norm": 4.261324882507324,
      "learning_rate": 1.092320966350302e-05,
      "loss": 0.7858,
      "step": 7370
    },
    {
      "epoch": 6.36755823986195,
      "grad_norm": 6.2129998207092285,
      "learning_rate": 1.089732528041415e-05,
      "loss": 0.8279,
      "step": 7380
    },
    {
      "epoch": 6.3761863675582395,
      "grad_norm": 58.87836837768555,
      "learning_rate": 1.0871440897325281e-05,
      "loss": 0.7829,
      "step": 7390
    },
    {
      "epoch": 6.384814495254529,
      "grad_norm": 5.809418678283691,
      "learning_rate": 1.084555651423641e-05,
      "loss": 0.8805,
      "step": 7400
    },
    {
      "epoch": 6.39344262295082,
      "grad_norm": 6.165597915649414,
      "learning_rate": 1.0819672131147542e-05,
      "loss": 0.9692,
      "step": 7410
    },
    {
      "epoch": 6.40207075064711,
      "grad_norm": 10.63205623626709,
      "learning_rate": 1.0793787748058671e-05,
      "loss": 0.8369,
      "step": 7420
    },
    {
      "epoch": 6.4106988783434,
      "grad_norm": 6.443229675292969,
      "learning_rate": 1.0767903364969803e-05,
      "loss": 0.8134,
      "step": 7430
    },
    {
      "epoch": 6.41932700603969,
      "grad_norm": 5.729144096374512,
      "learning_rate": 1.0742018981880932e-05,
      "loss": 0.7437,
      "step": 7440
    },
    {
      "epoch": 6.4279551337359795,
      "grad_norm": 4.525337219238281,
      "learning_rate": 1.0716134598792063e-05,
      "loss": 0.734,
      "step": 7450
    },
    {
      "epoch": 6.436583261432269,
      "grad_norm": 4.444895267486572,
      "learning_rate": 1.0690250215703193e-05,
      "loss": 0.8583,
      "step": 7460
    },
    {
      "epoch": 6.445211389128559,
      "grad_norm": 4.598628044128418,
      "learning_rate": 1.0664365832614324e-05,
      "loss": 0.8353,
      "step": 7470
    },
    {
      "epoch": 6.453839516824849,
      "grad_norm": 7.496971130371094,
      "learning_rate": 1.0638481449525453e-05,
      "loss": 0.852,
      "step": 7480
    },
    {
      "epoch": 6.462467644521139,
      "grad_norm": 7.047387599945068,
      "learning_rate": 1.0612597066436584e-05,
      "loss": 0.8137,
      "step": 7490
    },
    {
      "epoch": 6.471095772217429,
      "grad_norm": 7.02128791809082,
      "learning_rate": 1.0586712683347714e-05,
      "loss": 0.7986,
      "step": 7500
    },
    {
      "epoch": 6.4797238999137186,
      "grad_norm": 3.216564655303955,
      "learning_rate": 1.0560828300258845e-05,
      "loss": 0.8533,
      "step": 7510
    },
    {
      "epoch": 6.488352027610008,
      "grad_norm": 6.528343200683594,
      "learning_rate": 1.0534943917169975e-05,
      "loss": 0.8161,
      "step": 7520
    },
    {
      "epoch": 6.496980155306298,
      "grad_norm": 7.951927661895752,
      "learning_rate": 1.0509059534081106e-05,
      "loss": 0.9157,
      "step": 7530
    },
    {
      "epoch": 6.505608283002588,
      "grad_norm": 36.78913497924805,
      "learning_rate": 1.0483175150992235e-05,
      "loss": 1.0979,
      "step": 7540
    },
    {
      "epoch": 6.514236410698878,
      "grad_norm": 7.5087666511535645,
      "learning_rate": 1.0457290767903366e-05,
      "loss": 0.8821,
      "step": 7550
    },
    {
      "epoch": 6.522864538395169,
      "grad_norm": 5.519114971160889,
      "learning_rate": 1.0431406384814496e-05,
      "loss": 0.7229,
      "step": 7560
    },
    {
      "epoch": 6.5314926660914585,
      "grad_norm": 50.71177291870117,
      "learning_rate": 1.0405522001725627e-05,
      "loss": 0.7015,
      "step": 7570
    },
    {
      "epoch": 6.540120793787748,
      "grad_norm": 6.742881774902344,
      "learning_rate": 1.0379637618636757e-05,
      "loss": 0.8775,
      "step": 7580
    },
    {
      "epoch": 6.548748921484038,
      "grad_norm": 56.917396545410156,
      "learning_rate": 1.0353753235547888e-05,
      "loss": 0.7367,
      "step": 7590
    },
    {
      "epoch": 6.557377049180328,
      "grad_norm": 43.49650573730469,
      "learning_rate": 1.0327868852459017e-05,
      "loss": 0.723,
      "step": 7600
    },
    {
      "epoch": 6.566005176876618,
      "grad_norm": 22.711410522460938,
      "learning_rate": 1.0301984469370148e-05,
      "loss": 0.6796,
      "step": 7610
    },
    {
      "epoch": 6.574633304572908,
      "grad_norm": 6.824017524719238,
      "learning_rate": 1.0276100086281278e-05,
      "loss": 0.7618,
      "step": 7620
    },
    {
      "epoch": 6.583261432269198,
      "grad_norm": 8.289018630981445,
      "learning_rate": 1.0250215703192407e-05,
      "loss": 0.8564,
      "step": 7630
    },
    {
      "epoch": 6.591889559965487,
      "grad_norm": 7.245110511779785,
      "learning_rate": 1.0224331320103537e-05,
      "loss": 0.7388,
      "step": 7640
    },
    {
      "epoch": 6.600517687661777,
      "grad_norm": 5.102838039398193,
      "learning_rate": 1.0198446937014668e-05,
      "loss": 0.8053,
      "step": 7650
    },
    {
      "epoch": 6.609145815358067,
      "grad_norm": 8.976155281066895,
      "learning_rate": 1.0172562553925798e-05,
      "loss": 0.6869,
      "step": 7660
    },
    {
      "epoch": 6.617773943054357,
      "grad_norm": 6.995297431945801,
      "learning_rate": 1.0146678170836929e-05,
      "loss": 0.7405,
      "step": 7670
    },
    {
      "epoch": 6.626402070750647,
      "grad_norm": 5.051607608795166,
      "learning_rate": 1.0120793787748058e-05,
      "loss": 0.7416,
      "step": 7680
    },
    {
      "epoch": 6.635030198446937,
      "grad_norm": 17.367578506469727,
      "learning_rate": 1.009490940465919e-05,
      "loss": 0.8214,
      "step": 7690
    },
    {
      "epoch": 6.6436583261432265,
      "grad_norm": 5.616825580596924,
      "learning_rate": 1.0069025021570319e-05,
      "loss": 0.8563,
      "step": 7700
    },
    {
      "epoch": 6.652286453839517,
      "grad_norm": 25.64552116394043,
      "learning_rate": 1.004314063848145e-05,
      "loss": 0.791,
      "step": 7710
    },
    {
      "epoch": 6.660914581535807,
      "grad_norm": 5.25573205947876,
      "learning_rate": 1.001725625539258e-05,
      "loss": 0.7274,
      "step": 7720
    },
    {
      "epoch": 6.669542709232097,
      "grad_norm": 7.51232385635376,
      "learning_rate": 9.991371872303709e-06,
      "loss": 0.7134,
      "step": 7730
    },
    {
      "epoch": 6.678170836928387,
      "grad_norm": 62.217201232910156,
      "learning_rate": 9.96548748921484e-06,
      "loss": 0.8179,
      "step": 7740
    },
    {
      "epoch": 6.686798964624677,
      "grad_norm": 6.644719123840332,
      "learning_rate": 9.93960310612597e-06,
      "loss": 0.6058,
      "step": 7750
    },
    {
      "epoch": 6.6954270923209664,
      "grad_norm": 4.304411888122559,
      "learning_rate": 9.9137187230371e-06,
      "loss": 0.6437,
      "step": 7760
    },
    {
      "epoch": 6.704055220017256,
      "grad_norm": 6.47216272354126,
      "learning_rate": 9.88783433994823e-06,
      "loss": 0.6145,
      "step": 7770
    },
    {
      "epoch": 6.712683347713546,
      "grad_norm": 6.221397876739502,
      "learning_rate": 9.861949956859361e-06,
      "loss": 0.902,
      "step": 7780
    },
    {
      "epoch": 6.721311475409836,
      "grad_norm": 6.108424186706543,
      "learning_rate": 9.836065573770491e-06,
      "loss": 0.7016,
      "step": 7790
    },
    {
      "epoch": 6.729939603106126,
      "grad_norm": 9.64782428741455,
      "learning_rate": 9.810181190681622e-06,
      "loss": 0.8179,
      "step": 7800
    },
    {
      "epoch": 6.738567730802416,
      "grad_norm": 5.070145606994629,
      "learning_rate": 9.784296807592752e-06,
      "loss": 0.8122,
      "step": 7810
    },
    {
      "epoch": 6.7471958584987055,
      "grad_norm": 8.496355056762695,
      "learning_rate": 9.758412424503883e-06,
      "loss": 0.7143,
      "step": 7820
    },
    {
      "epoch": 6.755823986194995,
      "grad_norm": 9.148058891296387,
      "learning_rate": 9.732528041415012e-06,
      "loss": 1.0485,
      "step": 7830
    },
    {
      "epoch": 6.764452113891286,
      "grad_norm": 4.252940654754639,
      "learning_rate": 9.706643658326143e-06,
      "loss": 0.761,
      "step": 7840
    },
    {
      "epoch": 6.773080241587575,
      "grad_norm": 6.3572797775268555,
      "learning_rate": 9.680759275237273e-06,
      "loss": 0.7751,
      "step": 7850
    },
    {
      "epoch": 6.781708369283866,
      "grad_norm": 33.020164489746094,
      "learning_rate": 9.654874892148404e-06,
      "loss": 0.6436,
      "step": 7860
    },
    {
      "epoch": 6.790336496980156,
      "grad_norm": 5.5505805015563965,
      "learning_rate": 9.628990509059534e-06,
      "loss": 0.7442,
      "step": 7870
    },
    {
      "epoch": 6.7989646246764455,
      "grad_norm": 6.438973903656006,
      "learning_rate": 9.603106125970665e-06,
      "loss": 0.8717,
      "step": 7880
    },
    {
      "epoch": 6.807592752372735,
      "grad_norm": 5.089540481567383,
      "learning_rate": 9.577221742881794e-06,
      "loss": 0.7365,
      "step": 7890
    },
    {
      "epoch": 6.816220880069025,
      "grad_norm": 4.403569221496582,
      "learning_rate": 9.551337359792925e-06,
      "loss": 0.6937,
      "step": 7900
    },
    {
      "epoch": 6.824849007765315,
      "grad_norm": 3.388965368270874,
      "learning_rate": 9.525452976704055e-06,
      "loss": 0.8008,
      "step": 7910
    },
    {
      "epoch": 6.833477135461605,
      "grad_norm": 5.307678699493408,
      "learning_rate": 9.499568593615186e-06,
      "loss": 0.8396,
      "step": 7920
    },
    {
      "epoch": 6.842105263157895,
      "grad_norm": 6.961763858795166,
      "learning_rate": 9.473684210526315e-06,
      "loss": 0.7562,
      "step": 7930
    },
    {
      "epoch": 6.8507333908541845,
      "grad_norm": 4.563514709472656,
      "learning_rate": 9.447799827437447e-06,
      "loss": 0.7863,
      "step": 7940
    },
    {
      "epoch": 6.859361518550474,
      "grad_norm": 5.355233669281006,
      "learning_rate": 9.421915444348576e-06,
      "loss": 0.9447,
      "step": 7950
    },
    {
      "epoch": 6.867989646246764,
      "grad_norm": 6.1926703453063965,
      "learning_rate": 9.396031061259707e-06,
      "loss": 0.9672,
      "step": 7960
    },
    {
      "epoch": 6.876617773943054,
      "grad_norm": 5.336748123168945,
      "learning_rate": 9.370146678170837e-06,
      "loss": 0.7276,
      "step": 7970
    },
    {
      "epoch": 6.885245901639344,
      "grad_norm": 4.427100658416748,
      "learning_rate": 9.344262295081968e-06,
      "loss": 0.6763,
      "step": 7980
    },
    {
      "epoch": 6.893874029335635,
      "grad_norm": 6.9690375328063965,
      "learning_rate": 9.318377911993097e-06,
      "loss": 0.7188,
      "step": 7990
    },
    {
      "epoch": 6.902502157031924,
      "grad_norm": 6.2335615158081055,
      "learning_rate": 9.292493528904229e-06,
      "loss": 0.8121,
      "step": 8000
    },
    {
      "epoch": 6.911130284728214,
      "grad_norm": 8.080184936523438,
      "learning_rate": 9.266609145815358e-06,
      "loss": 0.8201,
      "step": 8010
    },
    {
      "epoch": 6.919758412424504,
      "grad_norm": 5.821708679199219,
      "learning_rate": 9.24072476272649e-06,
      "loss": 0.7889,
      "step": 8020
    },
    {
      "epoch": 6.928386540120794,
      "grad_norm": 6.182467460632324,
      "learning_rate": 9.214840379637619e-06,
      "loss": 0.7761,
      "step": 8030
    },
    {
      "epoch": 6.937014667817084,
      "grad_norm": 4.8180341720581055,
      "learning_rate": 9.18895599654875e-06,
      "loss": 0.9652,
      "step": 8040
    },
    {
      "epoch": 6.945642795513374,
      "grad_norm": 5.945802688598633,
      "learning_rate": 9.16307161345988e-06,
      "loss": 0.7099,
      "step": 8050
    },
    {
      "epoch": 6.954270923209664,
      "grad_norm": 6.415197372436523,
      "learning_rate": 9.13718723037101e-06,
      "loss": 0.8911,
      "step": 8060
    },
    {
      "epoch": 6.962899050905953,
      "grad_norm": 15.694412231445312,
      "learning_rate": 9.11130284728214e-06,
      "loss": 0.667,
      "step": 8070
    },
    {
      "epoch": 6.971527178602243,
      "grad_norm": 3.9175069332122803,
      "learning_rate": 9.085418464193271e-06,
      "loss": 0.7232,
      "step": 8080
    },
    {
      "epoch": 6.980155306298533,
      "grad_norm": 11.993420600891113,
      "learning_rate": 9.0595340811044e-06,
      "loss": 0.9837,
      "step": 8090
    },
    {
      "epoch": 6.988783433994823,
      "grad_norm": 4.623828411102295,
      "learning_rate": 9.033649698015532e-06,
      "loss": 0.7477,
      "step": 8100
    },
    {
      "epoch": 6.997411561691113,
      "grad_norm": 23.864242553710938,
      "learning_rate": 9.007765314926661e-06,
      "loss": 0.7927,
      "step": 8110
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.4971844553947449,
      "eval_runtime": 2.364,
      "eval_samples_per_second": 209.817,
      "eval_steps_per_second": 26.227,
      "step": 8113
    },
    {
      "epoch": 7.006039689387403,
      "grad_norm": 5.73969030380249,
      "learning_rate": 8.981880931837793e-06,
      "loss": 0.6428,
      "step": 8120
    },
    {
      "epoch": 7.0146678170836925,
      "grad_norm": 7.913454055786133,
      "learning_rate": 8.955996548748922e-06,
      "loss": 0.8455,
      "step": 8130
    },
    {
      "epoch": 7.023295944779982,
      "grad_norm": 4.874753475189209,
      "learning_rate": 8.930112165660053e-06,
      "loss": 0.5402,
      "step": 8140
    },
    {
      "epoch": 7.031924072476273,
      "grad_norm": 4.9084153175354,
      "learning_rate": 8.904227782571183e-06,
      "loss": 0.644,
      "step": 8150
    },
    {
      "epoch": 7.040552200172563,
      "grad_norm": 4.357726097106934,
      "learning_rate": 8.878343399482314e-06,
      "loss": 0.5605,
      "step": 8160
    },
    {
      "epoch": 7.049180327868853,
      "grad_norm": 5.242318630218506,
      "learning_rate": 8.852459016393443e-06,
      "loss": 0.7896,
      "step": 8170
    },
    {
      "epoch": 7.057808455565143,
      "grad_norm": 6.968837738037109,
      "learning_rate": 8.826574633304575e-06,
      "loss": 0.9014,
      "step": 8180
    },
    {
      "epoch": 7.066436583261432,
      "grad_norm": 5.8921380043029785,
      "learning_rate": 8.800690250215704e-06,
      "loss": 0.6698,
      "step": 8190
    },
    {
      "epoch": 7.075064710957722,
      "grad_norm": 16.016033172607422,
      "learning_rate": 8.774805867126835e-06,
      "loss": 0.898,
      "step": 8200
    },
    {
      "epoch": 7.083692838654012,
      "grad_norm": 8.162693977355957,
      "learning_rate": 8.748921484037965e-06,
      "loss": 0.8621,
      "step": 8210
    },
    {
      "epoch": 7.092320966350302,
      "grad_norm": 37.505558013916016,
      "learning_rate": 8.723037100949096e-06,
      "loss": 0.7879,
      "step": 8220
    },
    {
      "epoch": 7.100949094046592,
      "grad_norm": 8.861077308654785,
      "learning_rate": 8.697152717860225e-06,
      "loss": 0.7822,
      "step": 8230
    },
    {
      "epoch": 7.109577221742882,
      "grad_norm": 11.456365585327148,
      "learning_rate": 8.671268334771355e-06,
      "loss": 0.6722,
      "step": 8240
    },
    {
      "epoch": 7.1182053494391715,
      "grad_norm": 5.713781833648682,
      "learning_rate": 8.645383951682484e-06,
      "loss": 0.7316,
      "step": 8250
    },
    {
      "epoch": 7.126833477135461,
      "grad_norm": 5.3306779861450195,
      "learning_rate": 8.619499568593615e-06,
      "loss": 0.7627,
      "step": 8260
    },
    {
      "epoch": 7.135461604831751,
      "grad_norm": 8.826750755310059,
      "learning_rate": 8.593615185504745e-06,
      "loss": 0.7564,
      "step": 8270
    },
    {
      "epoch": 7.144089732528041,
      "grad_norm": 3.620530366897583,
      "learning_rate": 8.567730802415874e-06,
      "loss": 0.6841,
      "step": 8280
    },
    {
      "epoch": 7.152717860224332,
      "grad_norm": 7.504561424255371,
      "learning_rate": 8.541846419327006e-06,
      "loss": 0.6739,
      "step": 8290
    },
    {
      "epoch": 7.161345987920622,
      "grad_norm": 3.58125901222229,
      "learning_rate": 8.515962036238135e-06,
      "loss": 0.8564,
      "step": 8300
    },
    {
      "epoch": 7.1699741156169114,
      "grad_norm": 5.5241594314575195,
      "learning_rate": 8.490077653149266e-06,
      "loss": 0.7642,
      "step": 8310
    },
    {
      "epoch": 7.178602243313201,
      "grad_norm": 3.9744136333465576,
      "learning_rate": 8.464193270060396e-06,
      "loss": 0.6637,
      "step": 8320
    },
    {
      "epoch": 7.187230371009491,
      "grad_norm": 5.373923301696777,
      "learning_rate": 8.438308886971527e-06,
      "loss": 0.787,
      "step": 8330
    },
    {
      "epoch": 7.195858498705781,
      "grad_norm": 5.463251113891602,
      "learning_rate": 8.412424503882656e-06,
      "loss": 0.8233,
      "step": 8340
    },
    {
      "epoch": 7.204486626402071,
      "grad_norm": 6.615530967712402,
      "learning_rate": 8.386540120793788e-06,
      "loss": 0.6745,
      "step": 8350
    },
    {
      "epoch": 7.213114754098361,
      "grad_norm": 3.1121764183044434,
      "learning_rate": 8.360655737704917e-06,
      "loss": 0.6646,
      "step": 8360
    },
    {
      "epoch": 7.2217428817946505,
      "grad_norm": 8.52796745300293,
      "learning_rate": 8.334771354616048e-06,
      "loss": 0.8954,
      "step": 8370
    },
    {
      "epoch": 7.23037100949094,
      "grad_norm": 3.6466095447540283,
      "learning_rate": 8.308886971527178e-06,
      "loss": 0.6064,
      "step": 8380
    },
    {
      "epoch": 7.23899913718723,
      "grad_norm": 6.066898822784424,
      "learning_rate": 8.283002588438309e-06,
      "loss": 0.6939,
      "step": 8390
    },
    {
      "epoch": 7.24762726488352,
      "grad_norm": 16.309358596801758,
      "learning_rate": 8.257118205349438e-06,
      "loss": 0.7494,
      "step": 8400
    },
    {
      "epoch": 7.25625539257981,
      "grad_norm": 5.625136852264404,
      "learning_rate": 8.23123382226057e-06,
      "loss": 0.8791,
      "step": 8410
    },
    {
      "epoch": 7.2648835202761,
      "grad_norm": 3.6286394596099854,
      "learning_rate": 8.205349439171699e-06,
      "loss": 0.6955,
      "step": 8420
    },
    {
      "epoch": 7.27351164797239,
      "grad_norm": 3.9620776176452637,
      "learning_rate": 8.17946505608283e-06,
      "loss": 0.8087,
      "step": 8430
    },
    {
      "epoch": 7.28213977566868,
      "grad_norm": 5.352087497711182,
      "learning_rate": 8.15358067299396e-06,
      "loss": 1.0909,
      "step": 8440
    },
    {
      "epoch": 7.29076790336497,
      "grad_norm": 6.691981792449951,
      "learning_rate": 8.12769628990509e-06,
      "loss": 0.8538,
      "step": 8450
    },
    {
      "epoch": 7.29939603106126,
      "grad_norm": 4.6206254959106445,
      "learning_rate": 8.10181190681622e-06,
      "loss": 0.8124,
      "step": 8460
    },
    {
      "epoch": 7.30802415875755,
      "grad_norm": 5.29250955581665,
      "learning_rate": 8.075927523727351e-06,
      "loss": 0.8189,
      "step": 8470
    },
    {
      "epoch": 7.31665228645384,
      "grad_norm": 6.677212715148926,
      "learning_rate": 8.050043140638481e-06,
      "loss": 0.7121,
      "step": 8480
    },
    {
      "epoch": 7.3252804141501295,
      "grad_norm": 5.219426155090332,
      "learning_rate": 8.024158757549612e-06,
      "loss": 0.8184,
      "step": 8490
    },
    {
      "epoch": 7.333908541846419,
      "grad_norm": 6.0038065910339355,
      "learning_rate": 7.998274374460742e-06,
      "loss": 0.763,
      "step": 8500
    },
    {
      "epoch": 7.342536669542709,
      "grad_norm": 6.783365249633789,
      "learning_rate": 7.972389991371873e-06,
      "loss": 0.7484,
      "step": 8510
    },
    {
      "epoch": 7.351164797238999,
      "grad_norm": 3.6686580181121826,
      "learning_rate": 7.946505608283002e-06,
      "loss": 0.687,
      "step": 8520
    },
    {
      "epoch": 7.359792924935289,
      "grad_norm": 76.72689819335938,
      "learning_rate": 7.920621225194133e-06,
      "loss": 0.6776,
      "step": 8530
    },
    {
      "epoch": 7.368421052631579,
      "grad_norm": 6.283735752105713,
      "learning_rate": 7.894736842105263e-06,
      "loss": 0.9168,
      "step": 8540
    },
    {
      "epoch": 7.377049180327869,
      "grad_norm": 16.66443634033203,
      "learning_rate": 7.868852459016394e-06,
      "loss": 0.7808,
      "step": 8550
    },
    {
      "epoch": 7.3856773080241584,
      "grad_norm": 4.210818767547607,
      "learning_rate": 7.842968075927524e-06,
      "loss": 0.6857,
      "step": 8560
    },
    {
      "epoch": 7.394305435720448,
      "grad_norm": 5.3533854484558105,
      "learning_rate": 7.817083692838655e-06,
      "loss": 0.9309,
      "step": 8570
    },
    {
      "epoch": 7.402933563416738,
      "grad_norm": 4.629360198974609,
      "learning_rate": 7.791199309749784e-06,
      "loss": 0.7265,
      "step": 8580
    },
    {
      "epoch": 7.411561691113029,
      "grad_norm": 4.243597984313965,
      "learning_rate": 7.765314926660915e-06,
      "loss": 0.7921,
      "step": 8590
    },
    {
      "epoch": 7.420189818809319,
      "grad_norm": 3.859386444091797,
      "learning_rate": 7.739430543572045e-06,
      "loss": 0.7783,
      "step": 8600
    },
    {
      "epoch": 7.428817946505609,
      "grad_norm": 4.704702854156494,
      "learning_rate": 7.713546160483176e-06,
      "loss": 0.7641,
      "step": 8610
    },
    {
      "epoch": 7.437446074201898,
      "grad_norm": 12.579198837280273,
      "learning_rate": 7.687661777394306e-06,
      "loss": 0.6563,
      "step": 8620
    },
    {
      "epoch": 7.446074201898188,
      "grad_norm": 6.144749641418457,
      "learning_rate": 7.661777394305437e-06,
      "loss": 0.8117,
      "step": 8630
    },
    {
      "epoch": 7.454702329594478,
      "grad_norm": 7.070682525634766,
      "learning_rate": 7.635893011216566e-06,
      "loss": 0.8958,
      "step": 8640
    },
    {
      "epoch": 7.463330457290768,
      "grad_norm": 4.830162525177002,
      "learning_rate": 7.610008628127697e-06,
      "loss": 0.6654,
      "step": 8650
    },
    {
      "epoch": 7.471958584987058,
      "grad_norm": 9.729896545410156,
      "learning_rate": 7.584124245038827e-06,
      "loss": 0.7608,
      "step": 8660
    },
    {
      "epoch": 7.480586712683348,
      "grad_norm": 2.5480663776397705,
      "learning_rate": 7.558239861949958e-06,
      "loss": 0.7814,
      "step": 8670
    },
    {
      "epoch": 7.4892148403796375,
      "grad_norm": 12.240850448608398,
      "learning_rate": 7.5323554788610875e-06,
      "loss": 0.783,
      "step": 8680
    },
    {
      "epoch": 7.497842968075927,
      "grad_norm": 2.974457025527954,
      "learning_rate": 7.506471095772218e-06,
      "loss": 0.8174,
      "step": 8690
    },
    {
      "epoch": 7.506471095772217,
      "grad_norm": 6.005100727081299,
      "learning_rate": 7.480586712683347e-06,
      "loss": 0.6954,
      "step": 8700
    },
    {
      "epoch": 7.515099223468507,
      "grad_norm": 4.116114616394043,
      "learning_rate": 7.454702329594478e-06,
      "loss": 0.7711,
      "step": 8710
    },
    {
      "epoch": 7.523727351164797,
      "grad_norm": 4.667457580566406,
      "learning_rate": 7.428817946505608e-06,
      "loss": 0.7203,
      "step": 8720
    },
    {
      "epoch": 7.532355478861087,
      "grad_norm": 11.433850288391113,
      "learning_rate": 7.402933563416738e-06,
      "loss": 0.6907,
      "step": 8730
    },
    {
      "epoch": 7.540983606557377,
      "grad_norm": 4.282217025756836,
      "learning_rate": 7.377049180327869e-06,
      "loss": 0.6796,
      "step": 8740
    },
    {
      "epoch": 7.549611734253667,
      "grad_norm": 20.6163330078125,
      "learning_rate": 7.351164797238999e-06,
      "loss": 0.7365,
      "step": 8750
    },
    {
      "epoch": 7.558239861949957,
      "grad_norm": 8.673670768737793,
      "learning_rate": 7.325280414150129e-06,
      "loss": 0.703,
      "step": 8760
    },
    {
      "epoch": 7.566867989646247,
      "grad_norm": 5.4167022705078125,
      "learning_rate": 7.29939603106126e-06,
      "loss": 0.7855,
      "step": 8770
    },
    {
      "epoch": 7.575496117342537,
      "grad_norm": 83.13276672363281,
      "learning_rate": 7.27351164797239e-06,
      "loss": 0.781,
      "step": 8780
    },
    {
      "epoch": 7.584124245038827,
      "grad_norm": 4.775599956512451,
      "learning_rate": 7.24762726488352e-06,
      "loss": 0.6198,
      "step": 8790
    },
    {
      "epoch": 7.5927523727351165,
      "grad_norm": 5.238879680633545,
      "learning_rate": 7.2217428817946506e-06,
      "loss": 0.9206,
      "step": 8800
    },
    {
      "epoch": 7.601380500431406,
      "grad_norm": 5.402983665466309,
      "learning_rate": 7.195858498705781e-06,
      "loss": 0.7931,
      "step": 8810
    },
    {
      "epoch": 7.610008628127696,
      "grad_norm": 5.3523759841918945,
      "learning_rate": 7.169974115616911e-06,
      "loss": 0.7713,
      "step": 8820
    },
    {
      "epoch": 7.618636755823986,
      "grad_norm": 6.732028961181641,
      "learning_rate": 7.1440897325280416e-06,
      "loss": 0.8396,
      "step": 8830
    },
    {
      "epoch": 7.627264883520276,
      "grad_norm": 7.0077900886535645,
      "learning_rate": 7.118205349439172e-06,
      "loss": 0.7594,
      "step": 8840
    },
    {
      "epoch": 7.635893011216566,
      "grad_norm": 5.1671037673950195,
      "learning_rate": 7.092320966350302e-06,
      "loss": 0.5668,
      "step": 8850
    },
    {
      "epoch": 7.644521138912856,
      "grad_norm": 7.545320510864258,
      "learning_rate": 7.0664365832614325e-06,
      "loss": 0.7644,
      "step": 8860
    },
    {
      "epoch": 7.653149266609146,
      "grad_norm": 3.6301069259643555,
      "learning_rate": 7.040552200172563e-06,
      "loss": 0.8191,
      "step": 8870
    },
    {
      "epoch": 7.661777394305435,
      "grad_norm": 5.573315620422363,
      "learning_rate": 7.014667817083693e-06,
      "loss": 0.8203,
      "step": 8880
    },
    {
      "epoch": 7.670405522001726,
      "grad_norm": 8.478071212768555,
      "learning_rate": 6.9887834339948235e-06,
      "loss": 0.8294,
      "step": 8890
    },
    {
      "epoch": 7.679033649698016,
      "grad_norm": 13.247632026672363,
      "learning_rate": 6.962899050905954e-06,
      "loss": 0.7748,
      "step": 8900
    },
    {
      "epoch": 7.687661777394306,
      "grad_norm": 5.411312103271484,
      "learning_rate": 6.937014667817084e-06,
      "loss": 0.7462,
      "step": 8910
    },
    {
      "epoch": 7.6962899050905955,
      "grad_norm": 4.391733646392822,
      "learning_rate": 6.9111302847282145e-06,
      "loss": 0.6444,
      "step": 8920
    },
    {
      "epoch": 7.704918032786885,
      "grad_norm": 4.005643367767334,
      "learning_rate": 6.885245901639345e-06,
      "loss": 0.5996,
      "step": 8930
    },
    {
      "epoch": 7.713546160483175,
      "grad_norm": 6.080177307128906,
      "learning_rate": 6.859361518550475e-06,
      "loss": 0.7124,
      "step": 8940
    },
    {
      "epoch": 7.722174288179465,
      "grad_norm": 5.397679328918457,
      "learning_rate": 6.8334771354616055e-06,
      "loss": 0.6585,
      "step": 8950
    },
    {
      "epoch": 7.730802415875755,
      "grad_norm": 14.428597450256348,
      "learning_rate": 6.807592752372736e-06,
      "loss": 0.8889,
      "step": 8960
    },
    {
      "epoch": 7.739430543572045,
      "grad_norm": 6.665552139282227,
      "learning_rate": 6.781708369283866e-06,
      "loss": 0.8803,
      "step": 8970
    },
    {
      "epoch": 7.748058671268335,
      "grad_norm": 2.609198570251465,
      "learning_rate": 6.7558239861949965e-06,
      "loss": 0.6983,
      "step": 8980
    },
    {
      "epoch": 7.756686798964624,
      "grad_norm": 7.250010967254639,
      "learning_rate": 6.729939603106127e-06,
      "loss": 0.8145,
      "step": 8990
    },
    {
      "epoch": 7.765314926660914,
      "grad_norm": 7.645276069641113,
      "learning_rate": 6.704055220017256e-06,
      "loss": 0.8825,
      "step": 9000
    },
    {
      "epoch": 7.773943054357204,
      "grad_norm": 4.8200507164001465,
      "learning_rate": 6.678170836928387e-06,
      "loss": 0.6448,
      "step": 9010
    },
    {
      "epoch": 7.782571182053495,
      "grad_norm": 6.763604164123535,
      "learning_rate": 6.652286453839517e-06,
      "loss": 0.8246,
      "step": 9020
    },
    {
      "epoch": 7.791199309749784,
      "grad_norm": 5.207058906555176,
      "learning_rate": 6.626402070750647e-06,
      "loss": 0.8843,
      "step": 9030
    },
    {
      "epoch": 7.7998274374460745,
      "grad_norm": 13.109515190124512,
      "learning_rate": 6.600517687661778e-06,
      "loss": 0.7828,
      "step": 9040
    },
    {
      "epoch": 7.808455565142364,
      "grad_norm": 6.175139427185059,
      "learning_rate": 6.574633304572908e-06,
      "loss": 0.8806,
      "step": 9050
    },
    {
      "epoch": 7.817083692838654,
      "grad_norm": 70.8798599243164,
      "learning_rate": 6.548748921484038e-06,
      "loss": 0.8076,
      "step": 9060
    },
    {
      "epoch": 7.825711820534944,
      "grad_norm": 8.558797836303711,
      "learning_rate": 6.5228645383951686e-06,
      "loss": 0.9295,
      "step": 9070
    },
    {
      "epoch": 7.834339948231234,
      "grad_norm": 14.566186904907227,
      "learning_rate": 6.496980155306299e-06,
      "loss": 0.6604,
      "step": 9080
    },
    {
      "epoch": 7.842968075927524,
      "grad_norm": 5.74700927734375,
      "learning_rate": 6.471095772217429e-06,
      "loss": 0.6781,
      "step": 9090
    },
    {
      "epoch": 7.851596203623814,
      "grad_norm": 5.546004772186279,
      "learning_rate": 6.4452113891285595e-06,
      "loss": 0.7107,
      "step": 9100
    },
    {
      "epoch": 7.8602243313201035,
      "grad_norm": 3.286975860595703,
      "learning_rate": 6.419327006039689e-06,
      "loss": 0.7229,
      "step": 9110
    },
    {
      "epoch": 7.868852459016393,
      "grad_norm": 7.983513355255127,
      "learning_rate": 6.393442622950819e-06,
      "loss": 0.6902,
      "step": 9120
    },
    {
      "epoch": 7.877480586712683,
      "grad_norm": 13.388969421386719,
      "learning_rate": 6.36755823986195e-06,
      "loss": 0.6422,
      "step": 9130
    },
    {
      "epoch": 7.886108714408973,
      "grad_norm": 6.836667537689209,
      "learning_rate": 6.34167385677308e-06,
      "loss": 0.595,
      "step": 9140
    },
    {
      "epoch": 7.894736842105263,
      "grad_norm": 5.129755973815918,
      "learning_rate": 6.31578947368421e-06,
      "loss": 0.7039,
      "step": 9150
    },
    {
      "epoch": 7.903364969801553,
      "grad_norm": 5.421478748321533,
      "learning_rate": 6.289905090595341e-06,
      "loss": 0.726,
      "step": 9160
    },
    {
      "epoch": 7.911993097497843,
      "grad_norm": 5.609986305236816,
      "learning_rate": 6.264020707506471e-06,
      "loss": 0.8877,
      "step": 9170
    },
    {
      "epoch": 7.920621225194133,
      "grad_norm": 4.845059871673584,
      "learning_rate": 6.238136324417601e-06,
      "loss": 0.7033,
      "step": 9180
    },
    {
      "epoch": 7.929249352890423,
      "grad_norm": 115.10943603515625,
      "learning_rate": 6.212251941328732e-06,
      "loss": 0.8128,
      "step": 9190
    },
    {
      "epoch": 7.937877480586713,
      "grad_norm": 4.024110794067383,
      "learning_rate": 6.186367558239862e-06,
      "loss": 0.6661,
      "step": 9200
    },
    {
      "epoch": 7.946505608283003,
      "grad_norm": 4.359100818634033,
      "learning_rate": 6.160483175150992e-06,
      "loss": 0.6803,
      "step": 9210
    },
    {
      "epoch": 7.955133735979293,
      "grad_norm": 5.4546122550964355,
      "learning_rate": 6.134598792062123e-06,
      "loss": 0.8881,
      "step": 9220
    },
    {
      "epoch": 7.9637618636755825,
      "grad_norm": 12.596433639526367,
      "learning_rate": 6.108714408973253e-06,
      "loss": 0.5421,
      "step": 9230
    },
    {
      "epoch": 7.972389991371872,
      "grad_norm": 6.75309944152832,
      "learning_rate": 6.082830025884383e-06,
      "loss": 0.8172,
      "step": 9240
    },
    {
      "epoch": 7.981018119068162,
      "grad_norm": 3.845322847366333,
      "learning_rate": 6.056945642795514e-06,
      "loss": 0.791,
      "step": 9250
    },
    {
      "epoch": 7.989646246764452,
      "grad_norm": 4.358489513397217,
      "learning_rate": 6.031061259706644e-06,
      "loss": 0.9023,
      "step": 9260
    },
    {
      "epoch": 7.998274374460742,
      "grad_norm": 3.945279836654663,
      "learning_rate": 6.005176876617774e-06,
      "loss": 0.6232,
      "step": 9270
    },
    {
      "epoch": 8.0,
      "eval_loss": 0.4809688627719879,
      "eval_runtime": 2.3825,
      "eval_samples_per_second": 208.186,
      "eval_steps_per_second": 26.023,
      "step": 9272
    },
    {
      "epoch": 8.006902502157033,
      "grad_norm": 14.620058059692383,
      "learning_rate": 5.979292493528905e-06,
      "loss": 0.6918,
      "step": 9280
    },
    {
      "epoch": 8.015530629853322,
      "grad_norm": 6.219055652618408,
      "learning_rate": 5.953408110440035e-06,
      "loss": 0.8539,
      "step": 9290
    },
    {
      "epoch": 8.024158757549612,
      "grad_norm": 8.21485424041748,
      "learning_rate": 5.927523727351165e-06,
      "loss": 0.6419,
      "step": 9300
    },
    {
      "epoch": 8.032786885245901,
      "grad_norm": 4.981006145477295,
      "learning_rate": 5.901639344262295e-06,
      "loss": 0.6198,
      "step": 9310
    },
    {
      "epoch": 8.041415012942192,
      "grad_norm": 4.768369197845459,
      "learning_rate": 5.875754961173425e-06,
      "loss": 0.7031,
      "step": 9320
    },
    {
      "epoch": 8.050043140638481,
      "grad_norm": 4.247888565063477,
      "learning_rate": 5.849870578084555e-06,
      "loss": 0.654,
      "step": 9330
    },
    {
      "epoch": 8.058671268334772,
      "grad_norm": 7.711992263793945,
      "learning_rate": 5.823986194995686e-06,
      "loss": 0.8283,
      "step": 9340
    },
    {
      "epoch": 8.06729939603106,
      "grad_norm": 5.239496231079102,
      "learning_rate": 5.798101811906816e-06,
      "loss": 0.7411,
      "step": 9350
    },
    {
      "epoch": 8.075927523727351,
      "grad_norm": 40.985992431640625,
      "learning_rate": 5.772217428817946e-06,
      "loss": 0.7017,
      "step": 9360
    },
    {
      "epoch": 8.08455565142364,
      "grad_norm": 5.932005405426025,
      "learning_rate": 5.746333045729077e-06,
      "loss": 0.6427,
      "step": 9370
    },
    {
      "epoch": 8.093183779119931,
      "grad_norm": 5.363852024078369,
      "learning_rate": 5.720448662640207e-06,
      "loss": 0.8099,
      "step": 9380
    },
    {
      "epoch": 8.10181190681622,
      "grad_norm": 7.968230247497559,
      "learning_rate": 5.694564279551337e-06,
      "loss": 0.7021,
      "step": 9390
    },
    {
      "epoch": 8.11044003451251,
      "grad_norm": 4.1378960609436035,
      "learning_rate": 5.668679896462468e-06,
      "loss": 0.8948,
      "step": 9400
    },
    {
      "epoch": 8.119068162208801,
      "grad_norm": 9.417959213256836,
      "learning_rate": 5.642795513373598e-06,
      "loss": 0.7929,
      "step": 9410
    },
    {
      "epoch": 8.12769628990509,
      "grad_norm": 3.288618326187134,
      "learning_rate": 5.616911130284728e-06,
      "loss": 0.5779,
      "step": 9420
    },
    {
      "epoch": 8.136324417601381,
      "grad_norm": 85.78570556640625,
      "learning_rate": 5.591026747195859e-06,
      "loss": 0.7818,
      "step": 9430
    },
    {
      "epoch": 8.14495254529767,
      "grad_norm": 5.010623455047607,
      "learning_rate": 5.565142364106989e-06,
      "loss": 0.6896,
      "step": 9440
    },
    {
      "epoch": 8.15358067299396,
      "grad_norm": 4.644404888153076,
      "learning_rate": 5.539257981018119e-06,
      "loss": 0.6222,
      "step": 9450
    },
    {
      "epoch": 8.16220880069025,
      "grad_norm": 4.008598804473877,
      "learning_rate": 5.51337359792925e-06,
      "loss": 0.718,
      "step": 9460
    },
    {
      "epoch": 8.17083692838654,
      "grad_norm": 5.451451301574707,
      "learning_rate": 5.48748921484038e-06,
      "loss": 0.8325,
      "step": 9470
    },
    {
      "epoch": 8.17946505608283,
      "grad_norm": 6.437638282775879,
      "learning_rate": 5.46160483175151e-06,
      "loss": 0.6478,
      "step": 9480
    },
    {
      "epoch": 8.18809318377912,
      "grad_norm": 4.398123741149902,
      "learning_rate": 5.435720448662641e-06,
      "loss": 0.6336,
      "step": 9490
    },
    {
      "epoch": 8.19672131147541,
      "grad_norm": 57.26490783691406,
      "learning_rate": 5.409836065573771e-06,
      "loss": 0.7849,
      "step": 9500
    },
    {
      "epoch": 8.2053494391717,
      "grad_norm": 7.4766106605529785,
      "learning_rate": 5.383951682484901e-06,
      "loss": 0.9048,
      "step": 9510
    },
    {
      "epoch": 8.213977566867989,
      "grad_norm": 4.597006320953369,
      "learning_rate": 5.358067299396032e-06,
      "loss": 0.8714,
      "step": 9520
    },
    {
      "epoch": 8.22260569456428,
      "grad_norm": 11.197072982788086,
      "learning_rate": 5.332182916307162e-06,
      "loss": 0.635,
      "step": 9530
    },
    {
      "epoch": 8.23123382226057,
      "grad_norm": 5.743525505065918,
      "learning_rate": 5.306298533218292e-06,
      "loss": 0.7713,
      "step": 9540
    },
    {
      "epoch": 8.23986194995686,
      "grad_norm": 9.123408317565918,
      "learning_rate": 5.2804141501294226e-06,
      "loss": 0.6528,
      "step": 9550
    },
    {
      "epoch": 8.24849007765315,
      "grad_norm": 10.165141105651855,
      "learning_rate": 5.254529767040553e-06,
      "loss": 0.6898,
      "step": 9560
    },
    {
      "epoch": 8.257118205349439,
      "grad_norm": 10.318982124328613,
      "learning_rate": 5.228645383951683e-06,
      "loss": 0.9153,
      "step": 9570
    },
    {
      "epoch": 8.26574633304573,
      "grad_norm": 14.714646339416504,
      "learning_rate": 5.2027610008628136e-06,
      "loss": 0.7015,
      "step": 9580
    },
    {
      "epoch": 8.274374460742019,
      "grad_norm": 10.500186920166016,
      "learning_rate": 5.176876617773944e-06,
      "loss": 0.763,
      "step": 9590
    },
    {
      "epoch": 8.28300258843831,
      "grad_norm": 8.112595558166504,
      "learning_rate": 5.150992234685074e-06,
      "loss": 0.8367,
      "step": 9600
    },
    {
      "epoch": 8.291630716134598,
      "grad_norm": 4.034565448760986,
      "learning_rate": 5.125107851596204e-06,
      "loss": 0.662,
      "step": 9610
    },
    {
      "epoch": 8.300258843830889,
      "grad_norm": 6.148386001586914,
      "learning_rate": 5.099223468507334e-06,
      "loss": 0.8297,
      "step": 9620
    },
    {
      "epoch": 8.308886971527178,
      "grad_norm": 6.1654181480407715,
      "learning_rate": 5.073339085418464e-06,
      "loss": 0.6794,
      "step": 9630
    },
    {
      "epoch": 8.317515099223469,
      "grad_norm": 4.802417278289795,
      "learning_rate": 5.047454702329595e-06,
      "loss": 0.6897,
      "step": 9640
    },
    {
      "epoch": 8.326143226919758,
      "grad_norm": 6.001910209655762,
      "learning_rate": 5.021570319240725e-06,
      "loss": 0.7328,
      "step": 9650
    },
    {
      "epoch": 8.334771354616048,
      "grad_norm": 6.444402694702148,
      "learning_rate": 4.9956859361518545e-06,
      "loss": 0.7169,
      "step": 9660
    },
    {
      "epoch": 8.343399482312337,
      "grad_norm": 256.3158874511719,
      "learning_rate": 4.969801553062985e-06,
      "loss": 0.8013,
      "step": 9670
    },
    {
      "epoch": 8.352027610008628,
      "grad_norm": 6.627893447875977,
      "learning_rate": 4.943917169974115e-06,
      "loss": 0.6725,
      "step": 9680
    },
    {
      "epoch": 8.360655737704919,
      "grad_norm": 6.519357681274414,
      "learning_rate": 4.9180327868852455e-06,
      "loss": 0.7117,
      "step": 9690
    },
    {
      "epoch": 8.369283865401208,
      "grad_norm": 6.204165935516357,
      "learning_rate": 4.892148403796376e-06,
      "loss": 0.7807,
      "step": 9700
    },
    {
      "epoch": 8.377911993097499,
      "grad_norm": 4.9198079109191895,
      "learning_rate": 4.866264020707506e-06,
      "loss": 0.6483,
      "step": 9710
    },
    {
      "epoch": 8.386540120793788,
      "grad_norm": 4.043808937072754,
      "learning_rate": 4.8403796376186364e-06,
      "loss": 0.7491,
      "step": 9720
    },
    {
      "epoch": 8.395168248490078,
      "grad_norm": 5.478453636169434,
      "learning_rate": 4.814495254529767e-06,
      "loss": 0.8746,
      "step": 9730
    },
    {
      "epoch": 8.403796376186367,
      "grad_norm": 5.216427326202393,
      "learning_rate": 4.788610871440897e-06,
      "loss": 0.6975,
      "step": 9740
    },
    {
      "epoch": 8.412424503882658,
      "grad_norm": 4.245657444000244,
      "learning_rate": 4.762726488352027e-06,
      "loss": 0.664,
      "step": 9750
    },
    {
      "epoch": 8.421052631578947,
      "grad_norm": 4.23910665512085,
      "learning_rate": 4.736842105263158e-06,
      "loss": 0.8863,
      "step": 9760
    },
    {
      "epoch": 8.429680759275238,
      "grad_norm": 10.426054000854492,
      "learning_rate": 4.710957722174288e-06,
      "loss": 0.7453,
      "step": 9770
    },
    {
      "epoch": 8.438308886971527,
      "grad_norm": 6.257410526275635,
      "learning_rate": 4.685073339085418e-06,
      "loss": 0.7507,
      "step": 9780
    },
    {
      "epoch": 8.446937014667817,
      "grad_norm": 4.329375267028809,
      "learning_rate": 4.659188955996549e-06,
      "loss": 0.8196,
      "step": 9790
    },
    {
      "epoch": 8.455565142364106,
      "grad_norm": 4.230408191680908,
      "learning_rate": 4.633304572907679e-06,
      "loss": 0.6848,
      "step": 9800
    },
    {
      "epoch": 8.464193270060397,
      "grad_norm": 5.194533824920654,
      "learning_rate": 4.607420189818809e-06,
      "loss": 0.6253,
      "step": 9810
    },
    {
      "epoch": 8.472821397756686,
      "grad_norm": 4.355194091796875,
      "learning_rate": 4.58153580672994e-06,
      "loss": 0.5892,
      "step": 9820
    },
    {
      "epoch": 8.481449525452977,
      "grad_norm": 16.65220832824707,
      "learning_rate": 4.55565142364107e-06,
      "loss": 0.7311,
      "step": 9830
    },
    {
      "epoch": 8.490077653149267,
      "grad_norm": 4.827352046966553,
      "learning_rate": 4.5297670405522e-06,
      "loss": 0.6952,
      "step": 9840
    },
    {
      "epoch": 8.498705780845556,
      "grad_norm": 4.904386520385742,
      "learning_rate": 4.503882657463331e-06,
      "loss": 0.7131,
      "step": 9850
    },
    {
      "epoch": 8.507333908541847,
      "grad_norm": 6.814375400543213,
      "learning_rate": 4.477998274374461e-06,
      "loss": 0.7408,
      "step": 9860
    },
    {
      "epoch": 8.515962036238136,
      "grad_norm": 6.603330135345459,
      "learning_rate": 4.452113891285591e-06,
      "loss": 0.8729,
      "step": 9870
    },
    {
      "epoch": 8.524590163934427,
      "grad_norm": 4.076086521148682,
      "learning_rate": 4.426229508196722e-06,
      "loss": 0.6629,
      "step": 9880
    },
    {
      "epoch": 8.533218291630716,
      "grad_norm": 6.0083723068237305,
      "learning_rate": 4.400345125107852e-06,
      "loss": 0.9092,
      "step": 9890
    },
    {
      "epoch": 8.541846419327007,
      "grad_norm": 5.372541904449463,
      "learning_rate": 4.374460742018982e-06,
      "loss": 0.8135,
      "step": 9900
    },
    {
      "epoch": 8.550474547023295,
      "grad_norm": 40.64277648925781,
      "learning_rate": 4.348576358930113e-06,
      "loss": 0.7078,
      "step": 9910
    },
    {
      "epoch": 8.559102674719586,
      "grad_norm": 9.03693962097168,
      "learning_rate": 4.322691975841242e-06,
      "loss": 0.7613,
      "step": 9920
    },
    {
      "epoch": 8.567730802415875,
      "grad_norm": 59.17148208618164,
      "learning_rate": 4.2968075927523725e-06,
      "loss": 0.7282,
      "step": 9930
    },
    {
      "epoch": 8.576358930112166,
      "grad_norm": 25.04673957824707,
      "learning_rate": 4.270923209663503e-06,
      "loss": 0.8072,
      "step": 9940
    },
    {
      "epoch": 8.584987057808455,
      "grad_norm": 6.196641445159912,
      "learning_rate": 4.245038826574633e-06,
      "loss": 0.763,
      "step": 9950
    },
    {
      "epoch": 8.593615185504746,
      "grad_norm": 6.584428310394287,
      "learning_rate": 4.2191544434857634e-06,
      "loss": 0.8393,
      "step": 9960
    },
    {
      "epoch": 8.602243313201036,
      "grad_norm": 4.750526428222656,
      "learning_rate": 4.193270060396894e-06,
      "loss": 0.5658,
      "step": 9970
    },
    {
      "epoch": 8.610871440897325,
      "grad_norm": 6.236568450927734,
      "learning_rate": 4.167385677308024e-06,
      "loss": 0.7479,
      "step": 9980
    },
    {
      "epoch": 8.619499568593616,
      "grad_norm": 3.6876988410949707,
      "learning_rate": 4.1415012942191544e-06,
      "loss": 0.5501,
      "step": 9990
    },
    {
      "epoch": 8.628127696289905,
      "grad_norm": 6.972087860107422,
      "learning_rate": 4.115616911130285e-06,
      "loss": 0.8853,
      "step": 10000
    },
    {
      "epoch": 8.636755823986196,
      "grad_norm": 4.75618839263916,
      "learning_rate": 4.089732528041415e-06,
      "loss": 0.7309,
      "step": 10010
    },
    {
      "epoch": 8.645383951682485,
      "grad_norm": 3.766415596008301,
      "learning_rate": 4.063848144952545e-06,
      "loss": 0.6979,
      "step": 10020
    },
    {
      "epoch": 8.654012079378775,
      "grad_norm": 4.96164083480835,
      "learning_rate": 4.037963761863676e-06,
      "loss": 0.7271,
      "step": 10030
    },
    {
      "epoch": 8.662640207075064,
      "grad_norm": 4.205136775970459,
      "learning_rate": 4.012079378774806e-06,
      "loss": 0.8505,
      "step": 10040
    },
    {
      "epoch": 8.671268334771355,
      "grad_norm": 3.2185773849487305,
      "learning_rate": 3.986194995685936e-06,
      "loss": 0.8013,
      "step": 10050
    },
    {
      "epoch": 8.679896462467644,
      "grad_norm": 3.9162566661834717,
      "learning_rate": 3.960310612597067e-06,
      "loss": 0.7709,
      "step": 10060
    },
    {
      "epoch": 8.688524590163935,
      "grad_norm": 10.55040454864502,
      "learning_rate": 3.934426229508197e-06,
      "loss": 0.6855,
      "step": 10070
    },
    {
      "epoch": 8.697152717860224,
      "grad_norm": 43.55988693237305,
      "learning_rate": 3.908541846419327e-06,
      "loss": 0.6048,
      "step": 10080
    },
    {
      "epoch": 8.705780845556514,
      "grad_norm": 5.831573963165283,
      "learning_rate": 3.882657463330458e-06,
      "loss": 0.8296,
      "step": 10090
    },
    {
      "epoch": 8.714408973252803,
      "grad_norm": 4.620630264282227,
      "learning_rate": 3.856773080241588e-06,
      "loss": 0.7078,
      "step": 10100
    },
    {
      "epoch": 8.723037100949094,
      "grad_norm": 25.479066848754883,
      "learning_rate": 3.830888697152718e-06,
      "loss": 0.838,
      "step": 10110
    },
    {
      "epoch": 8.731665228645383,
      "grad_norm": 4.049366474151611,
      "learning_rate": 3.8050043140638487e-06,
      "loss": 0.6753,
      "step": 10120
    },
    {
      "epoch": 8.740293356341674,
      "grad_norm": 19.673669815063477,
      "learning_rate": 3.779119930974979e-06,
      "loss": 0.6643,
      "step": 10130
    },
    {
      "epoch": 8.748921484037965,
      "grad_norm": 8.21699333190918,
      "learning_rate": 3.753235547886109e-06,
      "loss": 0.683,
      "step": 10140
    },
    {
      "epoch": 8.757549611734254,
      "grad_norm": 7.334720134735107,
      "learning_rate": 3.727351164797239e-06,
      "loss": 0.8192,
      "step": 10150
    },
    {
      "epoch": 8.766177739430544,
      "grad_norm": 5.499689102172852,
      "learning_rate": 3.701466781708369e-06,
      "loss": 0.6293,
      "step": 10160
    },
    {
      "epoch": 8.774805867126833,
      "grad_norm": 28.219635009765625,
      "learning_rate": 3.6755823986194995e-06,
      "loss": 0.7431,
      "step": 10170
    },
    {
      "epoch": 8.783433994823124,
      "grad_norm": 3.8970091342926025,
      "learning_rate": 3.64969801553063e-06,
      "loss": 0.6904,
      "step": 10180
    },
    {
      "epoch": 8.792062122519413,
      "grad_norm": 5.093032360076904,
      "learning_rate": 3.62381363244176e-06,
      "loss": 0.742,
      "step": 10190
    },
    {
      "epoch": 8.800690250215704,
      "grad_norm": 9.620489120483398,
      "learning_rate": 3.5979292493528904e-06,
      "loss": 0.8716,
      "step": 10200
    },
    {
      "epoch": 8.809318377911993,
      "grad_norm": 3.820004463195801,
      "learning_rate": 3.5720448662640208e-06,
      "loss": 0.6118,
      "step": 10210
    },
    {
      "epoch": 8.817946505608283,
      "grad_norm": 6.995700836181641,
      "learning_rate": 3.546160483175151e-06,
      "loss": 0.8556,
      "step": 10220
    },
    {
      "epoch": 8.826574633304572,
      "grad_norm": 67.39883422851562,
      "learning_rate": 3.5202761000862814e-06,
      "loss": 0.7218,
      "step": 10230
    },
    {
      "epoch": 8.835202761000863,
      "grad_norm": 15.738824844360352,
      "learning_rate": 3.4943917169974118e-06,
      "loss": 0.7227,
      "step": 10240
    },
    {
      "epoch": 8.843830888697152,
      "grad_norm": 4.277131080627441,
      "learning_rate": 3.468507333908542e-06,
      "loss": 0.55,
      "step": 10250
    },
    {
      "epoch": 8.852459016393443,
      "grad_norm": 7.321753025054932,
      "learning_rate": 3.4426229508196724e-06,
      "loss": 0.7084,
      "step": 10260
    },
    {
      "epoch": 8.861087144089733,
      "grad_norm": 3.5490059852600098,
      "learning_rate": 3.4167385677308027e-06,
      "loss": 0.7172,
      "step": 10270
    },
    {
      "epoch": 8.869715271786022,
      "grad_norm": 4.8776421546936035,
      "learning_rate": 3.390854184641933e-06,
      "loss": 0.6992,
      "step": 10280
    },
    {
      "epoch": 8.878343399482313,
      "grad_norm": 4.072756767272949,
      "learning_rate": 3.3649698015530634e-06,
      "loss": 0.71,
      "step": 10290
    },
    {
      "epoch": 8.886971527178602,
      "grad_norm": 3.880828619003296,
      "learning_rate": 3.3390854184641933e-06,
      "loss": 0.7001,
      "step": 10300
    },
    {
      "epoch": 8.895599654874893,
      "grad_norm": 10.180113792419434,
      "learning_rate": 3.3132010353753236e-06,
      "loss": 0.7469,
      "step": 10310
    },
    {
      "epoch": 8.904227782571182,
      "grad_norm": 17.9007625579834,
      "learning_rate": 3.287316652286454e-06,
      "loss": 0.908,
      "step": 10320
    },
    {
      "epoch": 8.912855910267472,
      "grad_norm": 4.493538856506348,
      "learning_rate": 3.2614322691975843e-06,
      "loss": 0.7409,
      "step": 10330
    },
    {
      "epoch": 8.921484037963761,
      "grad_norm": 4.965084075927734,
      "learning_rate": 3.2355478861087146e-06,
      "loss": 0.7961,
      "step": 10340
    },
    {
      "epoch": 8.930112165660052,
      "grad_norm": 4.713344573974609,
      "learning_rate": 3.2096635030198445e-06,
      "loss": 0.723,
      "step": 10350
    },
    {
      "epoch": 8.938740293356341,
      "grad_norm": 3.997636318206787,
      "learning_rate": 3.183779119930975e-06,
      "loss": 0.8002,
      "step": 10360
    },
    {
      "epoch": 8.947368421052632,
      "grad_norm": 14.25683879852295,
      "learning_rate": 3.157894736842105e-06,
      "loss": 0.5538,
      "step": 10370
    },
    {
      "epoch": 8.95599654874892,
      "grad_norm": 3.449291229248047,
      "learning_rate": 3.1320103537532355e-06,
      "loss": 0.7759,
      "step": 10380
    },
    {
      "epoch": 8.964624676445212,
      "grad_norm": 5.378996849060059,
      "learning_rate": 3.106125970664366e-06,
      "loss": 0.8056,
      "step": 10390
    },
    {
      "epoch": 8.9732528041415,
      "grad_norm": 26.78990936279297,
      "learning_rate": 3.080241587575496e-06,
      "loss": 0.8165,
      "step": 10400
    },
    {
      "epoch": 8.981880931837791,
      "grad_norm": 10.195514678955078,
      "learning_rate": 3.0543572044866265e-06,
      "loss": 0.7454,
      "step": 10410
    },
    {
      "epoch": 8.99050905953408,
      "grad_norm": 4.519870281219482,
      "learning_rate": 3.028472821397757e-06,
      "loss": 0.6671,
      "step": 10420
    },
    {
      "epoch": 8.999137187230371,
      "grad_norm": 4.253199577331543,
      "learning_rate": 3.002588438308887e-06,
      "loss": 0.7149,
      "step": 10430
    },
    {
      "epoch": 9.0,
      "eval_loss": 0.4906157851219177,
      "eval_runtime": 2.4062,
      "eval_samples_per_second": 206.13,
      "eval_steps_per_second": 25.766,
      "step": 10431
    },
    {
      "epoch": 9.007765314926662,
      "grad_norm": 5.601800441741943,
      "learning_rate": 2.9767040552200175e-06,
      "loss": 0.6557,
      "step": 10440
    },
    {
      "epoch": 9.01639344262295,
      "grad_norm": 10.802886009216309,
      "learning_rate": 2.9508196721311474e-06,
      "loss": 0.6202,
      "step": 10450
    },
    {
      "epoch": 9.025021570319241,
      "grad_norm": 4.633174419403076,
      "learning_rate": 2.9249352890422777e-06,
      "loss": 0.801,
      "step": 10460
    },
    {
      "epoch": 9.03364969801553,
      "grad_norm": 3.7514946460723877,
      "learning_rate": 2.899050905953408e-06,
      "loss": 0.5345,
      "step": 10470
    },
    {
      "epoch": 9.042277825711821,
      "grad_norm": 88.50215911865234,
      "learning_rate": 2.8731665228645383e-06,
      "loss": 0.8113,
      "step": 10480
    },
    {
      "epoch": 9.05090595340811,
      "grad_norm": 312.89044189453125,
      "learning_rate": 2.8472821397756687e-06,
      "loss": 0.6851,
      "step": 10490
    },
    {
      "epoch": 9.0595340811044,
      "grad_norm": 7.737201690673828,
      "learning_rate": 2.821397756686799e-06,
      "loss": 0.7789,
      "step": 10500
    },
    {
      "epoch": 9.06816220880069,
      "grad_norm": 5.450870037078857,
      "learning_rate": 2.7955133735979293e-06,
      "loss": 0.7009,
      "step": 10510
    },
    {
      "epoch": 9.07679033649698,
      "grad_norm": 6.367637634277344,
      "learning_rate": 2.7696289905090596e-06,
      "loss": 0.5938,
      "step": 10520
    },
    {
      "epoch": 9.08541846419327,
      "grad_norm": 1406.65673828125,
      "learning_rate": 2.74374460742019e-06,
      "loss": 0.7198,
      "step": 10530
    },
    {
      "epoch": 9.09404659188956,
      "grad_norm": 5.015007972717285,
      "learning_rate": 2.7178602243313203e-06,
      "loss": 0.7038,
      "step": 10540
    },
    {
      "epoch": 9.102674719585849,
      "grad_norm": 4.922833442687988,
      "learning_rate": 2.6919758412424506e-06,
      "loss": 0.5847,
      "step": 10550
    },
    {
      "epoch": 9.11130284728214,
      "grad_norm": 5.092758655548096,
      "learning_rate": 2.666091458153581e-06,
      "loss": 0.7543,
      "step": 10560
    },
    {
      "epoch": 9.11993097497843,
      "grad_norm": 23.6734561920166,
      "learning_rate": 2.6402070750647113e-06,
      "loss": 0.785,
      "step": 10570
    },
    {
      "epoch": 9.12855910267472,
      "grad_norm": 5.918617248535156,
      "learning_rate": 2.6143226919758416e-06,
      "loss": 0.8571,
      "step": 10580
    },
    {
      "epoch": 9.13718723037101,
      "grad_norm": 6.892045021057129,
      "learning_rate": 2.588438308886972e-06,
      "loss": 0.7398,
      "step": 10590
    },
    {
      "epoch": 9.1458153580673,
      "grad_norm": 5.49359655380249,
      "learning_rate": 2.562553925798102e-06,
      "loss": 0.8627,
      "step": 10600
    },
    {
      "epoch": 9.15444348576359,
      "grad_norm": 3.7445292472839355,
      "learning_rate": 2.536669542709232e-06,
      "loss": 0.6307,
      "step": 10610
    },
    {
      "epoch": 9.163071613459879,
      "grad_norm": 4.316560745239258,
      "learning_rate": 2.5107851596203625e-06,
      "loss": 0.7194,
      "step": 10620
    },
    {
      "epoch": 9.17169974115617,
      "grad_norm": 21.370960235595703,
      "learning_rate": 2.4849007765314924e-06,
      "loss": 0.7232,
      "step": 10630
    },
    {
      "epoch": 9.180327868852459,
      "grad_norm": 11.940284729003906,
      "learning_rate": 2.4590163934426227e-06,
      "loss": 0.8681,
      "step": 10640
    },
    {
      "epoch": 9.18895599654875,
      "grad_norm": 27.628873825073242,
      "learning_rate": 2.433132010353753e-06,
      "loss": 0.8698,
      "step": 10650
    },
    {
      "epoch": 9.197584124245038,
      "grad_norm": 4.611938953399658,
      "learning_rate": 2.4072476272648834e-06,
      "loss": 0.7279,
      "step": 10660
    },
    {
      "epoch": 9.206212251941329,
      "grad_norm": 5.239058971405029,
      "learning_rate": 2.3813632441760137e-06,
      "loss": 0.7195,
      "step": 10670
    },
    {
      "epoch": 9.214840379637618,
      "grad_norm": 5.942753314971924,
      "learning_rate": 2.355478861087144e-06,
      "loss": 0.671,
      "step": 10680
    },
    {
      "epoch": 9.223468507333909,
      "grad_norm": 3.9936249256134033,
      "learning_rate": 2.3295944779982744e-06,
      "loss": 0.7712,
      "step": 10690
    },
    {
      "epoch": 9.232096635030198,
      "grad_norm": 8.59660816192627,
      "learning_rate": 2.3037100949094047e-06,
      "loss": 0.7945,
      "step": 10700
    },
    {
      "epoch": 9.240724762726488,
      "grad_norm": 8.047161102294922,
      "learning_rate": 2.277825711820535e-06,
      "loss": 0.841,
      "step": 10710
    },
    {
      "epoch": 9.249352890422779,
      "grad_norm": 4.786504745483398,
      "learning_rate": 2.2519413287316653e-06,
      "loss": 0.6519,
      "step": 10720
    },
    {
      "epoch": 9.257981018119068,
      "grad_norm": 8.487805366516113,
      "learning_rate": 2.2260569456427957e-06,
      "loss": 0.7171,
      "step": 10730
    },
    {
      "epoch": 9.266609145815359,
      "grad_norm": 3.3719699382781982,
      "learning_rate": 2.200172562553926e-06,
      "loss": 0.7977,
      "step": 10740
    },
    {
      "epoch": 9.275237273511648,
      "grad_norm": 7.838946342468262,
      "learning_rate": 2.1742881794650563e-06,
      "loss": 0.6936,
      "step": 10750
    },
    {
      "epoch": 9.283865401207938,
      "grad_norm": 2.9516522884368896,
      "learning_rate": 2.1484037963761862e-06,
      "loss": 0.7264,
      "step": 10760
    },
    {
      "epoch": 9.292493528904227,
      "grad_norm": 3.965463161468506,
      "learning_rate": 2.1225194132873166e-06,
      "loss": 0.6571,
      "step": 10770
    },
    {
      "epoch": 9.301121656600518,
      "grad_norm": 13.580305099487305,
      "learning_rate": 2.096635030198447e-06,
      "loss": 0.6233,
      "step": 10780
    },
    {
      "epoch": 9.309749784296807,
      "grad_norm": 3.6906301975250244,
      "learning_rate": 2.0707506471095772e-06,
      "loss": 0.7013,
      "step": 10790
    },
    {
      "epoch": 9.318377911993098,
      "grad_norm": 3.109433174133301,
      "learning_rate": 2.0448662640207075e-06,
      "loss": 0.6622,
      "step": 10800
    },
    {
      "epoch": 9.327006039689387,
      "grad_norm": 5.2991814613342285,
      "learning_rate": 2.018981880931838e-06,
      "loss": 0.6838,
      "step": 10810
    },
    {
      "epoch": 9.335634167385678,
      "grad_norm": 5.457651138305664,
      "learning_rate": 1.993097497842968e-06,
      "loss": 0.725,
      "step": 10820
    },
    {
      "epoch": 9.344262295081966,
      "grad_norm": 4.299841403961182,
      "learning_rate": 1.9672131147540985e-06,
      "loss": 0.7677,
      "step": 10830
    },
    {
      "epoch": 9.352890422778257,
      "grad_norm": 3.980337619781494,
      "learning_rate": 1.941328731665229e-06,
      "loss": 0.8088,
      "step": 10840
    },
    {
      "epoch": 9.361518550474546,
      "grad_norm": 4.3625807762146,
      "learning_rate": 1.915444348576359e-06,
      "loss": 0.7362,
      "step": 10850
    },
    {
      "epoch": 9.370146678170837,
      "grad_norm": 9.055144309997559,
      "learning_rate": 1.8895599654874895e-06,
      "loss": 0.704,
      "step": 10860
    },
    {
      "epoch": 9.378774805867128,
      "grad_norm": 5.093209266662598,
      "learning_rate": 1.8636755823986194e-06,
      "loss": 0.9471,
      "step": 10870
    },
    {
      "epoch": 9.387402933563417,
      "grad_norm": 7.333690643310547,
      "learning_rate": 1.8377911993097497e-06,
      "loss": 0.5905,
      "step": 10880
    },
    {
      "epoch": 9.396031061259707,
      "grad_norm": 5.6162004470825195,
      "learning_rate": 1.81190681622088e-06,
      "loss": 0.7364,
      "step": 10890
    },
    {
      "epoch": 9.404659188955996,
      "grad_norm": 4.875744819641113,
      "learning_rate": 1.7860224331320104e-06,
      "loss": 0.8009,
      "step": 10900
    },
    {
      "epoch": 9.413287316652287,
      "grad_norm": 4.816036701202393,
      "learning_rate": 1.7601380500431407e-06,
      "loss": 0.6537,
      "step": 10910
    },
    {
      "epoch": 9.421915444348576,
      "grad_norm": 6.125532627105713,
      "learning_rate": 1.734253666954271e-06,
      "loss": 0.6067,
      "step": 10920
    },
    {
      "epoch": 9.430543572044867,
      "grad_norm": 5.2592926025390625,
      "learning_rate": 1.7083692838654014e-06,
      "loss": 0.7439,
      "step": 10930
    },
    {
      "epoch": 9.439171699741156,
      "grad_norm": 8.11108684539795,
      "learning_rate": 1.6824849007765317e-06,
      "loss": 0.5872,
      "step": 10940
    },
    {
      "epoch": 9.447799827437446,
      "grad_norm": 30.658729553222656,
      "learning_rate": 1.6566005176876618e-06,
      "loss": 0.6766,
      "step": 10950
    },
    {
      "epoch": 9.456427955133735,
      "grad_norm": 4.8822503089904785,
      "learning_rate": 1.6307161345987921e-06,
      "loss": 0.8048,
      "step": 10960
    },
    {
      "epoch": 9.465056082830026,
      "grad_norm": 4.846551895141602,
      "learning_rate": 1.6048317515099223e-06,
      "loss": 0.7617,
      "step": 10970
    },
    {
      "epoch": 9.473684210526315,
      "grad_norm": 5.5252203941345215,
      "learning_rate": 1.5789473684210526e-06,
      "loss": 0.7963,
      "step": 10980
    },
    {
      "epoch": 9.482312338222606,
      "grad_norm": 3.7597970962524414,
      "learning_rate": 1.553062985332183e-06,
      "loss": 0.7835,
      "step": 10990
    },
    {
      "epoch": 9.490940465918897,
      "grad_norm": 5.7541985511779785,
      "learning_rate": 1.5271786022433132e-06,
      "loss": 0.8939,
      "step": 11000
    },
    {
      "epoch": 9.499568593615185,
      "grad_norm": 3.3889060020446777,
      "learning_rate": 1.5012942191544436e-06,
      "loss": 0.7474,
      "step": 11010
    },
    {
      "epoch": 9.508196721311476,
      "grad_norm": 6.543641090393066,
      "learning_rate": 1.4754098360655737e-06,
      "loss": 0.7897,
      "step": 11020
    },
    {
      "epoch": 9.516824849007765,
      "grad_norm": 3.8367443084716797,
      "learning_rate": 1.449525452976704e-06,
      "loss": 0.6328,
      "step": 11030
    },
    {
      "epoch": 9.525452976704056,
      "grad_norm": 6.42110013961792,
      "learning_rate": 1.4236410698878343e-06,
      "loss": 0.8273,
      "step": 11040
    },
    {
      "epoch": 9.534081104400345,
      "grad_norm": 5.072246551513672,
      "learning_rate": 1.3977566867989647e-06,
      "loss": 0.648,
      "step": 11050
    },
    {
      "epoch": 9.542709232096636,
      "grad_norm": 7.582367420196533,
      "learning_rate": 1.371872303710095e-06,
      "loss": 0.8215,
      "step": 11060
    },
    {
      "epoch": 9.551337359792925,
      "grad_norm": 4.653839111328125,
      "learning_rate": 1.3459879206212253e-06,
      "loss": 0.7358,
      "step": 11070
    },
    {
      "epoch": 9.559965487489215,
      "grad_norm": 10.850563049316406,
      "learning_rate": 1.3201035375323556e-06,
      "loss": 0.6644,
      "step": 11080
    },
    {
      "epoch": 9.568593615185504,
      "grad_norm": 2.5685057640075684,
      "learning_rate": 1.294219154443486e-06,
      "loss": 0.717,
      "step": 11090
    },
    {
      "epoch": 9.577221742881795,
      "grad_norm": 5.156591892242432,
      "learning_rate": 1.268334771354616e-06,
      "loss": 0.8414,
      "step": 11100
    },
    {
      "epoch": 9.585849870578084,
      "grad_norm": 5.50663423538208,
      "learning_rate": 1.2424503882657462e-06,
      "loss": 0.6853,
      "step": 11110
    },
    {
      "epoch": 9.594477998274375,
      "grad_norm": 6.55145263671875,
      "learning_rate": 1.2165660051768765e-06,
      "loss": 0.6675,
      "step": 11120
    },
    {
      "epoch": 9.603106125970664,
      "grad_norm": 7.329158306121826,
      "learning_rate": 1.1906816220880069e-06,
      "loss": 0.727,
      "step": 11130
    },
    {
      "epoch": 9.611734253666954,
      "grad_norm": 7.98805570602417,
      "learning_rate": 1.1647972389991372e-06,
      "loss": 0.6823,
      "step": 11140
    },
    {
      "epoch": 9.620362381363243,
      "grad_norm": 6.074347019195557,
      "learning_rate": 1.1389128559102675e-06,
      "loss": 0.7561,
      "step": 11150
    },
    {
      "epoch": 9.628990509059534,
      "grad_norm": 5.414069175720215,
      "learning_rate": 1.1130284728213978e-06,
      "loss": 0.8158,
      "step": 11160
    },
    {
      "epoch": 9.637618636755825,
      "grad_norm": 6.694360256195068,
      "learning_rate": 1.0871440897325282e-06,
      "loss": 0.7339,
      "step": 11170
    },
    {
      "epoch": 9.646246764452114,
      "grad_norm": 2.4755609035491943,
      "learning_rate": 1.0612597066436583e-06,
      "loss": 0.8641,
      "step": 11180
    },
    {
      "epoch": 9.654874892148404,
      "grad_norm": 4.353865146636963,
      "learning_rate": 1.0353753235547886e-06,
      "loss": 0.6922,
      "step": 11190
    },
    {
      "epoch": 9.663503019844693,
      "grad_norm": 4.618558406829834,
      "learning_rate": 1.009490940465919e-06,
      "loss": 0.8614,
      "step": 11200
    },
    {
      "epoch": 9.672131147540984,
      "grad_norm": 4.7899603843688965,
      "learning_rate": 9.836065573770493e-07,
      "loss": 0.7324,
      "step": 11210
    },
    {
      "epoch": 9.680759275237273,
      "grad_norm": 4.375484943389893,
      "learning_rate": 9.577221742881796e-07,
      "loss": 0.716,
      "step": 11220
    },
    {
      "epoch": 9.689387402933564,
      "grad_norm": 6.541473865509033,
      "learning_rate": 9.318377911993097e-07,
      "loss": 0.656,
      "step": 11230
    },
    {
      "epoch": 9.698015530629853,
      "grad_norm": 7.850377082824707,
      "learning_rate": 9.0595340811044e-07,
      "loss": 0.6242,
      "step": 11240
    },
    {
      "epoch": 9.706643658326144,
      "grad_norm": 37.99641036987305,
      "learning_rate": 8.800690250215704e-07,
      "loss": 0.7345,
      "step": 11250
    },
    {
      "epoch": 9.715271786022432,
      "grad_norm": 4.8309712409973145,
      "learning_rate": 8.541846419327007e-07,
      "loss": 0.7675,
      "step": 11260
    },
    {
      "epoch": 9.723899913718723,
      "grad_norm": 3.4119441509246826,
      "learning_rate": 8.283002588438309e-07,
      "loss": 0.5919,
      "step": 11270
    },
    {
      "epoch": 9.732528041415012,
      "grad_norm": 6.972571849822998,
      "learning_rate": 8.024158757549611e-07,
      "loss": 0.7667,
      "step": 11280
    },
    {
      "epoch": 9.741156169111303,
      "grad_norm": 4.658843517303467,
      "learning_rate": 7.765314926660915e-07,
      "loss": 0.7322,
      "step": 11290
    },
    {
      "epoch": 9.749784296807594,
      "grad_norm": 42.65409469604492,
      "learning_rate": 7.506471095772218e-07,
      "loss": 0.6134,
      "step": 11300
    },
    {
      "epoch": 9.758412424503883,
      "grad_norm": 23.425935745239258,
      "learning_rate": 7.24762726488352e-07,
      "loss": 0.604,
      "step": 11310
    },
    {
      "epoch": 9.767040552200173,
      "grad_norm": 4.275119304656982,
      "learning_rate": 6.988783433994823e-07,
      "loss": 0.692,
      "step": 11320
    },
    {
      "epoch": 9.775668679896462,
      "grad_norm": 6.952493667602539,
      "learning_rate": 6.729939603106127e-07,
      "loss": 0.6886,
      "step": 11330
    },
    {
      "epoch": 9.784296807592753,
      "grad_norm": 6.106539726257324,
      "learning_rate": 6.47109577221743e-07,
      "loss": 0.7784,
      "step": 11340
    },
    {
      "epoch": 9.792924935289042,
      "grad_norm": 13.747517585754395,
      "learning_rate": 6.212251941328731e-07,
      "loss": 0.7828,
      "step": 11350
    },
    {
      "epoch": 9.801553062985333,
      "grad_norm": 5.882225513458252,
      "learning_rate": 5.953408110440034e-07,
      "loss": 0.6168,
      "step": 11360
    },
    {
      "epoch": 9.810181190681622,
      "grad_norm": 4.990443706512451,
      "learning_rate": 5.694564279551338e-07,
      "loss": 0.8603,
      "step": 11370
    },
    {
      "epoch": 9.818809318377912,
      "grad_norm": 3.807476282119751,
      "learning_rate": 5.435720448662641e-07,
      "loss": 0.742,
      "step": 11380
    },
    {
      "epoch": 9.827437446074201,
      "grad_norm": 9.940132141113281,
      "learning_rate": 5.176876617773943e-07,
      "loss": 0.6449,
      "step": 11390
    },
    {
      "epoch": 9.836065573770492,
      "grad_norm": 3.163088083267212,
      "learning_rate": 4.918032786885246e-07,
      "loss": 0.7216,
      "step": 11400
    },
    {
      "epoch": 9.844693701466781,
      "grad_norm": 6.642813205718994,
      "learning_rate": 4.6591889559965485e-07,
      "loss": 0.6581,
      "step": 11410
    },
    {
      "epoch": 9.853321829163072,
      "grad_norm": 7.1987528800964355,
      "learning_rate": 4.400345125107852e-07,
      "loss": 0.6855,
      "step": 11420
    },
    {
      "epoch": 9.86194995685936,
      "grad_norm": 14.200976371765137,
      "learning_rate": 4.1415012942191545e-07,
      "loss": 0.7527,
      "step": 11430
    },
    {
      "epoch": 9.870578084555651,
      "grad_norm": 7.648218631744385,
      "learning_rate": 3.8826574633304573e-07,
      "loss": 0.7155,
      "step": 11440
    },
    {
      "epoch": 9.87920621225194,
      "grad_norm": 4.911346435546875,
      "learning_rate": 3.62381363244176e-07,
      "loss": 0.5227,
      "step": 11450
    },
    {
      "epoch": 9.887834339948231,
      "grad_norm": 6.593098163604736,
      "learning_rate": 3.3649698015530633e-07,
      "loss": 0.8243,
      "step": 11460
    },
    {
      "epoch": 9.896462467644522,
      "grad_norm": 19.72844123840332,
      "learning_rate": 3.1061259706643655e-07,
      "loss": 0.6718,
      "step": 11470
    },
    {
      "epoch": 9.90509059534081,
      "grad_norm": 5.458126544952393,
      "learning_rate": 2.847282139775669e-07,
      "loss": 0.7813,
      "step": 11480
    },
    {
      "epoch": 9.913718723037102,
      "grad_norm": 4.731016159057617,
      "learning_rate": 2.5884383088869715e-07,
      "loss": 0.7102,
      "step": 11490
    },
    {
      "epoch": 9.92234685073339,
      "grad_norm": 5.928655624389648,
      "learning_rate": 2.3295944779982743e-07,
      "loss": 0.7926,
      "step": 11500
    },
    {
      "epoch": 9.930974978429681,
      "grad_norm": 3.641859769821167,
      "learning_rate": 2.0707506471095773e-07,
      "loss": 0.7094,
      "step": 11510
    },
    {
      "epoch": 9.93960310612597,
      "grad_norm": 5.222039222717285,
      "learning_rate": 1.81190681622088e-07,
      "loss": 0.774,
      "step": 11520
    },
    {
      "epoch": 9.948231233822261,
      "grad_norm": 7.284361362457275,
      "learning_rate": 1.5530629853321828e-07,
      "loss": 0.7965,
      "step": 11530
    },
    {
      "epoch": 9.95685936151855,
      "grad_norm": 16.32720184326172,
      "learning_rate": 1.2942191544434858e-07,
      "loss": 0.6839,
      "step": 11540
    },
    {
      "epoch": 9.96548748921484,
      "grad_norm": 6.26604700088501,
      "learning_rate": 1.0353753235547886e-07,
      "loss": 0.6734,
      "step": 11550
    },
    {
      "epoch": 9.97411561691113,
      "grad_norm": 5.720871925354004,
      "learning_rate": 7.765314926660914e-08,
      "loss": 0.7578,
      "step": 11560
    },
    {
      "epoch": 9.98274374460742,
      "grad_norm": 6.920917987823486,
      "learning_rate": 5.176876617773943e-08,
      "loss": 0.7097,
      "step": 11570
    },
    {
      "epoch": 9.99137187230371,
      "grad_norm": 3.213798761367798,
      "learning_rate": 2.5884383088869716e-08,
      "loss": 0.709,
      "step": 11580
    },
    {
      "epoch": 10.0,
      "grad_norm": 8.872552871704102,
      "learning_rate": 0.0,
      "loss": 0.732,
      "step": 11590
    },
    {
      "epoch": 10.0,
      "eval_loss": 0.48961344361305237,
      "eval_runtime": 2.4025,
      "eval_samples_per_second": 206.448,
      "eval_steps_per_second": 25.806,
      "step": 11590
    }
  ],
  "logging_steps": 10,
  "max_steps": 11590,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2106359428055040.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
