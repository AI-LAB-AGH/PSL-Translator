{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.0,
  "eval_steps": 500,
  "global_step": 2020,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.019801980198019802,
      "grad_norm": 38.12550735473633,
      "learning_rate": 2.9881188118811883e-05,
      "loss": 12.5425,
      "step": 10
    },
    {
      "epoch": 0.039603960396039604,
      "grad_norm": 32.96445846557617,
      "learning_rate": 2.9762376237623763e-05,
      "loss": 9.072,
      "step": 20
    },
    {
      "epoch": 0.0594059405940594,
      "grad_norm": 14.493912696838379,
      "learning_rate": 2.9643564356435645e-05,
      "loss": 6.9646,
      "step": 30
    },
    {
      "epoch": 0.07920792079207921,
      "grad_norm": 12.397454261779785,
      "learning_rate": 2.9524752475247524e-05,
      "loss": 5.8368,
      "step": 40
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 15.740983009338379,
      "learning_rate": 2.9405940594059407e-05,
      "loss": 5.0583,
      "step": 50
    },
    {
      "epoch": 0.1188118811881188,
      "grad_norm": 13.017190933227539,
      "learning_rate": 2.9287128712871286e-05,
      "loss": 4.4776,
      "step": 60
    },
    {
      "epoch": 0.13861386138613863,
      "grad_norm": 10.921607971191406,
      "learning_rate": 2.916831683168317e-05,
      "loss": 3.9375,
      "step": 70
    },
    {
      "epoch": 0.15841584158415842,
      "grad_norm": 7.021208763122559,
      "learning_rate": 2.9049504950495048e-05,
      "loss": 3.7,
      "step": 80
    },
    {
      "epoch": 0.1782178217821782,
      "grad_norm": 6.010225296020508,
      "learning_rate": 2.893069306930693e-05,
      "loss": 3.4276,
      "step": 90
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 12.569280624389648,
      "learning_rate": 2.881188118811881e-05,
      "loss": 3.3032,
      "step": 100
    },
    {
      "epoch": 0.21782178217821782,
      "grad_norm": 4.946533679962158,
      "learning_rate": 2.8693069306930693e-05,
      "loss": 2.7774,
      "step": 110
    },
    {
      "epoch": 0.2376237623762376,
      "grad_norm": 5.074427127838135,
      "learning_rate": 2.8574257425742575e-05,
      "loss": 2.6842,
      "step": 120
    },
    {
      "epoch": 0.25742574257425743,
      "grad_norm": 5.9205322265625,
      "learning_rate": 2.8455445544554455e-05,
      "loss": 2.5,
      "step": 130
    },
    {
      "epoch": 0.27722772277227725,
      "grad_norm": 9.872967720031738,
      "learning_rate": 2.8336633663366337e-05,
      "loss": 2.3236,
      "step": 140
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 5.113757610321045,
      "learning_rate": 2.8217821782178216e-05,
      "loss": 2.2348,
      "step": 150
    },
    {
      "epoch": 0.31683168316831684,
      "grad_norm": 8.995770454406738,
      "learning_rate": 2.8099009900990102e-05,
      "loss": 2.1043,
      "step": 160
    },
    {
      "epoch": 0.33663366336633666,
      "grad_norm": 4.2637176513671875,
      "learning_rate": 2.7980198019801982e-05,
      "loss": 1.9763,
      "step": 170
    },
    {
      "epoch": 0.3564356435643564,
      "grad_norm": 3.959303140640259,
      "learning_rate": 2.7861386138613864e-05,
      "loss": 1.7891,
      "step": 180
    },
    {
      "epoch": 0.37623762376237624,
      "grad_norm": 4.666933536529541,
      "learning_rate": 2.7742574257425744e-05,
      "loss": 1.6111,
      "step": 190
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 3.484611988067627,
      "learning_rate": 2.7623762376237626e-05,
      "loss": 1.7559,
      "step": 200
    },
    {
      "epoch": 0.4158415841584158,
      "grad_norm": 4.388702392578125,
      "learning_rate": 2.7504950495049505e-05,
      "loss": 1.646,
      "step": 210
    },
    {
      "epoch": 0.43564356435643564,
      "grad_norm": 3.2604176998138428,
      "learning_rate": 2.7386138613861388e-05,
      "loss": 1.4643,
      "step": 220
    },
    {
      "epoch": 0.45544554455445546,
      "grad_norm": 3.340087652206421,
      "learning_rate": 2.7267326732673267e-05,
      "loss": 1.5881,
      "step": 230
    },
    {
      "epoch": 0.4752475247524752,
      "grad_norm": 3.865102529525757,
      "learning_rate": 2.714851485148515e-05,
      "loss": 1.5261,
      "step": 240
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 3.747357130050659,
      "learning_rate": 2.7029702970297033e-05,
      "loss": 1.3242,
      "step": 250
    },
    {
      "epoch": 0.5148514851485149,
      "grad_norm": 4.392080783843994,
      "learning_rate": 2.6910891089108912e-05,
      "loss": 1.4095,
      "step": 260
    },
    {
      "epoch": 0.5346534653465347,
      "grad_norm": 7.580249786376953,
      "learning_rate": 2.6792079207920794e-05,
      "loss": 1.3273,
      "step": 270
    },
    {
      "epoch": 0.5544554455445545,
      "grad_norm": 3.128220319747925,
      "learning_rate": 2.6673267326732674e-05,
      "loss": 1.0683,
      "step": 280
    },
    {
      "epoch": 0.5742574257425742,
      "grad_norm": 3.857161045074463,
      "learning_rate": 2.6554455445544556e-05,
      "loss": 1.1971,
      "step": 290
    },
    {
      "epoch": 0.594059405940594,
      "grad_norm": 8.679313659667969,
      "learning_rate": 2.6435643564356436e-05,
      "loss": 1.1008,
      "step": 300
    },
    {
      "epoch": 0.6138613861386139,
      "grad_norm": 4.309866428375244,
      "learning_rate": 2.6316831683168318e-05,
      "loss": 1.007,
      "step": 310
    },
    {
      "epoch": 0.6336633663366337,
      "grad_norm": 3.0419178009033203,
      "learning_rate": 2.6198019801980197e-05,
      "loss": 0.8885,
      "step": 320
    },
    {
      "epoch": 0.6534653465346535,
      "grad_norm": 3.275751829147339,
      "learning_rate": 2.607920792079208e-05,
      "loss": 0.9512,
      "step": 330
    },
    {
      "epoch": 0.6732673267326733,
      "grad_norm": 2.785710573196411,
      "learning_rate": 2.596039603960396e-05,
      "loss": 0.9313,
      "step": 340
    },
    {
      "epoch": 0.693069306930693,
      "grad_norm": 2.9661803245544434,
      "learning_rate": 2.5841584158415842e-05,
      "loss": 1.1114,
      "step": 350
    },
    {
      "epoch": 0.7128712871287128,
      "grad_norm": 3.2327215671539307,
      "learning_rate": 2.5722772277227725e-05,
      "loss": 0.8893,
      "step": 360
    },
    {
      "epoch": 0.7326732673267327,
      "grad_norm": 2.81118106842041,
      "learning_rate": 2.5603960396039604e-05,
      "loss": 0.9219,
      "step": 370
    },
    {
      "epoch": 0.7524752475247525,
      "grad_norm": 3.379028558731079,
      "learning_rate": 2.5485148514851486e-05,
      "loss": 0.8657,
      "step": 380
    },
    {
      "epoch": 0.7722772277227723,
      "grad_norm": 2.580869197845459,
      "learning_rate": 2.5366336633663366e-05,
      "loss": 0.8696,
      "step": 390
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 2.7757349014282227,
      "learning_rate": 2.5247524752475248e-05,
      "loss": 0.8513,
      "step": 400
    },
    {
      "epoch": 0.8118811881188119,
      "grad_norm": 2.764021396636963,
      "learning_rate": 2.5128712871287128e-05,
      "loss": 0.8181,
      "step": 410
    },
    {
      "epoch": 0.8316831683168316,
      "grad_norm": 2.729079484939575,
      "learning_rate": 2.500990099009901e-05,
      "loss": 0.8378,
      "step": 420
    },
    {
      "epoch": 0.8514851485148515,
      "grad_norm": 2.2258450984954834,
      "learning_rate": 2.489108910891089e-05,
      "loss": 0.8074,
      "step": 430
    },
    {
      "epoch": 0.8712871287128713,
      "grad_norm": 3.0495071411132812,
      "learning_rate": 2.4772277227722772e-05,
      "loss": 0.8174,
      "step": 440
    },
    {
      "epoch": 0.8910891089108911,
      "grad_norm": 2.494236946105957,
      "learning_rate": 2.465346534653465e-05,
      "loss": 0.7935,
      "step": 450
    },
    {
      "epoch": 0.9108910891089109,
      "grad_norm": 3.2536869049072266,
      "learning_rate": 2.4534653465346534e-05,
      "loss": 0.7086,
      "step": 460
    },
    {
      "epoch": 0.9306930693069307,
      "grad_norm": 2.863539457321167,
      "learning_rate": 2.4415841584158416e-05,
      "loss": 0.8141,
      "step": 470
    },
    {
      "epoch": 0.9504950495049505,
      "grad_norm": 2.550044059753418,
      "learning_rate": 2.42970297029703e-05,
      "loss": 0.7714,
      "step": 480
    },
    {
      "epoch": 0.9702970297029703,
      "grad_norm": 3.0317583084106445,
      "learning_rate": 2.4178217821782182e-05,
      "loss": 0.7911,
      "step": 490
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 2.924025774002075,
      "learning_rate": 2.405940594059406e-05,
      "loss": 0.7948,
      "step": 500
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.32246899604797363,
      "eval_runtime": 6.034,
      "eval_samples_per_second": 90.652,
      "eval_steps_per_second": 11.435,
      "step": 505
    },
    {
      "epoch": 1.00990099009901,
      "grad_norm": 2.429272413253784,
      "learning_rate": 2.3940594059405944e-05,
      "loss": 0.6283,
      "step": 510
    },
    {
      "epoch": 1.0297029702970297,
      "grad_norm": 1.982556700706482,
      "learning_rate": 2.3821782178217823e-05,
      "loss": 0.7122,
      "step": 520
    },
    {
      "epoch": 1.0495049504950495,
      "grad_norm": 2.341482639312744,
      "learning_rate": 2.3702970297029705e-05,
      "loss": 0.6526,
      "step": 530
    },
    {
      "epoch": 1.0693069306930694,
      "grad_norm": 2.7333855628967285,
      "learning_rate": 2.3584158415841585e-05,
      "loss": 0.6866,
      "step": 540
    },
    {
      "epoch": 1.0891089108910892,
      "grad_norm": 2.3562450408935547,
      "learning_rate": 2.3465346534653467e-05,
      "loss": 0.7023,
      "step": 550
    },
    {
      "epoch": 1.108910891089109,
      "grad_norm": 2.1178932189941406,
      "learning_rate": 2.3346534653465347e-05,
      "loss": 0.5783,
      "step": 560
    },
    {
      "epoch": 1.1287128712871288,
      "grad_norm": 2.847126007080078,
      "learning_rate": 2.322772277227723e-05,
      "loss": 0.592,
      "step": 570
    },
    {
      "epoch": 1.1485148514851484,
      "grad_norm": 2.3719100952148438,
      "learning_rate": 2.310891089108911e-05,
      "loss": 0.6981,
      "step": 580
    },
    {
      "epoch": 1.1683168316831682,
      "grad_norm": 2.7156760692596436,
      "learning_rate": 2.299009900990099e-05,
      "loss": 0.6591,
      "step": 590
    },
    {
      "epoch": 1.188118811881188,
      "grad_norm": 2.4275765419006348,
      "learning_rate": 2.2871287128712874e-05,
      "loss": 0.6999,
      "step": 600
    },
    {
      "epoch": 1.2079207920792079,
      "grad_norm": 2.4191553592681885,
      "learning_rate": 2.2752475247524753e-05,
      "loss": 0.7456,
      "step": 610
    },
    {
      "epoch": 1.2277227722772277,
      "grad_norm": 2.8736071586608887,
      "learning_rate": 2.2633663366336636e-05,
      "loss": 0.7022,
      "step": 620
    },
    {
      "epoch": 1.2475247524752475,
      "grad_norm": 2.0258493423461914,
      "learning_rate": 2.2514851485148515e-05,
      "loss": 0.6902,
      "step": 630
    },
    {
      "epoch": 1.2673267326732673,
      "grad_norm": 2.315809726715088,
      "learning_rate": 2.2396039603960397e-05,
      "loss": 0.6328,
      "step": 640
    },
    {
      "epoch": 1.2871287128712872,
      "grad_norm": 2.127931833267212,
      "learning_rate": 2.2277227722772277e-05,
      "loss": 0.6257,
      "step": 650
    },
    {
      "epoch": 1.306930693069307,
      "grad_norm": 2.047898530960083,
      "learning_rate": 2.215841584158416e-05,
      "loss": 0.7193,
      "step": 660
    },
    {
      "epoch": 1.3267326732673268,
      "grad_norm": 2.4019863605499268,
      "learning_rate": 2.203960396039604e-05,
      "loss": 0.5724,
      "step": 670
    },
    {
      "epoch": 1.3465346534653464,
      "grad_norm": 2.0510144233703613,
      "learning_rate": 2.192079207920792e-05,
      "loss": 0.6108,
      "step": 680
    },
    {
      "epoch": 1.3663366336633662,
      "grad_norm": 2.4758083820343018,
      "learning_rate": 2.18019801980198e-05,
      "loss": 0.6559,
      "step": 690
    },
    {
      "epoch": 1.386138613861386,
      "grad_norm": 2.133113145828247,
      "learning_rate": 2.1683168316831683e-05,
      "loss": 0.5931,
      "step": 700
    },
    {
      "epoch": 1.4059405940594059,
      "grad_norm": 2.048666477203369,
      "learning_rate": 2.1564356435643566e-05,
      "loss": 0.6248,
      "step": 710
    },
    {
      "epoch": 1.4257425742574257,
      "grad_norm": 2.2061798572540283,
      "learning_rate": 2.1445544554455445e-05,
      "loss": 0.6246,
      "step": 720
    },
    {
      "epoch": 1.4455445544554455,
      "grad_norm": 2.289154291152954,
      "learning_rate": 2.1326732673267328e-05,
      "loss": 0.6633,
      "step": 730
    },
    {
      "epoch": 1.4653465346534653,
      "grad_norm": 2.0739400386810303,
      "learning_rate": 2.1207920792079207e-05,
      "loss": 0.689,
      "step": 740
    },
    {
      "epoch": 1.4851485148514851,
      "grad_norm": 2.0674381256103516,
      "learning_rate": 2.108910891089109e-05,
      "loss": 0.5792,
      "step": 750
    },
    {
      "epoch": 1.504950495049505,
      "grad_norm": 2.068389654159546,
      "learning_rate": 2.097029702970297e-05,
      "loss": 0.6064,
      "step": 760
    },
    {
      "epoch": 1.5247524752475248,
      "grad_norm": 1.8387564420700073,
      "learning_rate": 2.085148514851485e-05,
      "loss": 0.589,
      "step": 770
    },
    {
      "epoch": 1.5445544554455446,
      "grad_norm": 2.6564300060272217,
      "learning_rate": 2.073267326732673e-05,
      "loss": 0.6127,
      "step": 780
    },
    {
      "epoch": 1.5643564356435644,
      "grad_norm": 2.0864927768707275,
      "learning_rate": 2.0613861386138617e-05,
      "loss": 0.6196,
      "step": 790
    },
    {
      "epoch": 1.5841584158415842,
      "grad_norm": 2.154181957244873,
      "learning_rate": 2.0495049504950496e-05,
      "loss": 0.6145,
      "step": 800
    },
    {
      "epoch": 1.603960396039604,
      "grad_norm": 3.2405600547790527,
      "learning_rate": 2.037623762376238e-05,
      "loss": 0.6737,
      "step": 810
    },
    {
      "epoch": 1.6237623762376239,
      "grad_norm": 2.0464649200439453,
      "learning_rate": 2.0257425742574258e-05,
      "loss": 0.6342,
      "step": 820
    },
    {
      "epoch": 1.6435643564356437,
      "grad_norm": 2.229060649871826,
      "learning_rate": 2.013861386138614e-05,
      "loss": 0.5696,
      "step": 830
    },
    {
      "epoch": 1.6633663366336635,
      "grad_norm": 2.7601022720336914,
      "learning_rate": 2.0019801980198023e-05,
      "loss": 0.6188,
      "step": 840
    },
    {
      "epoch": 1.6831683168316833,
      "grad_norm": 1.9663379192352295,
      "learning_rate": 1.9900990099009902e-05,
      "loss": 0.5749,
      "step": 850
    },
    {
      "epoch": 1.702970297029703,
      "grad_norm": 2.2884645462036133,
      "learning_rate": 1.9782178217821785e-05,
      "loss": 0.5645,
      "step": 860
    },
    {
      "epoch": 1.7227722772277227,
      "grad_norm": 1.9451783895492554,
      "learning_rate": 1.9663366336633664e-05,
      "loss": 0.6513,
      "step": 870
    },
    {
      "epoch": 1.7425742574257426,
      "grad_norm": 1.9360140562057495,
      "learning_rate": 1.9544554455445547e-05,
      "loss": 0.6961,
      "step": 880
    },
    {
      "epoch": 1.7623762376237624,
      "grad_norm": 1.8710848093032837,
      "learning_rate": 1.9425742574257426e-05,
      "loss": 0.5974,
      "step": 890
    },
    {
      "epoch": 1.7821782178217822,
      "grad_norm": 1.7916975021362305,
      "learning_rate": 1.930693069306931e-05,
      "loss": 0.7182,
      "step": 900
    },
    {
      "epoch": 1.801980198019802,
      "grad_norm": 2.7772579193115234,
      "learning_rate": 1.9188118811881188e-05,
      "loss": 0.6033,
      "step": 910
    },
    {
      "epoch": 1.8217821782178216,
      "grad_norm": 2.064223527908325,
      "learning_rate": 1.906930693069307e-05,
      "loss": 0.5889,
      "step": 920
    },
    {
      "epoch": 1.8415841584158414,
      "grad_norm": 1.8616582155227661,
      "learning_rate": 1.895049504950495e-05,
      "loss": 0.6087,
      "step": 930
    },
    {
      "epoch": 1.8613861386138613,
      "grad_norm": 2.0658254623413086,
      "learning_rate": 1.8831683168316832e-05,
      "loss": 0.6282,
      "step": 940
    },
    {
      "epoch": 1.881188118811881,
      "grad_norm": 2.262714385986328,
      "learning_rate": 1.8712871287128715e-05,
      "loss": 0.5828,
      "step": 950
    },
    {
      "epoch": 1.900990099009901,
      "grad_norm": 2.255544424057007,
      "learning_rate": 1.8594059405940594e-05,
      "loss": 0.4895,
      "step": 960
    },
    {
      "epoch": 1.9207920792079207,
      "grad_norm": 2.5977721214294434,
      "learning_rate": 1.8475247524752477e-05,
      "loss": 0.6288,
      "step": 970
    },
    {
      "epoch": 1.9405940594059405,
      "grad_norm": 1.9332832098007202,
      "learning_rate": 1.8356435643564356e-05,
      "loss": 0.6247,
      "step": 980
    },
    {
      "epoch": 1.9603960396039604,
      "grad_norm": 1.7136707305908203,
      "learning_rate": 1.823762376237624e-05,
      "loss": 0.5968,
      "step": 990
    },
    {
      "epoch": 1.9801980198019802,
      "grad_norm": 2.2190468311309814,
      "learning_rate": 1.8118811881188118e-05,
      "loss": 0.5888,
      "step": 1000
    },
    {
      "epoch": 2.0,
      "grad_norm": 4.22716760635376,
      "learning_rate": 1.8e-05,
      "loss": 0.5861,
      "step": 1010
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.29473876953125,
      "eval_runtime": 5.7083,
      "eval_samples_per_second": 95.825,
      "eval_steps_per_second": 12.088,
      "step": 1010
    },
    {
      "epoch": 2.01980198019802,
      "grad_norm": 2.3837461471557617,
      "learning_rate": 1.788118811881188e-05,
      "loss": 0.6891,
      "step": 1020
    },
    {
      "epoch": 2.0396039603960396,
      "grad_norm": 2.129300355911255,
      "learning_rate": 1.7762376237623762e-05,
      "loss": 0.6144,
      "step": 1030
    },
    {
      "epoch": 2.0594059405940595,
      "grad_norm": 1.8922765254974365,
      "learning_rate": 1.764356435643564e-05,
      "loss": 0.5804,
      "step": 1040
    },
    {
      "epoch": 2.0792079207920793,
      "grad_norm": 2.22236704826355,
      "learning_rate": 1.7524752475247524e-05,
      "loss": 0.5948,
      "step": 1050
    },
    {
      "epoch": 2.099009900990099,
      "grad_norm": 1.9059120416641235,
      "learning_rate": 1.7405940594059407e-05,
      "loss": 0.5606,
      "step": 1060
    },
    {
      "epoch": 2.118811881188119,
      "grad_norm": 1.3787951469421387,
      "learning_rate": 1.7287128712871286e-05,
      "loss": 0.5028,
      "step": 1070
    },
    {
      "epoch": 2.1386138613861387,
      "grad_norm": 1.8838022947311401,
      "learning_rate": 1.716831683168317e-05,
      "loss": 0.5071,
      "step": 1080
    },
    {
      "epoch": 2.1584158415841586,
      "grad_norm": 1.7257975339889526,
      "learning_rate": 1.7049504950495048e-05,
      "loss": 0.5604,
      "step": 1090
    },
    {
      "epoch": 2.1782178217821784,
      "grad_norm": 1.3175967931747437,
      "learning_rate": 1.693069306930693e-05,
      "loss": 0.4842,
      "step": 1100
    },
    {
      "epoch": 2.198019801980198,
      "grad_norm": 1.9293378591537476,
      "learning_rate": 1.6811881188118813e-05,
      "loss": 0.557,
      "step": 1110
    },
    {
      "epoch": 2.217821782178218,
      "grad_norm": 2.268157482147217,
      "learning_rate": 1.6693069306930696e-05,
      "loss": 0.547,
      "step": 1120
    },
    {
      "epoch": 2.237623762376238,
      "grad_norm": 2.2699811458587646,
      "learning_rate": 1.6574257425742575e-05,
      "loss": 0.5541,
      "step": 1130
    },
    {
      "epoch": 2.2574257425742577,
      "grad_norm": 1.5036952495574951,
      "learning_rate": 1.6455445544554458e-05,
      "loss": 0.5159,
      "step": 1140
    },
    {
      "epoch": 2.2772277227722775,
      "grad_norm": 1.5300132036209106,
      "learning_rate": 1.6336633663366337e-05,
      "loss": 0.4922,
      "step": 1150
    },
    {
      "epoch": 2.297029702970297,
      "grad_norm": 2.255286931991577,
      "learning_rate": 1.621782178217822e-05,
      "loss": 0.5078,
      "step": 1160
    },
    {
      "epoch": 2.3168316831683167,
      "grad_norm": 1.2540018558502197,
      "learning_rate": 1.60990099009901e-05,
      "loss": 0.4888,
      "step": 1170
    },
    {
      "epoch": 2.3366336633663365,
      "grad_norm": 1.9462065696716309,
      "learning_rate": 1.598019801980198e-05,
      "loss": 0.581,
      "step": 1180
    },
    {
      "epoch": 2.3564356435643563,
      "grad_norm": 1.770648717880249,
      "learning_rate": 1.5861386138613864e-05,
      "loss": 0.4993,
      "step": 1190
    },
    {
      "epoch": 2.376237623762376,
      "grad_norm": 1.895848274230957,
      "learning_rate": 1.5742574257425743e-05,
      "loss": 0.5323,
      "step": 1200
    },
    {
      "epoch": 2.396039603960396,
      "grad_norm": 2.009218692779541,
      "learning_rate": 1.5623762376237626e-05,
      "loss": 0.5792,
      "step": 1210
    },
    {
      "epoch": 2.4158415841584158,
      "grad_norm": 1.3278840780258179,
      "learning_rate": 1.5504950495049505e-05,
      "loss": 0.5308,
      "step": 1220
    },
    {
      "epoch": 2.4356435643564356,
      "grad_norm": 1.98183274269104,
      "learning_rate": 1.5386138613861388e-05,
      "loss": 0.5499,
      "step": 1230
    },
    {
      "epoch": 2.4554455445544554,
      "grad_norm": 1.9045469760894775,
      "learning_rate": 1.5267326732673267e-05,
      "loss": 0.5579,
      "step": 1240
    },
    {
      "epoch": 2.4752475247524752,
      "grad_norm": 1.874794840812683,
      "learning_rate": 1.514851485148515e-05,
      "loss": 0.4946,
      "step": 1250
    },
    {
      "epoch": 2.495049504950495,
      "grad_norm": 1.7118942737579346,
      "learning_rate": 1.502970297029703e-05,
      "loss": 0.4581,
      "step": 1260
    },
    {
      "epoch": 2.514851485148515,
      "grad_norm": 2.1018588542938232,
      "learning_rate": 1.4910891089108912e-05,
      "loss": 0.5743,
      "step": 1270
    },
    {
      "epoch": 2.5346534653465347,
      "grad_norm": 1.826080083847046,
      "learning_rate": 1.4792079207920792e-05,
      "loss": 0.4525,
      "step": 1280
    },
    {
      "epoch": 2.5544554455445545,
      "grad_norm": 2.1994760036468506,
      "learning_rate": 1.4673267326732673e-05,
      "loss": 0.4975,
      "step": 1290
    },
    {
      "epoch": 2.5742574257425743,
      "grad_norm": 1.5840123891830444,
      "learning_rate": 1.4554455445544554e-05,
      "loss": 0.58,
      "step": 1300
    },
    {
      "epoch": 2.594059405940594,
      "grad_norm": 1.9679232835769653,
      "learning_rate": 1.4435643564356435e-05,
      "loss": 0.4734,
      "step": 1310
    },
    {
      "epoch": 2.613861386138614,
      "grad_norm": 1.672106385231018,
      "learning_rate": 1.4316831683168316e-05,
      "loss": 0.4812,
      "step": 1320
    },
    {
      "epoch": 2.633663366336634,
      "grad_norm": 1.9565469026565552,
      "learning_rate": 1.4198019801980197e-05,
      "loss": 0.516,
      "step": 1330
    },
    {
      "epoch": 2.6534653465346536,
      "grad_norm": 1.6015108823776245,
      "learning_rate": 1.4079207920792078e-05,
      "loss": 0.5504,
      "step": 1340
    },
    {
      "epoch": 2.6732673267326734,
      "grad_norm": 2.4536962509155273,
      "learning_rate": 1.396039603960396e-05,
      "loss": 0.5393,
      "step": 1350
    },
    {
      "epoch": 2.693069306930693,
      "grad_norm": 1.7051535844802856,
      "learning_rate": 1.3841584158415843e-05,
      "loss": 0.448,
      "step": 1360
    },
    {
      "epoch": 2.7128712871287126,
      "grad_norm": 1.749902606010437,
      "learning_rate": 1.3722772277227724e-05,
      "loss": 0.5265,
      "step": 1370
    },
    {
      "epoch": 2.7326732673267324,
      "grad_norm": 1.4500759840011597,
      "learning_rate": 1.3603960396039605e-05,
      "loss": 0.5199,
      "step": 1380
    },
    {
      "epoch": 2.7524752475247523,
      "grad_norm": 1.9551588296890259,
      "learning_rate": 1.3485148514851486e-05,
      "loss": 0.5542,
      "step": 1390
    },
    {
      "epoch": 2.772277227722772,
      "grad_norm": 2.1837267875671387,
      "learning_rate": 1.3366336633663367e-05,
      "loss": 0.4728,
      "step": 1400
    },
    {
      "epoch": 2.792079207920792,
      "grad_norm": 1.8055111169815063,
      "learning_rate": 1.3247524752475248e-05,
      "loss": 0.6125,
      "step": 1410
    },
    {
      "epoch": 2.8118811881188117,
      "grad_norm": 1.8288227319717407,
      "learning_rate": 1.3128712871287129e-05,
      "loss": 0.5986,
      "step": 1420
    },
    {
      "epoch": 2.8316831683168315,
      "grad_norm": 1.6507045030593872,
      "learning_rate": 1.300990099009901e-05,
      "loss": 0.5469,
      "step": 1430
    },
    {
      "epoch": 2.8514851485148514,
      "grad_norm": 1.8299683332443237,
      "learning_rate": 1.289108910891089e-05,
      "loss": 0.4561,
      "step": 1440
    },
    {
      "epoch": 2.871287128712871,
      "grad_norm": 1.4156686067581177,
      "learning_rate": 1.2772277227722772e-05,
      "loss": 0.4684,
      "step": 1450
    },
    {
      "epoch": 2.891089108910891,
      "grad_norm": 1.5390243530273438,
      "learning_rate": 1.2653465346534653e-05,
      "loss": 0.5176,
      "step": 1460
    },
    {
      "epoch": 2.910891089108911,
      "grad_norm": 2.1708450317382812,
      "learning_rate": 1.2534653465346535e-05,
      "loss": 0.5132,
      "step": 1470
    },
    {
      "epoch": 2.9306930693069306,
      "grad_norm": 1.560080885887146,
      "learning_rate": 1.2415841584158416e-05,
      "loss": 0.5428,
      "step": 1480
    },
    {
      "epoch": 2.9504950495049505,
      "grad_norm": 1.4789292812347412,
      "learning_rate": 1.2297029702970297e-05,
      "loss": 0.5022,
      "step": 1490
    },
    {
      "epoch": 2.9702970297029703,
      "grad_norm": 1.9953274726867676,
      "learning_rate": 1.217821782178218e-05,
      "loss": 0.4473,
      "step": 1500
    },
    {
      "epoch": 2.99009900990099,
      "grad_norm": 1.7670557498931885,
      "learning_rate": 1.205940594059406e-05,
      "loss": 0.5286,
      "step": 1510
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.2848391532897949,
      "eval_runtime": 6.0165,
      "eval_samples_per_second": 90.916,
      "eval_steps_per_second": 11.468,
      "step": 1515
    },
    {
      "epoch": 3.00990099009901,
      "grad_norm": 6.544393062591553,
      "learning_rate": 1.1940594059405942e-05,
      "loss": 0.4638,
      "step": 1520
    },
    {
      "epoch": 3.0297029702970297,
      "grad_norm": 1.6041896343231201,
      "learning_rate": 1.1821782178217823e-05,
      "loss": 0.5919,
      "step": 1530
    },
    {
      "epoch": 3.0495049504950495,
      "grad_norm": 1.6012760400772095,
      "learning_rate": 1.1702970297029703e-05,
      "loss": 0.5308,
      "step": 1540
    },
    {
      "epoch": 3.0693069306930694,
      "grad_norm": 1.376835823059082,
      "learning_rate": 1.1584158415841584e-05,
      "loss": 0.4834,
      "step": 1550
    },
    {
      "epoch": 3.089108910891089,
      "grad_norm": 1.6777757406234741,
      "learning_rate": 1.1465346534653465e-05,
      "loss": 0.4187,
      "step": 1560
    },
    {
      "epoch": 3.108910891089109,
      "grad_norm": 1.8213821649551392,
      "learning_rate": 1.1346534653465346e-05,
      "loss": 0.5143,
      "step": 1570
    },
    {
      "epoch": 3.128712871287129,
      "grad_norm": 2.3585283756256104,
      "learning_rate": 1.1227722772277227e-05,
      "loss": 0.51,
      "step": 1580
    },
    {
      "epoch": 3.1485148514851486,
      "grad_norm": 2.138979196548462,
      "learning_rate": 1.110891089108911e-05,
      "loss": 0.5165,
      "step": 1590
    },
    {
      "epoch": 3.1683168316831685,
      "grad_norm": 1.6521729230880737,
      "learning_rate": 1.099009900990099e-05,
      "loss": 0.4287,
      "step": 1600
    },
    {
      "epoch": 3.1881188118811883,
      "grad_norm": 1.58677077293396,
      "learning_rate": 1.0871287128712872e-05,
      "loss": 0.4323,
      "step": 1610
    },
    {
      "epoch": 3.207920792079208,
      "grad_norm": 1.6867989301681519,
      "learning_rate": 1.0752475247524753e-05,
      "loss": 0.488,
      "step": 1620
    },
    {
      "epoch": 3.227722772277228,
      "grad_norm": 1.5288575887680054,
      "learning_rate": 1.0633663366336634e-05,
      "loss": 0.4668,
      "step": 1630
    },
    {
      "epoch": 3.2475247524752477,
      "grad_norm": 2.204434633255005,
      "learning_rate": 1.0514851485148515e-05,
      "loss": 0.5585,
      "step": 1640
    },
    {
      "epoch": 3.2673267326732676,
      "grad_norm": 1.9861711263656616,
      "learning_rate": 1.0396039603960395e-05,
      "loss": 0.4811,
      "step": 1650
    },
    {
      "epoch": 3.287128712871287,
      "grad_norm": 2.195047378540039,
      "learning_rate": 1.0277227722772278e-05,
      "loss": 0.4983,
      "step": 1660
    },
    {
      "epoch": 3.3069306930693068,
      "grad_norm": 2.211698293685913,
      "learning_rate": 1.0158415841584159e-05,
      "loss": 0.5464,
      "step": 1670
    },
    {
      "epoch": 3.3267326732673266,
      "grad_norm": 1.402193546295166,
      "learning_rate": 1.003960396039604e-05,
      "loss": 0.429,
      "step": 1680
    },
    {
      "epoch": 3.3465346534653464,
      "grad_norm": 1.7204363346099854,
      "learning_rate": 9.920792079207921e-06,
      "loss": 0.5626,
      "step": 1690
    },
    {
      "epoch": 3.366336633663366,
      "grad_norm": 2.0316762924194336,
      "learning_rate": 9.801980198019802e-06,
      "loss": 0.4675,
      "step": 1700
    },
    {
      "epoch": 3.386138613861386,
      "grad_norm": 1.8258095979690552,
      "learning_rate": 9.683168316831684e-06,
      "loss": 0.5378,
      "step": 1710
    },
    {
      "epoch": 3.405940594059406,
      "grad_norm": 1.7830135822296143,
      "learning_rate": 9.564356435643565e-06,
      "loss": 0.4794,
      "step": 1720
    },
    {
      "epoch": 3.4257425742574257,
      "grad_norm": 1.7632091045379639,
      "learning_rate": 9.445544554455446e-06,
      "loss": 0.4485,
      "step": 1730
    },
    {
      "epoch": 3.4455445544554455,
      "grad_norm": 2.7745139598846436,
      "learning_rate": 9.326732673267327e-06,
      "loss": 0.4452,
      "step": 1740
    },
    {
      "epoch": 3.4653465346534653,
      "grad_norm": 1.8178116083145142,
      "learning_rate": 9.207920792079208e-06,
      "loss": 0.4853,
      "step": 1750
    },
    {
      "epoch": 3.485148514851485,
      "grad_norm": 1.800622820854187,
      "learning_rate": 9.089108910891089e-06,
      "loss": 0.4949,
      "step": 1760
    },
    {
      "epoch": 3.504950495049505,
      "grad_norm": 1.5975542068481445,
      "learning_rate": 8.97029702970297e-06,
      "loss": 0.4712,
      "step": 1770
    },
    {
      "epoch": 3.5247524752475248,
      "grad_norm": 1.928769826889038,
      "learning_rate": 8.851485148514851e-06,
      "loss": 0.538,
      "step": 1780
    },
    {
      "epoch": 3.5445544554455446,
      "grad_norm": 1.8128551244735718,
      "learning_rate": 8.732673267326732e-06,
      "loss": 0.5025,
      "step": 1790
    },
    {
      "epoch": 3.5643564356435644,
      "grad_norm": 2.086594581604004,
      "learning_rate": 8.613861386138613e-06,
      "loss": 0.4971,
      "step": 1800
    },
    {
      "epoch": 3.5841584158415842,
      "grad_norm": 1.6538180112838745,
      "learning_rate": 8.495049504950494e-06,
      "loss": 0.4586,
      "step": 1810
    },
    {
      "epoch": 3.603960396039604,
      "grad_norm": 1.8771182298660278,
      "learning_rate": 8.376237623762376e-06,
      "loss": 0.4552,
      "step": 1820
    },
    {
      "epoch": 3.623762376237624,
      "grad_norm": 1.4935930967330933,
      "learning_rate": 8.257425742574259e-06,
      "loss": 0.5175,
      "step": 1830
    },
    {
      "epoch": 3.6435643564356437,
      "grad_norm": 2.066499948501587,
      "learning_rate": 8.13861386138614e-06,
      "loss": 0.4508,
      "step": 1840
    },
    {
      "epoch": 3.6633663366336635,
      "grad_norm": 1.9446419477462769,
      "learning_rate": 8.019801980198021e-06,
      "loss": 0.4594,
      "step": 1850
    },
    {
      "epoch": 3.6831683168316833,
      "grad_norm": 1.9727294445037842,
      "learning_rate": 7.900990099009902e-06,
      "loss": 0.5291,
      "step": 1860
    },
    {
      "epoch": 3.7029702970297027,
      "grad_norm": 2.283475399017334,
      "learning_rate": 7.782178217821783e-06,
      "loss": 0.4383,
      "step": 1870
    },
    {
      "epoch": 3.7227722772277225,
      "grad_norm": 1.8771032094955444,
      "learning_rate": 7.663366336633664e-06,
      "loss": 0.5391,
      "step": 1880
    },
    {
      "epoch": 3.7425742574257423,
      "grad_norm": 1.745961308479309,
      "learning_rate": 7.544554455445545e-06,
      "loss": 0.4674,
      "step": 1890
    },
    {
      "epoch": 3.762376237623762,
      "grad_norm": 1.6841022968292236,
      "learning_rate": 7.4257425742574256e-06,
      "loss": 0.41,
      "step": 1900
    },
    {
      "epoch": 3.782178217821782,
      "grad_norm": 1.5469650030136108,
      "learning_rate": 7.306930693069307e-06,
      "loss": 0.449,
      "step": 1910
    },
    {
      "epoch": 3.801980198019802,
      "grad_norm": 1.7648041248321533,
      "learning_rate": 7.188118811881188e-06,
      "loss": 0.4283,
      "step": 1920
    },
    {
      "epoch": 3.8217821782178216,
      "grad_norm": 1.7141064405441284,
      "learning_rate": 7.069306930693069e-06,
      "loss": 0.4367,
      "step": 1930
    },
    {
      "epoch": 3.8415841584158414,
      "grad_norm": 1.9565105438232422,
      "learning_rate": 6.950495049504951e-06,
      "loss": 0.5151,
      "step": 1940
    },
    {
      "epoch": 3.8613861386138613,
      "grad_norm": 2.44484543800354,
      "learning_rate": 6.831683168316832e-06,
      "loss": 0.484,
      "step": 1950
    },
    {
      "epoch": 3.881188118811881,
      "grad_norm": 1.4052760601043701,
      "learning_rate": 6.712871287128713e-06,
      "loss": 0.4757,
      "step": 1960
    },
    {
      "epoch": 3.900990099009901,
      "grad_norm": 3.6747074127197266,
      "learning_rate": 6.594059405940595e-06,
      "loss": 0.4427,
      "step": 1970
    },
    {
      "epoch": 3.9207920792079207,
      "grad_norm": 1.64024019241333,
      "learning_rate": 6.4752475247524756e-06,
      "loss": 0.5068,
      "step": 1980
    },
    {
      "epoch": 3.9405940594059405,
      "grad_norm": 1.6409430503845215,
      "learning_rate": 6.3564356435643565e-06,
      "loss": 0.4298,
      "step": 1990
    },
    {
      "epoch": 3.9603960396039604,
      "grad_norm": 2.0780415534973145,
      "learning_rate": 6.2376237623762374e-06,
      "loss": 0.494,
      "step": 2000
    },
    {
      "epoch": 3.98019801980198,
      "grad_norm": 1.964881420135498,
      "learning_rate": 6.118811881188118e-06,
      "loss": 0.4575,
      "step": 2010
    },
    {
      "epoch": 4.0,
      "grad_norm": 3.023618698120117,
      "learning_rate": 6e-06,
      "loss": 0.4091,
      "step": 2020
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.28254860639572144,
      "eval_runtime": 5.7072,
      "eval_samples_per_second": 95.844,
      "eval_steps_per_second": 12.09,
      "step": 2020
    }
  ],
  "logging_steps": 10,
  "max_steps": 2525,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4289025343422464e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
